{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Create realtime-component-definitions.yaml",
        "description": "Extend the existing webservice ComponentDefinition in application-component-definitions.yaml to add realtime parameter support instead of creating a new file.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "details": "Modify the existing webservice ComponentDefinition in `/Users/socrateshlapolosa/Development/health-service-idp/crossplane/oam/application-component-definitions.yaml` to add realtime support. Add a 'realtime' property to the parameter schema that references a realtime-platform component name. When this parameter is specified, the webservice will automatically get connection secrets injected. This follows DRY principles by extending existing proven infrastructure rather than duplicating functionality. Update the CUE template to handle realtime integration and secret injection.",
        "testStrategy": "Validate the modified YAML syntax using a linter. Ensure the updated file follows the OAM ComponentDefinition schema and that the CUE template is syntactically correct. Test that the file can be loaded by KubeVela without errors. Verify that a webservice with the realtime parameter correctly gets connection secrets injected.",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze existing component definition files",
            "description": "Review existing component definition files in the project to understand the structure and patterns used for OAM ComponentDefinitions.",
            "status": "done",
            "dependencies": [],
            "details": "Examine the webservice ComponentDefinition in `/Users/socrateshlapolosa/Development/health-service-idp/crossplane/oam/application-component-definitions.yaml` and other component definition files in the same directory. Identify how parameters are exposed in the developer interface and how they are mapped to underlying resources. Pay special attention to how secret injection is handled in existing components. Document the key elements that need to be modified to add realtime support to the webservice ComponentDefinition.",
            "testStrategy": "Create a checklist of required elements and patterns found in existing component definitions to ensure consistency."
          },
          {
            "id": 2,
            "title": "Identify integration points in webservice ComponentDefinition",
            "description": "Identify the specific sections of the webservice ComponentDefinition that need to be modified to add realtime support.",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "Analyze the webservice ComponentDefinition in application-component-definitions.yaml to identify where the realtime parameter should be added in the parameter schema. Determine how connection secrets should be injected when the realtime parameter is specified. Identify any existing patterns for referencing other components or injecting secrets that can be leveraged for realtime integration.",
            "testStrategy": "Document the specific sections of the file that will be modified and the changes that will be made to each section."
          },
          {
            "id": 3,
            "title": "Add realtime parameter to webservice parameter schema",
            "description": "Add the realtime parameter to the webservice ComponentDefinition parameter schema.",
            "status": "done",
            "dependencies": [
              2
            ],
            "details": "Modify the parameter schema in the webservice ComponentDefinition to add a new 'realtime' parameter. The parameter should be optional and accept a string value that references a realtime-platform component name. Include appropriate description, type definition, and default value (if applicable). Ensure the parameter is well-documented with clear examples of how it should be used.",
            "testStrategy": "Validate the modified parameter schema for syntax correctness. Ensure the new parameter is properly integrated with the existing schema."
          },
          {
            "id": 4,
            "title": "Update CUE template for realtime integration",
            "description": "Update the CUE template in the webservice ComponentDefinition to handle realtime integration and secret injection.",
            "status": "done",
            "dependencies": [
              3
            ],
            "details": "Modify the CUE template in the webservice ComponentDefinition to check for the presence of the realtime parameter. When the parameter is specified, add logic to inject the appropriate connection secrets from the referenced realtime-platform component. Implement conditional resource generation based on whether realtime integration is enabled. Ensure proper error handling if the referenced realtime component doesn't exist or doesn't provide the expected secrets.",
            "testStrategy": "Manually validate the CUE syntax. Create test parameter sets with and without the realtime parameter and review the rendered output to ensure it generates the expected resources with proper secret injection."
          },
          {
            "id": 5,
            "title": "Add documentation and finalize the changes",
            "description": "Add comprehensive inline documentation and finalize the changes to the application-component-definitions.yaml file.",
            "status": "done",
            "dependencies": [
              4
            ],
            "details": "Add detailed comments and documentation within the file to explain the purpose and usage of the realtime parameter in the webservice ComponentDefinition. Include examples of how to use the realtime parameter to connect a webservice to a realtime-platform component. Document how the connection secrets are injected and how they can be accessed by the application. Review the entire file for consistency, proper indentation, and adherence to project conventions. Ensure all modified sections are properly formatted and documented.",
            "testStrategy": "Validate the YAML syntax using a linter. Ensure the file can be loaded by KubeVela without errors. Have a peer review the changes for completeness and clarity of documentation."
          }
        ]
      },
      {
        "id": 2,
        "title": "Create realtime-xrds.yaml for RealtimePlatformClaim",
        "description": "Extend the existing ApplicationClaim XRD to add real-time streaming support, following DRY principles by building on the existing comprehensive schema.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "details": "Modify the existing file at `/Users/socrateshlapolosa/Development/health-service-idp/crossplane/application-claim-xrd.yaml` to extend the ApplicationClaim XRD with real-time streaming capabilities. Add the following properties to the schema: 1) 'realtime' (optional string that references realtime-platform component), 2) 'websocket' (boolean for WebSocket endpoint support), and 3) streaming properties for Kafka topic configuration. This approach leverages the proven ApplicationClaim infrastructure that already supports microservices with databases, caches, and API exposure, avoiding duplication of functionality.",
        "testStrategy": "Validate the modified XRD YAML syntax. Test that the updated XRD can be applied to a Kubernetes cluster with Crossplane installed. Verify that the schema validation works correctly for both existing and new real-time related fields. Ensure backward compatibility with existing ApplicationClaims.",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze existing ApplicationClaim XRD structure",
            "description": "Review the existing ApplicationClaim XRD in application-claim-xrd.yaml to understand its structure, schema, and extension points.",
            "status": "done",
            "dependencies": [],
            "details": "Open and analyze the file `/Users/socrateshlapolosa/Development/health-service-idp/crossplane/application-claim-xrd.yaml`. Document the current schema structure, required and optional properties, and status fields. Identify where and how to add real-time streaming properties without disrupting existing functionality. Create a plan for extending the schema that maintains backward compatibility.",
            "testStrategy": "Create a documentation of the existing schema structure and the planned changes. Review with team members to ensure understanding and alignment."
          },
          {
            "id": 2,
            "title": "Add realtime property to ApplicationClaim schema",
            "description": "Add the optional 'realtime' string property to the ApplicationClaim XRD schema that references a realtime-platform component.",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "Extend the properties object in the ApplicationClaim XRD schema to include a 'realtime' field with type 'string'. Add a description explaining that this property references a realtime-platform component name that will be used for streaming data. Make this property optional (not included in the required array). Document how this property integrates with the existing application infrastructure.",
            "testStrategy": "Validate the modified YAML syntax. Test with sample claims that both include and omit the realtime property to verify that it's properly handled as optional."
          },
          {
            "id": 3,
            "title": "Add websocket property to ApplicationClaim schema",
            "description": "Add the optional 'websocket' boolean property to the ApplicationClaim XRD schema for WebSocket endpoint support.",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "Extend the properties object in the ApplicationClaim XRD schema to include a 'websocket' field with type 'boolean' and default value of false. Add a description explaining that when true, a WebSocket endpoint will be provisioned for real-time client communications. Make this property optional and document its relationship with the realtime property.",
            "testStrategy": "Test with sample claims that both include and omit the websocket property to verify that defaults are applied correctly when the field is omitted."
          },
          {
            "id": 4,
            "title": "Add streaming properties for Kafka configuration",
            "description": "Add Kafka topic configuration properties to the ApplicationClaim XRD schema for streaming data management.",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "Add a 'streaming' object property to the ApplicationClaim XRD schema with nested properties for Kafka topic configuration, including: 1) 'topics' (array of topic objects with name, partitions, and replication factor), 2) 'retention' (object with time and size retention policies), and 3) 'security' (object with authentication and authorization settings). Make the streaming property optional and only relevant when the realtime property is specified.",
            "testStrategy": "Validate the complex schema structures. Create test claims with various combinations of streaming properties to ensure they are properly validated."
          },
          {
            "id": 5,
            "title": "Update status fields for real-time connection information",
            "description": "Extend the status fields in the ApplicationClaim XRD to include real-time connection endpoints and secrets.",
            "status": "done",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Extend the status section of the ApplicationClaim XRD schema to include additional fields for real-time connections: 1) 'kafkaEndpoint' (string for Kafka broker endpoint), 2) 'websocketEndpoint' (string for WebSocket endpoint if enabled), and 3) 'streamingSecretRef' (object with name and namespace properties to reference streaming connection secrets). Include appropriate descriptions for each new status field explaining what information it provides and how it can be used.",
            "testStrategy": "Verify that the extended status schema is correctly defined. Ensure that these fields will be properly populated by the composition when real-time resources are provisioned."
          },
          {
            "id": 6,
            "title": "Ensure backward compatibility",
            "description": "Verify and ensure that the extended ApplicationClaim XRD maintains backward compatibility with existing claims.",
            "status": "done",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Review all changes made to the ApplicationClaim XRD to ensure they don't break existing functionality. Verify that all new properties are optional and have sensible defaults where appropriate. Test with existing ApplicationClaim examples to confirm they still work without modification. Document any migration considerations for existing claims.",
            "testStrategy": "Apply the modified XRD to a test cluster with existing ApplicationClaims. Verify that existing claims continue to function correctly without any changes. Test creating new claims with various combinations of old and new properties."
          }
        ]
      },
      {
        "id": 3,
        "title": "Enhance webservice ComponentDefinition with realtime integration",
        "description": "Modify the existing webservice ComponentDefinition to add optional realtime platform integration capabilities with an ultra-minimal developer interface.",
        "status": "done",
        "dependencies": [
          1
        ],
        "priority": "high",
        "details": "Update the webservice ComponentDefinition in `/Users/socrateshlapolosa/Development/health-service-idp/crossplane/oam/application-component-definitions.yaml` to add an optional `realtime` parameter. This parameter will accept the name of a realtime-platform component to connect to. Implement the logic to check if the specified realtime-platform exists in the cluster, and if so, inject the appropriate connection secrets into the webservice environment. The secrets to inject include: {realtime-name}-mqtt-secret, {realtime-name}-kafka-secret, {realtime-name}-db-secret, {realtime-name}-metabase-secret, and {realtime-name}-lenses-secret. The XRD schema should provide an ultra-minimal developer interface with only essential parameters: name (required), database (optional, default: postgres), visualization (optional, default: metabase), iot (optional, default: true). All other complexity (Lenses configuration, Kafka setup, connector configs, volume mounts, health checks, service dependencies) must be abstracted and handled by Crossplane compositions.",
        "testStrategy": "Test the modified webservice ComponentDefinition by creating a webservice component that references a realtime-platform. Verify that the connection secrets are correctly injected into the webservice environment. Test with both existing and non-existing realtime-platform names to ensure proper error handling. Validate that the minimal parameter set works correctly with default values and that all complexity is properly abstracted in the Crossplane compositions.",
        "subtasks": [
          {
            "id": 1,
            "title": "Update ComponentDefinition schema with realtime parameter",
            "description": "Modify the webservice ComponentDefinition schema in the application-component-definitions.yaml file to include an optional 'realtime' parameter that accepts a string value representing the name of a realtime-platform component.",
            "status": "done",
            "dependencies": [],
            "details": "Open the file at `/Users/socrateshlapolosa/Development/health-service-idp/crossplane/oam/application-component-definitions.yaml`. Locate the webservice ComponentDefinition section. Add the new 'realtime' parameter to the schema definition in the CUE template. The parameter should be optional and accept a string value. Add appropriate documentation comments explaining that this parameter is used to connect to a realtime-platform component. Update any validation logic to accommodate the new parameter.",
            "testStrategy": "Validate the YAML syntax after modification. Ensure the CUE template is still syntactically correct. Verify that the schema correctly marks the realtime parameter as optional."
          },
          {
            "id": 2,
            "title": "Implement ultra-minimal XRD schema for realtime integration",
            "description": "Update the XRD schema to provide a minimal developer interface with only essential parameters while abstracting all complexity.",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "Modify the XRD schema to include only the following parameters: name (required), database (optional, default: postgres), visualization (optional, default: metabase), iot (optional, default: true). Ensure that all other complexity including Lenses configuration, Kafka setup, connector configs, volume mounts, health checks, and service dependencies are abstracted away from the developer interface. Add appropriate validation rules to enforce this minimal parameter set. Update the CUE template to handle these parameters and map them to the appropriate underlying configurations in the Crossplane compositions.",
            "testStrategy": "Verify that the XRD schema correctly defines the minimal parameter set with appropriate defaults. Test creating components with various combinations of parameters and defaults to ensure they are processed correctly."
          },
          {
            "id": 3,
            "title": "Implement realtime platform existence check",
            "description": "Add logic to verify if the specified realtime-platform component exists in the cluster before attempting to inject connection secrets.",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "In the webservice ComponentDefinition's CUE template, add conditional logic that checks if the 'realtime' parameter is provided. If it is, implement a Kubernetes API call to check if a component with the specified name exists in the cluster. This can be done using a Kubernetes Job or by leveraging KubeVela's built-in capabilities for cross-component references. The check should return a clear error message if the specified realtime-platform does not exist.",
            "testStrategy": "Test with both existing and non-existing realtime-platform names. Verify that appropriate error messages are generated when a non-existent platform is referenced."
          },
          {
            "id": 4,
            "title": "Implement secret injection mechanism",
            "description": "Create the logic to inject the appropriate connection secrets from the realtime platform into the webservice environment variables.",
            "status": "done",
            "dependencies": [
              3
            ],
            "details": "Modify the webservice ComponentDefinition to include logic that, when a valid realtime platform is specified, injects the following secrets into the webservice environment: {realtime-name}-mqtt-secret, {realtime-name}-kafka-secret, {realtime-name}-db-secret, {realtime-name}-metabase-secret, and {realtime-name}-lenses-secret. Use the Kubernetes secretRef mechanism to reference these secrets. Ensure that the secret names are properly constructed using the provided realtime platform name as a prefix.",
            "testStrategy": "Create a test webservice component that references an existing realtime platform. Verify that all five required secrets are correctly injected into the environment variables of the deployed webservice."
          },
          {
            "id": 5,
            "title": "Implement abstraction layer for complex configurations",
            "description": "Create an abstraction layer that handles all complex configurations based on the minimal parameter set.",
            "status": "done",
            "dependencies": [
              2,
              4
            ],
            "details": "Implement logic in the Crossplane compositions that translates the minimal parameter set (name, database, visualization, iot) into the full set of configurations needed for the realtime platform integration. This includes automatically configuring Lenses, Kafka, connectors, volume mounts, health checks, and service dependencies based on the provided parameters and their defaults. Ensure that all complexity is hidden from the developer interface while still providing full functionality.",
            "testStrategy": "Test various combinations of the minimal parameter set to ensure they correctly generate the appropriate complex configurations. Verify that changing each parameter results in the expected changes to the underlying configurations."
          },
          {
            "id": 6,
            "title": "Add error handling and validation",
            "description": "Implement comprehensive error handling and validation for the realtime integration feature to ensure robustness.",
            "status": "done",
            "dependencies": [
              4
            ],
            "details": "Enhance the implementation with proper error handling for various scenarios: when secrets don't exist, when the realtime platform exists but is not fully deployed, or when there are permission issues accessing the secrets. Add validation for the realtime parameter format (e.g., allowed characters, length). Implement graceful degradation so that if the realtime integration fails, the webservice can still deploy with appropriate warnings rather than failing completely, unless the integration is marked as critical.",
            "testStrategy": "Test various error scenarios including missing secrets, partially deployed realtime platforms, and invalid parameter formats. Verify that appropriate error messages are generated and that the webservice deployment behaves as expected in each case."
          },
          {
            "id": 7,
            "title": "Document the realtime integration feature",
            "description": "Update documentation and add examples to demonstrate how to use the new realtime integration feature in webservice components.",
            "status": "done",
            "dependencies": [
              6
            ],
            "details": "Add comprehensive comments in the ComponentDefinition file explaining the realtime integration feature and the ultra-minimal developer interface. Create example YAML snippets that show how to use the minimal parameter set in webservice component definitions. Document the default values and how they affect the underlying configurations. Explain how the abstraction layer works and how developers can customize behavior if needed. Include troubleshooting guidance for common issues.",
            "testStrategy": "Review the documentation for clarity and completeness. Have another team member attempt to use the feature based solely on the documentation to verify its usability."
          }
        ]
      },
      {
        "id": 4,
        "title": "Create initial realtime-compositions.yaml",
        "description": "Extend the existing application-claim-composition.yaml to add real-time streaming capabilities to the platform.",
        "status": "done",
        "dependencies": [
          2
        ],
        "priority": "high",
        "details": "Modify the existing comprehensive `/Users/socrateshlapolosa/Development/health-service-idp/crossplane/application-claim-composition.yaml` file (1000+ lines) to add real-time streaming capabilities. Add conditional logic to provision real-time infrastructure components (Kafka, MQTT Broker, Lenses HQ, Lenses Agent, and Metabase) when the realtime parameter is specified. Ensure proper integration with the existing microservice creation, database provisioning, secret management, and Knative deployment that already follows CLAUDE.md principles. Implement parameter propagation, connection secret generation, and proper dependency ordering between components.",
        "testStrategy": "Validate the modified Composition YAML syntax. Test that the updated Composition can be applied to a Kubernetes cluster with Crossplane installed. Verify that the conditional logic works correctly - real-time components should only be provisioned when the realtime parameter is specified. Test the integration between existing components and new real-time components.",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze existing application-claim-composition.yaml structure",
            "description": "Review the existing comprehensive application-claim-composition.yaml to understand its structure, component organization, and parameter handling.",
            "status": "done",
            "dependencies": [],
            "details": "Analyze the existing `/Users/socrateshlapolosa/Development/health-service-idp/crossplane/application-claim-composition.yaml` file (1000+ lines) to understand how components are organized, how parameters are propagated, and how conditional logic is implemented. Identify the appropriate places to add real-time components and determine how to integrate them with existing components. Document the key sections that will need modification and the integration points for real-time capabilities.",
            "testStrategy": "Create a detailed analysis document outlining the structure of the existing composition, the integration points for real-time components, and the approach for implementing conditional logic."
          },
          {
            "id": 2,
            "title": "Implement conditional logic for real-time components",
            "description": "Add conditional logic to the composition to provision real-time infrastructure when the realtime parameter is specified.",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "Modify the application-claim-composition.yaml to add conditional logic that checks for the presence of a realtime parameter. Implement the necessary patches and conditions to ensure real-time components are only provisioned when this parameter is set. Ensure the conditional logic follows the same patterns used elsewhere in the composition for consistency.",
            "testStrategy": "Test the conditional logic by applying the composition with and without the realtime parameter specified. Verify that real-time components are only provisioned when the parameter is set."
          },
          {
            "id": 3,
            "title": "Add Kafka and MQTT Broker components",
            "description": "Implement the Kafka and MQTT Broker components in the composition with proper configuration and conditional provisioning.",
            "status": "done",
            "dependencies": [
              2
            ],
            "details": "Add the Kafka and MQTT Broker components to the composition, ensuring they are only provisioned when the realtime parameter is specified. Configure Kafka with appropriate storage, replication, and security settings. Configure the MQTT Broker with appropriate authentication and authorization settings. Set up patches to propagate parameters from the composite resource and ensure proper integration with existing components. Set up connection secret generation for both components.",
            "testStrategy": "Validate that the Kafka and MQTT Broker components are correctly defined and that they are only provisioned when the realtime parameter is specified. Verify that the connection secrets are properly generated and that the components can be accessed by other components in the system."
          },
          {
            "id": 4,
            "title": "Add Lenses HQ and Lenses Agent components",
            "description": "Implement the Lenses HQ and Lenses Agent components in the composition with proper dependencies and configuration.",
            "status": "done",
            "dependencies": [
              3
            ],
            "details": "Add the Lenses HQ and Lenses Agent components to the composition, ensuring they are only provisioned when the realtime parameter is specified. Configure Lenses HQ to depend on the PostgreSQL database (which should already exist in the composition). Configure the Lenses Agent to connect to both Kafka and Lenses HQ using the connection secrets generated by those components. Set up patches to propagate parameters from the composite resource and ensure proper integration with existing components.",
            "testStrategy": "Verify that the Lenses HQ and Lenses Agent components are correctly defined and that they are only provisioned when the realtime parameter is specified. Test that they can properly connect to the other components in the system using the generated connection secrets."
          },
          {
            "id": 5,
            "title": "Add Metabase component and finalize integration",
            "description": "Implement the Metabase component and finalize the integration of real-time components with the existing composition.",
            "status": "done",
            "dependencies": [
              4
            ],
            "details": "Add the Metabase component to the composition, ensuring it is only provisioned when the realtime parameter is specified. Configure it to connect to the PostgreSQL database using the connection secret. Set up patches to propagate parameters from the composite resource. Finalize the integration by ensuring all real-time components have proper dependency ordering and that connection secrets are properly generated and propagated. Update the writeConnectionSecretToRef section to include the real-time component secrets in the consolidated connection secret. Ensure proper integration with existing microservice creation, database provisioning, secret management, and Knative deployment.",
            "testStrategy": "Test the complete composition by applying it to a Kubernetes cluster with Crossplane installed. Verify that all components are created in the correct order and that the connection secrets are properly generated and propagated. Test the integration between existing components and new real-time components. Verify that the consolidated connection secret contains all necessary information for external components to connect to the platform."
          },
          {
            "id": 6,
            "title": "Implement WebSocket endpoint support",
            "description": "Add support for WebSocket endpoints in the composition for real-time communication.",
            "status": "done",
            "dependencies": [
              3
            ],
            "details": "Extend the composition to support WebSocket endpoints for real-time communication. Implement the necessary components and configuration to enable WebSocket connections. Ensure proper integration with Kafka and MQTT for message passing. Set up authentication and authorization for WebSocket connections. Configure connection secret generation for WebSocket endpoints.",
            "testStrategy": "Test the WebSocket endpoint functionality by creating a test client that connects to the endpoint and exchanges messages. Verify that messages are properly routed to and from Kafka and MQTT. Test authentication and authorization for WebSocket connections."
          },
          {
            "id": 7,
            "title": "Update documentation and examples",
            "description": "Update documentation and examples to reflect the changes to the composition.",
            "status": "done",
            "dependencies": [
              6
            ],
            "details": "Update the documentation to reflect the changes to the composition. Document the new realtime parameter and how it triggers the provisioning of real-time components. Update examples to demonstrate how to use the realtime parameter and how to access the real-time components. Document the integration between existing components and new real-time components. Include troubleshooting information for common issues.",
            "testStrategy": "Review the updated documentation for accuracy and completeness. Test the examples to ensure they work as described. Have other team members review the documentation and provide feedback."
          }
        ]
      },
      {
        "id": 6,
        "title": "Create Configuration Generator Component",
        "description": "Implement the Configuration Generator component that will create the necessary configuration files for Lenses HQ and Agent.",
        "details": "In the realtime-compositions.yaml file, implement a Kubernetes Job using the busybox:latest image that generates the configuration files needed by Lenses components. Create the Lenses HQ config.yaml with HTTP settings, authentication, database connection, and license configuration. Generate the Agent lenses.conf and provisioning.yaml files. Use environment variables and ConfigMaps to store the generated configurations. Ensure the configurations are mounted as volumes that can be accessed by the Lenses components.",
        "testStrategy": "Deploy the Configuration Generator job in isolation and verify that it completes successfully. Inspect the generated configuration files to ensure they contain the correct settings. Test that the configurations can be mounted as volumes in other pods.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Kubernetes Job Structure in realtime-compositions.yaml",
            "description": "Set up the basic structure for the Configuration Generator Kubernetes Job in the realtime-compositions.yaml file using the busybox:latest image.",
            "dependencies": [],
            "details": "Add a new resource to the realtime-compositions.yaml file that creates a Kubernetes Job. Configure the Job to use the busybox:latest image with appropriate restart policy (OnFailure). Define the necessary environment variables that will be used for configuration generation, including database connection details, authentication settings, and component-specific parameters. Set up volume mounts for storing the generated configuration files. Ensure the Job has appropriate labels and annotations to identify it as part of the realtime platform.",
            "status": "done",
            "testStrategy": "Validate the YAML syntax of the Job definition. Apply the Job to a test Kubernetes cluster and verify it creates successfully without immediate errors."
          },
          {
            "id": 2,
            "title": "Implement Lenses HQ Configuration Generation Script",
            "description": "Create the script that will generate the config.yaml file for Lenses HQ with HTTP settings, authentication, database connection, and license configuration.",
            "dependencies": [
              "6.1"
            ],
            "details": "Within the Job's container command, implement a shell script that generates the Lenses HQ config.yaml file. The script should create a YAML structure with sections for: 1) HTTP server settings including port and context path, 2) Authentication configuration with user credentials, 3) Database connection details using environment variables, 4) License configuration. The generated file should be written to a mounted volume path that will be accessible as a ConfigMap. Include proper error handling and logging in the script.",
            "status": "done",
            "testStrategy": "Run the script in isolation with test environment variables to verify it produces a valid config.yaml file. Validate the generated YAML syntax and structure against Lenses HQ requirements."
          },
          {
            "id": 3,
            "title": "Implement Agent Configuration Generation",
            "description": "Create the script sections that generate the lenses.conf and provisioning.yaml files for the Lenses Agent.",
            "dependencies": [
              "6.1"
            ],
            "details": "Extend the configuration generation script to create the Lenses Agent configuration files. For lenses.conf, include connection settings to Lenses HQ, authentication details, and logging configuration. For provisioning.yaml, define the data sources (Kafka, MQTT) that the Agent will connect to, including connection strings, authentication, and monitoring settings. Write these configurations to separate files in the mounted volume path. Ensure the configurations are compatible with the Agent version being used.",
            "status": "done",
            "testStrategy": "Validate the generated Agent configuration files against the expected schema. Test with sample Agent deployment to verify the configurations are correctly recognized."
          },
          {
            "id": 4,
            "title": "Create ConfigMap Storage for Generated Configurations",
            "description": "Implement the mechanism to store the generated configuration files in Kubernetes ConfigMaps that can be mounted by other components.",
            "dependencies": [
              "6.2",
              "6.3"
            ],
            "details": "After the configuration files are generated, create a mechanism within the Job to store them in Kubernetes ConfigMaps. This could be done by either: 1) Having the Job create the ConfigMaps directly using kubectl commands, or 2) Setting up a post-job process that reads the generated files and creates ConfigMaps. Define separate ConfigMaps for Lenses HQ and Agent configurations. Include appropriate metadata and labels on the ConfigMaps to enable them to be discovered by the dependent components.",
            "status": "done",
            "testStrategy": "Verify that the ConfigMaps are created successfully after the Job runs. Check that the data in the ConfigMaps matches the expected configuration content. Test mounting the ConfigMaps in a simple pod to ensure they are accessible."
          },
          {
            "id": 5,
            "title": "Implement Volume Mounting Configuration for Lenses Components",
            "description": "Configure the volume definitions that will allow Lenses HQ and Agent pods to mount and access the generated configuration files.",
            "dependencies": [
              "6.4"
            ],
            "details": "Define the volume and volumeMount specifications that will be used by the Lenses HQ and Agent deployments to access their respective configuration files. In the realtime-compositions.yaml file, add the necessary patches to inject these volume configurations into the appropriate component resources. For each component, specify the correct mount paths where the configurations should be accessible. Include readiness probes that verify the configuration files are properly mounted before the components start.",
            "status": "done",
            "testStrategy": "Deploy test pods that use the volume configurations to verify they can access the ConfigMaps correctly. Validate that the mount paths match what the Lenses components expect. Test the complete flow from Job execution to configuration mounting in dependent components."
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement Lenses HQ Component",
        "description": "Implement the Lenses HQ component that serves as the control plane for the real-time streaming platform.",
        "details": "In the realtime-compositions.yaml file, implement the Lenses HQ component using the lensting/lenses-hq:6-preview image. Configure the deployment with the appropriate environment variables and volume mounts for the configuration files generated by the Configuration Generator. Set up the service to expose port 9991 for the Web UI. Configure health checks using the lenses-hq is-up command. Ensure the component depends on the PostgreSQL database and Configuration Generator components.",
        "testStrategy": "Deploy the Lenses HQ component and verify that it starts correctly. Test the Web UI by accessing it on port 9991. Verify that the health checks are working properly. Test the integration with the PostgreSQL database.",
        "priority": "high",
        "dependencies": [
          6
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Lenses HQ component in YAML with basic configuration",
            "description": "Create the initial component definition for Lenses HQ in the realtime-compositions.yaml file, specifying the container image, basic resource requirements, and component metadata.",
            "dependencies": [],
            "details": "Add a new component to realtime-compositions.yaml with the following configuration:\n- Use image: lensting/lenses-hq:6-preview\n- Set appropriate resource requests and limits (CPU: 500m-1000m, Memory: 1Gi-2Gi)\n- Add metadata labels for component identification\n- Define restart policy as 'Always'\n- Add initial placeholder for environment variables section",
            "status": "done",
            "testStrategy": "Validate YAML syntax and ensure the component definition is correctly structured."
          },
          {
            "id": 2,
            "title": "Configure environment variables and dependencies",
            "description": "Set up all required environment variables for Lenses HQ and define dependencies on PostgreSQL and Configuration Generator components.",
            "dependencies": [
              "7.1"
            ],
            "details": "Configure the following environment variables:\n- LENSES_DB_URL: jdbc:postgresql://postgresql:5432/hq\n- LENSES_DB_USER: from secret\n- LENSES_DB_PASSWORD: from secret\n- LENSES_HEAP_OPTS: -Xms512M -Xmx1G\n- Add any additional required environment variables\n\nDefine dependencies:\n- Add depends_on section referencing the PostgreSQL component (Task 5)\n- Add depends_on for Configuration Generator component (Task 6)",
            "status": "done",
            "testStrategy": "Verify environment variables are correctly defined and dependencies are properly specified."
          },
          {
            "id": 3,
            "title": "Configure volume mounts for configuration files",
            "description": "Set up volume mounts to access configuration files generated by the Configuration Generator component.",
            "dependencies": [
              "7.2"
            ],
            "details": "Add volume mounts section with:\n- Mount path: /opt/lenses/config\n  Source: ConfigMap or volume created by Configuration Generator\n- Mount path: /opt/lenses/secrets\n  Source: Secret containing sensitive configuration\n- Mount path: /opt/lenses/data\n  Source: PersistentVolumeClaim for data persistence\n- Ensure proper read/write permissions are set for each mount",
            "status": "done",
            "testStrategy": "Check that all required volume mounts are defined with correct paths and sources."
          },
          {
            "id": 4,
            "title": "Set up service and port configuration",
            "description": "Configure the Kubernetes service to expose Lenses HQ Web UI on port 9991 and any additional required ports.",
            "dependencies": [
              "7.1"
            ],
            "details": "Create service configuration with:\n- Service type: ClusterIP (or LoadBalancer/NodePort based on environment)\n- Port mapping: 9991:9991 for Web UI access\n- Add appropriate service labels and selectors\n- Configure any additional required ports\n- Set up annotations for service discovery if needed",
            "status": "done",
            "testStrategy": "Verify service definition exposes the correct ports and is properly configured for external access."
          },
          {
            "id": 5,
            "title": "Implement health checks and readiness probes",
            "description": "Configure liveness and readiness probes using the lenses-hq is-up command to ensure proper health monitoring.",
            "dependencies": [
              "7.2",
              "7.3",
              "7.4"
            ],
            "details": "Add health check configuration:\n- Liveness probe:\n  Command: [\"lenses-hq\", \"is-up\"]\n  Initial delay: 60 seconds\n  Period: 15 seconds\n  Timeout: 5 seconds\n  Failure threshold: 3\n- Readiness probe:\n  Command: [\"lenses-hq\", \"is-up\"]\n  Initial delay: 30 seconds\n  Period: 10 seconds\n  Success threshold: 1\n- Configure appropriate startup probe if needed",
            "status": "done",
            "testStrategy": "Deploy the component and verify that health checks are functioning correctly by monitoring pod status and logs."
          }
        ]
      },
      {
        "id": 8,
        "title": "Implement Kafka Cluster Component",
        "description": "Implement the Kafka Cluster component that provides the event streaming platform with Schema Registry and Connect.",
        "details": "In the realtime-compositions.yaml file, implement the Kafka Cluster component using the lensesio/fast-data-dev:3.9.0 image. Configure the deployment with the appropriate environment variables for Kafka listeners (PLAINTEXT:9092, DOCKERCOMPOSE:19092), Schema Registry (8081), and Connect (8083). Set up volume mounts for the Snowflake connector JAR and data persistence. Configure the RUNNING_SAMPLEDATA environment variable to enable sample data generation. Set up services to expose the necessary ports.",
        "testStrategy": "Deploy the Kafka Cluster component and verify that it starts correctly. Test Kafka connectivity by producing and consuming messages. Verify that the Schema Registry and Connect APIs are accessible. Test that the Snowflake connector is properly loaded.",
        "priority": "high",
        "dependencies": [
          4
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Kafka Cluster service in realtime-compositions.yaml",
            "description": "Create the initial service definition for the Kafka Cluster component in the realtime-compositions.yaml file, including the basic container configuration and port exposures.",
            "dependencies": [],
            "details": "Add a new service named 'kafka' to the realtime-compositions.yaml file using the lensesio/fast-data-dev:3.9.0 image. Configure the service to expose the necessary ports: 9092 (Kafka PLAINTEXT), 19092 (Kafka DOCKERCOMPOSE), 8081 (Schema Registry), and 8083 (Connect). Set up the container with restart: always policy and appropriate resource limits.",
            "status": "done",
            "testStrategy": "Verify that the service definition is syntactically correct by running a YAML validation. Check that all required ports are properly exposed."
          },
          {
            "id": 2,
            "title": "Configure Kafka environment variables",
            "description": "Set up all required environment variables for the Kafka Cluster to properly function with the correct listeners, Schema Registry, and Connect configurations.",
            "dependencies": [
              "8.1"
            ],
            "details": "Configure the following environment variables for the Kafka service:\n- ADV_HOST: '${HOSTNAME_COMMAND}'\n- KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,DOCKERCOMPOSE://localhost:19092\n- KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,DOCKERCOMPOSE://0.0.0.0:19092\n- KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,DOCKERCOMPOSE:PLAINTEXT\n- SCHEMA_REGISTRY_PORT: 8081\n- CONNECT_PORT: 8083\n- RUNNING_SAMPLEDATA: 'true'\n- RUNTESTS: 0\n- FORWARDLOGS: 0\n- SAMPLEDATA: 0",
            "status": "done",
            "testStrategy": "Validate that all environment variables are correctly defined with appropriate values. Ensure that the KAFKA_ADVERTISED_LISTENERS configuration is properly set up for both internal and external access."
          },
          {
            "id": 3,
            "title": "Set up volume mounts for Snowflake connector and data persistence",
            "description": "Configure the necessary volume mounts to ensure data persistence and to make the Snowflake connector JAR available to the Kafka Connect service.",
            "dependencies": [
              "8.1"
            ],
            "details": "Add the following volume mounts to the Kafka service:\n1. Create a named volume 'kafka-data' for data persistence and mount it to /data\n2. Create a bind mount for the Snowflake connector JAR, mapping from ./connectors/snowflake-kafka-connector.jar to /connectors/snowflake-kafka-connector.jar in the container\n3. Ensure the volumes are properly defined in the top-level volumes section of the composition file",
            "status": "done",
            "testStrategy": "Verify that the volume mounts are correctly configured by inspecting the container after deployment. Check that the Snowflake connector JAR is accessible within the container at the specified path."
          },
          {
            "id": 4,
            "title": "Implement health checks and dependency management",
            "description": "Configure health checks to ensure the Kafka Cluster is properly running and set up dependencies on other required services.",
            "dependencies": [
              "8.2",
              "8.3"
            ],
            "details": "Add health check configuration to the Kafka service using the following settings:\n- test: [\"CMD-SHELL\", \"kafka-topics --bootstrap-server localhost:9092 --list\"]\n- interval: 30s\n- timeout: 10s\n- retries: 5\n- start_period: 120s\n\nIf there are dependencies on other services (like PostgreSQL), add a 'depends_on' section with appropriate health checks to ensure proper startup order.",
            "status": "done",
            "testStrategy": "Test the health check by deploying the service and verifying that it properly reports its health status. Confirm that the service starts up in the correct order relative to its dependencies."
          },
          {
            "id": 5,
            "title": "Configure network settings and finalize the Kafka Cluster component",
            "description": "Set up the network configuration for the Kafka Cluster and finalize the component with any additional required settings.",
            "dependencies": [
              "8.4"
            ],
            "details": "Complete the Kafka Cluster configuration by:\n1. Adding the service to the appropriate network in the composition file\n2. Setting up any required labels for service discovery\n3. Adding any additional configuration parameters needed for production readiness\n4. Ensuring the component is properly integrated with the rest of the system\n5. Adding comments to document the purpose and configuration of the Kafka Cluster component\n\nFinalize by validating the complete configuration against the requirements.",
            "status": "done",
            "testStrategy": "Deploy the complete Kafka Cluster component and verify that it starts correctly. Test Kafka connectivity by producing and consuming messages. Verify that the Schema Registry and Connect APIs are accessible on ports 8081 and 8083 respectively. Test that the Snowflake connector is properly loaded by listing available connectors through the Connect API."
          }
        ]
      },
      {
        "id": 9,
        "title": "Implement MQTT Broker Component",
        "description": "Implement the MQTT Broker component that provides IoT device connectivity using Eclipse Mosquitto.",
        "details": "In the realtime-compositions.yaml file, implement the MQTT Broker component using the eclipse-mosquitto:latest image. Configure the deployment with volume mounts for mosquitto.conf, data, and logs. Set up the service to expose ports 1883 (MQTT) and 9001 (WebSockets). Configure authentication with the default user credentials (user1/password) or custom credentials from the mqttUsers parameter. Set up health checks and persistence for the MQTT broker.",
        "testStrategy": "Deploy the MQTT Broker component and verify that it starts correctly. Test MQTT connectivity by publishing and subscribing to topics. Verify that authentication is working properly. Test WebSocket connectivity on port 9001.",
        "priority": "high",
        "dependencies": [
          4
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Mosquitto configuration files",
            "description": "Create the necessary configuration files for the Eclipse Mosquitto MQTT broker, including mosquitto.conf with authentication settings, password file, and ACL configuration.",
            "dependencies": [],
            "details": "Create a ConfigMap in the realtime-compositions.yaml file that contains the mosquitto.conf configuration file. The configuration should enable password-based authentication, set up listeners for MQTT (1883) and WebSockets (9001), and configure logging. Also include a password file generator script that will create the password file during initialization based on the mqttUsers parameter or default to user1/password if not specified.",
            "status": "done",
            "testStrategy": "Verify the ConfigMap is created correctly with all required configuration files. Check that the mosquitto.conf file has the correct settings for authentication, listeners, and logging."
          },
          {
            "id": 2,
            "title": "Implement MQTT Broker deployment",
            "description": "Create the Deployment resource for the Eclipse Mosquitto MQTT broker using the eclipse-mosquitto:latest image with appropriate resource limits and environment variables.",
            "dependencies": [
              "9.1"
            ],
            "details": "In the realtime-compositions.yaml file, create a Deployment for the MQTT Broker using the eclipse-mosquitto:latest image. Configure resource requests and limits appropriate for the broker. Set up volume mounts for the configuration files created in the previous subtask, as well as volumes for data persistence (/mosquitto/data) and logs (/mosquitto/log). Add an init container that will generate the password file using the script from the ConfigMap based on the mqttUsers parameter.",
            "status": "done",
            "testStrategy": "Deploy the MQTT Broker and verify that the pods start correctly. Check that the volume mounts are correctly configured and that the init container successfully generates the password file."
          },
          {
            "id": 3,
            "title": "Configure MQTT Broker service and networking",
            "description": "Create the Service resource to expose the MQTT Broker on ports 1883 (MQTT) and 9001 (WebSockets), and configure any necessary network policies.",
            "dependencies": [
              "9.2"
            ],
            "details": "In the realtime-compositions.yaml file, create a Service resource for the MQTT Broker that exposes port 1883 for MQTT traffic and port 9001 for WebSockets. Configure the service type as ClusterIP for internal access, and optionally as NodePort or LoadBalancer if external access is required. Add appropriate labels and selectors to match the Deployment created in the previous subtask.",
            "status": "done",
            "testStrategy": "Verify that the Service is created correctly and that the ports are exposed as expected. Test connectivity to the MQTT Broker from within the cluster using a simple MQTT client."
          },
          {
            "id": 4,
            "title": "Implement health checks and persistence",
            "description": "Configure health checks (liveness and readiness probes) for the MQTT Broker and set up persistent storage for MQTT data and logs.",
            "dependencies": [
              "9.2"
            ],
            "details": "Add liveness and readiness probes to the MQTT Broker Deployment to ensure proper health monitoring. The probes should check that the MQTT port (1883) is open and accepting connections. Configure PersistentVolumeClaims for the data and logs directories to ensure data persistence across pod restarts. Update the volume mounts in the Deployment to use these PVCs instead of emptyDir volumes.",
            "status": "done",
            "testStrategy": "Test the health checks by deploying the MQTT Broker and verifying that the probes succeed. Simulate a failure condition and verify that Kubernetes restarts the pod. Test persistence by publishing messages, restarting the pod, and verifying that subscriptions and retained messages are still available."
          },
          {
            "id": 5,
            "title": "Document and test MQTT authentication",
            "description": "Document the authentication mechanism and test that the MQTT Broker correctly enforces authentication with the configured credentials.",
            "dependencies": [
              "9.1",
              "9.2",
              "9.3",
              "9.4"
            ],
            "details": "Create documentation in the form of comments in the realtime-compositions.yaml file that explains how to connect to the MQTT Broker, including authentication requirements. Include examples of how to connect using common MQTT clients like mosquitto_pub/sub, MQTT.js, or Paho. Implement a test script that verifies authentication is working by attempting to connect with valid and invalid credentials.",
            "status": "done",
            "testStrategy": "Run the test script to verify that authentication is working correctly. Test publishing and subscribing to topics with valid credentials. Verify that connection attempts with invalid credentials are rejected. Test WebSocket connectivity on port 9001 using a browser-based MQTT client."
          }
        ]
      },
      {
        "id": 10,
        "title": "Implement Lenses Agent Component",
        "description": "Implement the Lenses Agent component that provides the data plane for stream processing and connector management.",
        "details": "In the realtime-compositions.yaml file, implement the Lenses Agent component using the lensting/lenses-agent:6-preview image. Configure the deployment with the appropriate environment variables for connecting to Lenses HQ (DEMO_HQ_URL, DEMO_HQ_USER, DEMO_HQ_PASSWORD) and heap settings (LENSES_HEAP_OPTS). Set up volume mounts for the shared settings directory. Configure the component to depend on Lenses HQ, PostgreSQL, and Kafka components. Set up health checks and service discovery.",
        "testStrategy": "Deploy the Lenses Agent component and verify that it starts correctly. Test the connection to Lenses HQ by checking the agent status in the HQ UI. Verify that the agent can manage connectors and process streams.",
        "priority": "high",
        "dependencies": [
          7,
          8
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Lenses Agent component in realtime-compositions.yaml",
            "description": "Create the initial component definition for Lenses Agent in the realtime-compositions.yaml file, specifying the container image and basic configuration.",
            "dependencies": [],
            "details": "Add a new component to the realtime-compositions.yaml file with the following configuration:\n- Component name: lenses-agent\n- Image: lensting/lenses-agent:6-preview\n- Container port: 8080\n- Basic resource limits (CPU: 500m, Memory: 1Gi)\n- Set restart policy to Always\n- Add labels for component identification and version tracking",
            "status": "done",
            "testStrategy": "Validate the YAML syntax after adding the component definition. Ensure the component appears in the composition without deployment errors."
          },
          {
            "id": 2,
            "title": "Configure environment variables for Lenses HQ connectivity",
            "description": "Set up the required environment variables for the Lenses Agent to connect to Lenses HQ and configure heap settings.",
            "dependencies": [
              "10.1"
            ],
            "details": "Add the following environment variables to the Lenses Agent component:\n- DEMO_HQ_URL: Set to the service URL of the Lenses HQ component (e.g., http://lenses-hq:8080)\n- DEMO_HQ_USER: Set to the admin username for Lenses HQ\n- DEMO_HQ_PASSWORD: Reference this from a secret or set directly based on project requirements\n- LENSES_HEAP_OPTS: Set to '-Xms512M -Xmx1G' for initial heap configuration\n- Add any additional required environment variables for agent configuration",
            "status": "done",
            "testStrategy": "Verify that environment variables are correctly defined in the YAML. Test that the agent can connect to HQ using these credentials after deployment."
          },
          {
            "id": 3,
            "title": "Configure volume mounts and shared settings",
            "description": "Set up the necessary volume mounts for the Lenses Agent, including the shared settings directory for configuration files.",
            "dependencies": [
              "10.1"
            ],
            "details": "Configure the following volume mounts for the Lenses Agent component:\n- Create a volume for the shared settings directory (e.g., /opt/lenses/shared-settings)\n- Mount this volume to the container at the appropriate path\n- Ensure the volume has the correct permissions (ReadWriteMany if using PVC)\n- Configure any additional required volumes for logs or data persistence\n- Set up volume claims or emptyDir volumes as appropriate for the deployment environment",
            "status": "done",
            "testStrategy": "Verify that volumes are correctly mounted after deployment. Check that the agent can read from and write to the shared settings directory."
          },
          {
            "id": 4,
            "title": "Set up component dependencies and health checks",
            "description": "Configure the Lenses Agent component to depend on Lenses HQ, PostgreSQL, and Kafka components, and implement health checks for service readiness and liveness.",
            "dependencies": [
              "10.2",
              "10.3"
            ],
            "details": "Add the following configurations to the Lenses Agent component:\n- Define explicit dependencies on Lenses HQ, PostgreSQL, and Kafka components\n- Configure readiness probe:\n  - HTTP GET endpoint: /api/v1/health or appropriate health endpoint\n  - Initial delay: 30 seconds\n  - Period: 10 seconds\n  - Timeout: 5 seconds\n- Configure liveness probe with similar parameters but longer initial delay\n- Set appropriate failure thresholds for both probes",
            "status": "done",
            "testStrategy": "Test the component dependencies by deploying the full stack. Verify that the agent starts only after its dependencies are ready. Test health checks by monitoring the readiness and liveness status."
          },
          {
            "id": 5,
            "title": "Implement service discovery and network configuration",
            "description": "Set up service discovery for the Lenses Agent component and configure network settings for communication with other components.",
            "dependencies": [
              "10.4"
            ],
            "details": "Configure the following for service discovery and networking:\n- Create a Kubernetes Service for the Lenses Agent (type: ClusterIP)\n- Expose the appropriate ports (e.g., 8080 for API access)\n- Set up any required annotations for service discovery\n- Configure network policies to allow communication between:\n  - Lenses Agent and Lenses HQ\n  - Lenses Agent and PostgreSQL\n  - Lenses Agent and Kafka\n- Add any required labels for service mesh integration if applicable",
            "status": "done",
            "testStrategy": "Test service discovery by verifying that other components can reach the Lenses Agent using its service name. Verify network connectivity between all dependent components."
          }
        ]
      },
      {
        "id": 11,
        "title": "Implement MQTT Source Connector Configuration",
        "description": "Configure the MQTT Source Connector that bridges data from the MQTT broker to Kafka topics.",
        "details": "In the realtime-compositions.yaml file, implement the configuration for the MQTT Source Connector using the Lenses Stream Reactor connector. Configure the connector with the appropriate settings for connecting to the MQTT broker (connect_mqtt_hosts: tcp://mqtt5:1883) and mapping MQTT topics to Kafka topics (connect_mqtt_kcql: INSERT INTO device_data SELECT * FROM health/device_data WITHKEY(deviceId)). Set up authentication, quality of service, and error handling. Ensure the connector is deployed and managed by the Lenses Agent.",
        "testStrategy": "Deploy the MQTT Source Connector configuration and verify that it is created correctly. Test the connector by publishing messages to the MQTT topic and verifying that they appear in the Kafka topic. Test error handling by simulating connection failures.",
        "priority": "medium",
        "dependencies": [
          9,
          10
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define MQTT Source Connector Basic Configuration",
            "description": "Create the initial configuration for the MQTT Source Connector in the realtime-compositions.yaml file, including the connector name, class, and basic settings.",
            "dependencies": [],
            "details": "Add a new connector configuration section in the realtime-compositions.yaml file under the Lenses Agent component. Define the connector with name 'mqtt-source', class 'com.datamountaineer.streamreactor.connect.mqtt.source.MqttSourceConnector', and set the basic configuration parameters including 'connect.mqtt.hosts: tcp://mqtt5:1883' and 'tasks.max: 1'. Ensure the connector is managed by the Lenses Agent by setting the appropriate deployment settings.",
            "status": "done",
            "testStrategy": "Verify that the connector configuration is syntactically correct by validating the YAML file. Check that the connector appears in the Lenses UI after deployment."
          },
          {
            "id": 2,
            "title": "Configure MQTT Topic to Kafka Topic Mapping",
            "description": "Implement the KCQL (Kafka Connect Query Language) configuration to map MQTT topics to Kafka topics with appropriate key extraction.",
            "dependencies": [
              "11.1"
            ],
            "details": "Add the 'connect.mqtt.kcql' parameter to the connector configuration with the value 'INSERT INTO device_data SELECT * FROM health/device_data WITHKEY(deviceId)'. This maps the MQTT topic 'health/device_data' to the Kafka topic 'device_data' and uses the 'deviceId' field as the message key. Ensure the Kafka topic exists or is set to be auto-created.",
            "status": "done",
            "testStrategy": "Test the mapping by publishing a sample message to the MQTT topic 'health/device_data' with a deviceId field and verify it appears in the Kafka topic 'device_data' with the correct key."
          },
          {
            "id": 3,
            "title": "Implement MQTT Authentication and Security Settings",
            "description": "Configure the authentication credentials and security settings for connecting to the MQTT broker.",
            "dependencies": [
              "11.1"
            ],
            "details": "Add authentication parameters to the connector configuration including 'connect.mqtt.username' and 'connect.mqtt.password' with values matching the credentials configured in the MQTT broker (Task 9). Set the 'connect.mqtt.clean.session' to 'true' and configure 'connect.mqtt.connection.timeout' and 'connect.mqtt.keep.alive.interval' with appropriate values (e.g., 30000ms and 60000ms respectively).",
            "status": "done",
            "testStrategy": "Test authentication by ensuring the connector can successfully connect to the MQTT broker. Verify connection by checking connector status and logs for authentication failures."
          },
          {
            "id": 4,
            "title": "Configure Quality of Service and Error Handling",
            "description": "Set up the quality of service level and error handling mechanisms for the MQTT Source Connector.",
            "dependencies": [
              "11.2",
              "11.3"
            ],
            "details": "Configure 'connect.mqtt.qos' parameter to an appropriate QoS level (0, 1, or 2, with 1 recommended for balanced reliability). Add error handling parameters including 'connect.mqtt.connection.clean': true, 'connect.mqtt.throw.on.error': false, and 'connect.mqtt.retry.backoff.ms': 3000. Set up 'errors.tolerance' to 'all' and 'errors.log.enable' to 'true' to ensure errors are logged but don't stop the connector.",
            "status": "done",
            "testStrategy": "Test error handling by temporarily disabling the MQTT broker and verifying the connector attempts to reconnect. Check logs for appropriate error messages and verify the connector recovers when the broker is available again."
          },
          {
            "id": 5,
            "title": "Implement Connector Monitoring and Validation",
            "description": "Configure monitoring settings and validate the complete MQTT Source Connector configuration.",
            "dependencies": [
              "11.4"
            ],
            "details": "Add monitoring parameters to track connector performance, including 'connect.mqtt.kcql.monitoring.enabled': true and appropriate JMX metrics settings. Review the complete connector configuration to ensure all required parameters are set correctly. Add comments in the YAML file to document the purpose of each configuration section. Ensure the connector is properly integrated with the Lenses Agent component defined in Task 10.",
            "status": "done",
            "testStrategy": "Deploy the complete MQTT Source Connector configuration and verify it starts successfully. Monitor the connector metrics through JMX or Lenses UI. Perform an end-to-end test by publishing messages to the MQTT topic and confirming they flow through to Kafka with the expected format and key."
          }
        ]
      },
      {
        "id": 12,
        "title": "Implement Stream Processing Queries",
        "description": "Configure the Lenses SQL queries for data decomposition and topic routing.",
        "details": "In the realtime-compositions.yaml file, implement the configuration for the Lenses SQL queries that transform data from the device_data topic to specialized topics (blood_pressure_device_topic, heart_rate_device_topic, oxygen_saturation_device_topic, temperature_device_topic). Use the SQL query templates provided in the PRD. Configure the queries to use the appropriate Avro schemas and key formats. Ensure the queries are deployed and managed by the Lenses Agent.",
        "testStrategy": "Deploy the Stream Processing Queries configuration and verify that the queries are created correctly. Test the queries by publishing messages to the device_data topic and verifying that they are transformed and routed to the specialized topics. Verify that the Avro schemas are applied correctly.",
        "priority": "medium",
        "dependencies": [
          10,
          11
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create realtime-compositions.yaml file structure for Lenses SQL queries",
            "description": "Set up the basic structure in the realtime-compositions.yaml file for implementing the Lenses SQL queries configuration. This includes defining the necessary sections for stream processing and establishing the connection to the device_data topic.",
            "dependencies": [],
            "details": "Create or update the realtime-compositions.yaml file with a dedicated section for Lenses SQL queries. Define the configuration structure that will contain the four transformation queries. Include the connection parameters to the Kafka cluster and ensure the device_data topic is properly referenced as the source. Set up the basic configuration that will be common across all queries, such as error handling and processing guarantees.",
            "status": "done",
            "testStrategy": "Validate the YAML file syntax using a YAML linter. Verify that the configuration structure is correct and compatible with the Lenses Agent requirements."
          },
          {
            "id": 2,
            "title": "Implement blood pressure and heart rate transformation queries",
            "description": "Configure the first two SQL queries that transform data from the device_data topic to the blood_pressure_device_topic and heart_rate_device_topic, following the templates from the PRD.",
            "dependencies": [
              "12.1"
            ],
            "details": "Implement the SQL queries for blood pressure and heart rate data transformation. For the blood pressure query, extract systolic and diastolic readings from the device_data topic and route them to blood_pressure_device_topic. For the heart rate query, extract pulse rate measurements and route them to heart_rate_device_topic. Configure both queries to use the appropriate Avro schemas for the output topics and set the correct key formats based on patient and device identifiers. Include proper error handling and data validation in the queries.",
            "status": "done",
            "testStrategy": "Test each query individually by publishing sample blood pressure and heart rate data to the device_data topic and verifying that the data is correctly transformed and routed to the respective specialized topics. Validate that the Avro schema is correctly applied to the output messages."
          },
          {
            "id": 3,
            "title": "Implement oxygen saturation and temperature transformation queries",
            "description": "Configure the remaining two SQL queries that transform data from the device_data topic to the oxygen_saturation_device_topic and temperature_device_topic, following the templates from the PRD.",
            "dependencies": [
              "12.1"
            ],
            "details": "Implement the SQL queries for oxygen saturation and temperature data transformation. For the oxygen saturation query, extract SpO2 readings from the device_data topic and route them to oxygen_saturation_device_topic. For the temperature query, extract temperature measurements and route them to temperature_device_topic. Configure both queries to use the appropriate Avro schemas for the output topics and set the correct key formats based on patient and device identifiers. Include proper error handling and data validation in the queries.",
            "status": "done",
            "testStrategy": "Test each query individually by publishing sample oxygen saturation and temperature data to the device_data topic and verifying that the data is correctly transformed and routed to the respective specialized topics. Validate that the Avro schema is correctly applied to the output messages."
          },
          {
            "id": 4,
            "title": "Configure Lenses Agent integration for query deployment",
            "description": "Set up the configuration to ensure the Lenses Agent properly deploys and manages the SQL queries. This includes specifying deployment parameters, resource allocation, and monitoring settings.",
            "dependencies": [
              "12.2",
              "12.3"
            ],
            "details": "Configure the Lenses Agent integration for the SQL queries by specifying the deployment parameters such as parallelism, checkpoint intervals, and state store configuration. Set appropriate resource allocation (CPU, memory) for each query based on expected throughput. Configure monitoring and alerting for the queries, including metrics collection and error reporting. Ensure the queries are automatically deployed when the Lenses Agent starts and that they can be updated without service interruption.",
            "status": "done",
            "testStrategy": "Deploy the configuration and verify in the Lenses HQ UI that the queries are properly registered with the Lenses Agent. Check that the queries have the correct resource allocation and monitoring settings. Test the deployment process by making a small change to a query and verifying that it is updated without service interruption."
          },
          {
            "id": 5,
            "title": "Implement error handling and validation for all queries",
            "description": "Enhance all SQL queries with comprehensive error handling, data validation, and logging to ensure robust operation in production.",
            "dependencies": [
              "12.2",
              "12.3",
              "12.4"
            ],
            "details": "Implement error handling for all queries to manage scenarios such as malformed input data, schema evolution, and downstream connectivity issues. Add data validation logic to ensure only valid measurements are processed and routed. Configure dead-letter queues for messages that fail processing. Implement detailed logging for query execution, including processing rates, latency metrics, and error counts. Set up alerting thresholds for critical error conditions. Ensure all queries have consistent error handling patterns and logging formats.",
            "status": "done",
            "testStrategy": "Test error handling by publishing malformed data to the device_data topic and verifying that it is properly handled without crashing the queries. Verify that invalid data is sent to the dead-letter queue with appropriate error context. Check that logs contain sufficient information for troubleshooting. Simulate downstream topic unavailability and verify that the queries handle the situation gracefully."
          }
        ]
      },
      {
        "id": 13,
        "title": "Implement Metabase Analytics Dashboard",
        "description": "Implement the Metabase component that provides real-time analytics dashboards for health data.",
        "details": "In the realtime-compositions.yaml file, implement the Metabase component using the metabase/metabase:latest image. Configure the deployment with the appropriate environment variables for connecting to the PostgreSQL database. Set up the service to expose port 3000 for the Web UI. Configure health checks and persistence. Set up the component to depend on the PostgreSQL database. Configure Metabase to connect to Snowflake if the snowflake parameter is enabled.",
        "testStrategy": "Deploy the Metabase component and verify that it starts correctly. Test the Web UI by accessing it on port 3000. Verify that Metabase can connect to the PostgreSQL database. Test the Snowflake connection if enabled.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Metabase Component Definition to YAML",
            "description": "Create the basic Metabase component definition in the realtime-compositions.yaml file using the metabase/metabase:latest image with appropriate resource allocations and container settings.",
            "dependencies": [],
            "details": "Add a new component section in realtime-compositions.yaml with name 'metabase'. Use metabase/metabase:latest as the container image. Configure basic container settings including resource requests and limits (recommend 1-2 CPU, 2-4Gi memory). Set restart policy to 'Always' and add appropriate labels for the component.",
            "status": "done",
            "testStrategy": "Validate the YAML syntax using a linter. Ensure the component definition follows the project's component structure pattern."
          },
          {
            "id": 2,
            "title": "Configure Metabase Environment Variables for PostgreSQL Connection",
            "description": "Set up the required environment variables to enable Metabase to connect to the PostgreSQL database component.",
            "dependencies": [
              "13.1"
            ],
            "details": "Add environment variables section to the Metabase component with: MB_DB_TYPE=postgres, MB_DB_DBNAME=metabaseappdb, MB_DB_PORT=5432, MB_DB_USER and MB_DB_PASS referencing PostgreSQL credentials from the PostgreSQL component's secret. Add MB_DB_HOST pointing to the PostgreSQL service name. Include dependency configuration to ensure Metabase starts after PostgreSQL is ready.",
            "status": "done",
            "testStrategy": "Verify environment variables are correctly formatted and reference the appropriate PostgreSQL secrets and service names."
          },
          {
            "id": 3,
            "title": "Implement Metabase Service and Port Configuration",
            "description": "Configure the Kubernetes service to expose Metabase's web interface on port 3000 and set up any additional required networking configurations.",
            "dependencies": [
              "13.1"
            ],
            "details": "Create a Kubernetes Service definition for Metabase that exposes port 3000 for the web UI. Configure the service type as ClusterIP for internal access or LoadBalancer/NodePort if external access is required. Add appropriate selector labels to match the Metabase pod. Include annotations for any service mesh integration if applicable.",
            "status": "done",
            "testStrategy": "Verify the service configuration correctly maps to port 3000 and uses the right selectors to target the Metabase pods."
          },
          {
            "id": 4,
            "title": "Set Up Metabase Persistence and Health Checks",
            "description": "Configure persistent volume claims for Metabase data and implement health checks to ensure proper operation and reliability.",
            "dependencies": [
              "13.1",
              "13.2"
            ],
            "details": "Add a persistent volume claim for Metabase data storage (recommend at least 5Gi). Mount the PVC to /metabase-data in the container. Configure both liveness and readiness probes: use an HTTP GET probe on /api/health endpoint at port 3000 with appropriate initial delay (60s), timeout (5s), and period (10s) settings. Add appropriate failure thresholds to prevent premature restarts during initial database setup.",
            "status": "done",
            "testStrategy": "Test that the persistent volume is correctly mounted and that health checks properly detect Metabase availability without causing restart loops during initialization."
          },
          {
            "id": 5,
            "title": "Implement Conditional Snowflake Connection Configuration",
            "description": "Add configuration to conditionally connect Metabase to Snowflake when the snowflake parameter is enabled in the deployment.",
            "dependencies": [
              "13.2",
              "13.3",
              "13.4"
            ],
            "details": "Implement conditional logic in the component definition that checks for a 'snowflake.enabled' parameter. When enabled, add additional environment variables for Snowflake connection: MB_SNOWFLAKE_JDBC_URL with the Snowflake connection string, MB_SNOWFLAKE_USER and MB_SNOWFLAKE_PASSWORD referencing credentials from a Kubernetes secret. If using External Secrets Operator, configure the appropriate SecretStore and ExternalSecret resources to fetch Snowflake credentials from the external secret manager.",
            "status": "done",
            "testStrategy": "Test the conditional configuration by deploying with snowflake.enabled=false and verify no Snowflake configuration is present. Then deploy with snowflake.enabled=true and verify the Snowflake connection environment variables are correctly configured."
          }
        ]
      },
      {
        "id": 14,
        "title": "Implement Snowflake Sink Connector Configuration",
        "description": "Configure the Snowflake Sink Connector that sends processed data to Snowflake for analytics.",
        "details": "In the realtime-compositions.yaml file, implement the configuration for the Snowflake Sink Connector. Configure the connector with the appropriate settings for connecting to Snowflake (snowflake_url_name, snowflake_user_name, snowflake_database_name, snowflake_schema_name) and mapping Kafka topics to Snowflake tables. Set up authentication using the credentials from the snowflake parameter. Configure buffer settings and Avro conversion. Ensure the connector is deployed and managed by the Lenses Agent. Make this component conditional based on the snowflake.enabled parameter.",
        "testStrategy": "Deploy the Snowflake Sink Connector configuration and verify that it is created correctly. Test the connector by publishing messages to the Kafka topics and verifying that they appear in Snowflake tables. Test error handling and buffer settings.",
        "priority": "low",
        "dependencies": [
          10,
          12
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Snowflake Sink Connector Base Configuration",
            "description": "Create the basic configuration structure for the Snowflake Sink Connector in the realtime-compositions.yaml file, including component name, image, and conditional deployment based on snowflake.enabled parameter.",
            "dependencies": [],
            "details": "Add a new component to realtime-compositions.yaml with name 'snowflake-sink-connector'. Use the Kafka Connect framework with the Snowflake connector plugin. Make the component conditional by adding a condition that checks if snowflake.enabled is true. Include basic deployment configuration with resource limits and requests. Set up the component to be managed by the Lenses Agent.",
            "status": "pending",
            "testStrategy": "Verify that the component is defined correctly in the YAML file and that it is only deployed when snowflake.enabled is set to true."
          },
          {
            "id": 2,
            "title": "Configure Snowflake Connection Parameters",
            "description": "Set up the connection parameters for Snowflake, including URL, user, database, schema, and authentication credentials from the snowflake parameter.",
            "dependencies": [
              "14.1"
            ],
            "details": "Configure environment variables or connector properties for Snowflake connection: SNOWFLAKE_URL_NAME, SNOWFLAKE_USER_NAME, SNOWFLAKE_DATABASE_NAME, and SNOWFLAKE_SCHEMA_NAME. Set up authentication using credentials from the snowflake parameter, referencing them as secrets. Include proper error handling for connection failures and implement retry logic.",
            "status": "pending",
            "testStrategy": "Test the connection to Snowflake by deploying the connector with test credentials and verifying that it can establish a connection successfully."
          },
          {
            "id": 3,
            "title": "Implement Kafka Topic to Snowflake Table Mapping",
            "description": "Configure the mapping between Kafka topics and Snowflake tables, including topic patterns, table naming conventions, and schema handling.",
            "dependencies": [
              "14.2"
            ],
            "details": "Define the topics to be consumed by the connector using either specific topic names or patterns. Configure the table naming strategy to map Kafka topics to Snowflake tables (e.g., using the same name or a prefix/suffix). Set up schema handling to ensure that Kafka message schemas are properly translated to Snowflake table schemas. Configure the connector to handle schema evolution appropriately.",
            "status": "pending",
            "testStrategy": "Test the mapping by publishing messages to Kafka topics and verifying that they appear in the correct Snowflake tables with the expected schema."
          },
          {
            "id": 4,
            "title": "Configure Buffer Settings and Data Conversion",
            "description": "Set up buffer settings for the connector to handle data flow efficiently and configure Avro conversion for data serialization.",
            "dependencies": [
              "14.3"
            ],
            "details": "Configure buffer settings including batch size, flush interval, and retry parameters to optimize performance and reliability. Set up Avro conversion for data serialization, ensuring compatibility with the Kafka Schema Registry. Configure error handling for data conversion issues. Set appropriate buffer limits to prevent memory issues during high load.",
            "status": "pending",
            "testStrategy": "Test buffer settings by sending large volumes of data and verifying that the connector handles it efficiently. Test error scenarios by sending malformed data and verifying that error handling works correctly."
          },
          {
            "id": 5,
            "title": "Implement Monitoring and Integration with Lenses Agent",
            "description": "Configure monitoring for the Snowflake Sink Connector and ensure proper integration with the Lenses Agent for management and observability.",
            "dependencies": [
              "14.4"
            ],
            "details": "Set up JMX metrics for the connector to monitor performance and health. Configure logging with appropriate levels for troubleshooting. Ensure the connector is properly registered with the Lenses Agent for management. Set up health checks to monitor the connector's status. Configure alerts for critical issues such as connection failures or data processing errors.",
            "status": "pending",
            "testStrategy": "Verify that the connector metrics are visible in the Lenses HQ UI. Test failure scenarios and verify that alerts are triggered appropriately. Check that the Lenses Agent can properly manage the connector lifecycle (start, stop, restart)."
          }
        ]
      },
      {
        "id": 15,
        "title": "Implement Secret Generation and Management",
        "description": "Implement the generation and management of connection secrets for all components in the real-time platform.",
        "details": "In the realtime-compositions.yaml file, implement the logic for generating and managing connection secrets for all components. Create standardized secret names following the {name}-{service}-secret pattern. Include all necessary connection details (host, port, database, user, password) for each service. Configure the secrets to be exposed in the RealtimePlatformClaim status. Implement integration with External Secrets Operator for Snowflake credentials if the snowflake parameter is enabled.",
        "testStrategy": "Deploy the Secret Generation and Management logic and verify that all secrets are created correctly. Test that the secrets contain the correct connection details. Verify that the secrets are exposed in the RealtimePlatformClaim status. Test the integration with External Secrets Operator if applicable.",
        "priority": "high",
        "dependencies": [
          7,
          8,
          9,
          10,
          13
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Secret Naming Convention and Structure",
            "description": "Establish a standardized naming convention and structure for all connection secrets in the real-time platform following the {name}-{service}-secret pattern. Define the required fields for each type of service (PostgreSQL, Kafka, MQTT, etc.).",
            "dependencies": [],
            "details": "In the realtime-compositions.yaml file, create a helper function that generates secret names consistently. Define templates for different service types with all necessary connection fields: PostgreSQL (host, port, database, user, password), Kafka (bootstrap servers, schema registry URL), MQTT (broker URL, credentials), etc. Document the naming convention and field structure in comments.",
            "status": "done",
            "testStrategy": "Create unit tests for the secret naming function to verify it produces the expected format. Validate that all required fields are included in the secret templates for each service type."
          },
          {
            "id": 2,
            "title": "Implement Secret Generation for Database Components",
            "description": "Implement the generation of connection secrets for PostgreSQL database components, ensuring they contain all necessary connection details.",
            "dependencies": [
              "15.1"
            ],
            "details": "In the realtime-compositions.yaml file, implement the logic to generate secrets for PostgreSQL database components. Extract connection details (host, port, database, user, password) from the PostgreSQL component definition. Create Kubernetes Secret resources with these details using the standardized naming pattern. Ensure the secrets are properly labeled and annotated for discovery.",
            "status": "done",
            "testStrategy": "Deploy a PostgreSQL component and verify that the corresponding secret is created with the correct name and contains all required connection details. Test connecting to the database using the information in the secret."
          },
          {
            "id": 3,
            "title": "Implement Secret Generation for Messaging Components",
            "description": "Implement the generation of connection secrets for Kafka and MQTT broker components, ensuring they contain all necessary connection details.",
            "dependencies": [
              "15.1"
            ],
            "details": "In the realtime-compositions.yaml file, implement the logic to generate secrets for Kafka and MQTT components. For Kafka, include bootstrap servers, schema registry URL, and any authentication details. For MQTT, include broker URL, ports (1883 for MQTT, 9001 for WebSockets), and authentication credentials. Create Kubernetes Secret resources with these details using the standardized naming pattern.",
            "status": "done",
            "testStrategy": "Deploy Kafka and MQTT components and verify that the corresponding secrets are created with the correct names and contain all required connection details. Test connecting to both services using the information in the secrets."
          },
          {
            "id": 4,
            "title": "Expose Secrets in RealtimePlatformClaim Status",
            "description": "Configure the generated secrets to be exposed in the RealtimePlatformClaim status to make them discoverable by applications.",
            "dependencies": [
              "15.2",
              "15.3"
            ],
            "details": "Modify the RealtimePlatformClaim controller to collect all generated secrets and expose their names in the status field. Create a structured format in the status that organizes secrets by service type (database, messaging, etc.). Include secret names, service types, and optionally non-sensitive connection details like hostnames and ports. Ensure sensitive information like passwords is not exposed in the status.",
            "status": "done",
            "testStrategy": "Create a RealtimePlatformClaim and verify that its status correctly reflects all the generated secrets. Check that the information is properly structured and that no sensitive data is exposed."
          },
          {
            "id": 5,
            "title": "Implement External Secrets Operator Integration for Snowflake",
            "description": "Implement integration with External Secrets Operator to manage Snowflake credentials when the snowflake parameter is enabled.",
            "dependencies": [
              "15.1"
            ],
            "details": "In the realtime-compositions.yaml file, add conditional logic that checks if the snowflake parameter is enabled. If enabled, create an ExternalSecret resource that references Snowflake credentials stored in an external secrets manager. Configure the ExternalSecret to create a Kubernetes Secret with the standardized naming pattern. Include all necessary Snowflake connection details (account, username, password, role, warehouse, database). Ensure the secret is properly labeled and annotated for discovery.",
            "status": "done",
            "testStrategy": "Enable the snowflake parameter and verify that the ExternalSecret resource is created correctly. Confirm that the resulting Kubernetes Secret contains all required Snowflake connection details. Test the Snowflake connector using these credentials."
          }
        ]
      },
      {
        "id": 16,
        "title": "Create MINIMAL-REALTIME-OAM.yaml Example",
        "description": "Create a minimal example OAM application that demonstrates the use of the realtime-platform component with default settings.",
        "details": "Create a new file at `/Users/socrateshlapolosa/Development/health-service-idp/MINIMAL-REALTIME-OAM.yaml`. Implement a minimal OAM application that uses the realtime-platform component with default settings. Include a webservice component that references the realtime-platform using the realtime parameter. Use the example from the PRD as a reference. Ensure the example is simple and focuses on the minimal required configuration.",
        "testStrategy": "Validate the YAML syntax. Test that the example can be applied to a KubeVela environment. Verify that the realtime-platform and webservice components are created correctly. Test the integration between the components.",
        "priority": "medium",
        "dependencies": [
          1,
          3,
          4,
          15
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create basic OAM application structure",
            "description": "Set up the basic structure of the MINIMAL-REALTIME-OAM.yaml file with the required OAM application components and metadata.",
            "dependencies": [],
            "details": "Create a new file at `/Users/socrateshlapolosa/Development/health-service-idp/MINIMAL-REALTIME-OAM.yaml`. Initialize the file with the standard OAM application structure including apiVersion, kind, metadata, and spec sections. Set the application name to 'minimal-realtime-app'. Include placeholders for components that will be defined in subsequent steps.",
            "status": "done",
            "testStrategy": "Validate the YAML syntax using a YAML linter to ensure proper structure and indentation."
          },
          {
            "id": 2,
            "title": "Define realtime-platform component",
            "description": "Add the realtime-platform component definition to the OAM application with default settings.",
            "dependencies": [
              "16.1"
            ],
            "details": "Within the components section of the OAM application, define a realtime-platform component named 'minimal-realtime'. Use the type 'realtime-platform' and include only the required parameters with default values. Do not override any optional parameters to demonstrate the minimal configuration approach.",
            "status": "done",
            "testStrategy": "Verify that the component definition matches the schema defined in the realtime-component-definitions.yaml file."
          },
          {
            "id": 3,
            "title": "Define webservice component",
            "description": "Create a webservice component that references the realtime-platform component.",
            "dependencies": [
              "16.2"
            ],
            "details": "Add a webservice component named 'minimal-service' to the components section. Configure it with basic settings (image, ports, etc.). Include the realtime parameter that references the realtime-platform component created in the previous step. Use the simplest possible configuration to demonstrate the integration.",
            "status": "done",
            "testStrategy": "Check that the webservice component correctly references the realtime-platform component and includes all required parameters."
          },
          {
            "id": 4,
            "title": "Define application policies",
            "description": "Add necessary policies for the application such as health policy and topology policy.",
            "dependencies": [
              "16.3"
            ],
            "details": "In the policies section of the OAM application, define a health policy that monitors the status of the components. Add a topology policy that defines the relationship between the webservice and realtime-platform components. Keep the policies minimal and focused on demonstrating the basic functionality.",
            "status": "done",
            "testStrategy": "Validate that the policies are correctly defined and reference the components appropriately."
          },
          {
            "id": 5,
            "title": "Add comments and documentation",
            "description": "Add inline comments and documentation to explain the purpose and structure of the example.",
            "dependencies": [
              "16.4"
            ],
            "details": "Add comprehensive comments throughout the YAML file to explain each section and parameter. Include a header comment that describes the purpose of the example, the components used, and how they interact. Add references to related documentation or examples for users who want more information. Ensure the example is self-explanatory for new users.",
            "status": "done",
            "testStrategy": "Review the comments for clarity and accuracy. Ensure they provide sufficient guidance for users to understand the example without external documentation."
          }
        ]
      },
      {
        "id": 17,
        "title": "Create REALTIME-OAM-EXAMPLE.yaml",
        "description": "Create a comprehensive example OAM application that demonstrates all features of the realtime-platform component.",
        "details": "Create a new file at `/Users/socrateshlapolosa/Development/health-service-idp/REALTIME-OAM-EXAMPLE.yaml`. Implement a comprehensive OAM application that uses the realtime-platform component with all available configuration options. Include multiple webservice components that reference the realtime-platform using the realtime parameter. Demonstrate the use of advanced features such as custom MQTT users, data retention settings, scaling configuration, and Snowflake integration. Use the example from the PRD as a reference.",
        "testStrategy": "Validate the YAML syntax. Test that the example can be applied to a KubeVela environment. Verify that the realtime-platform and webservice components are created correctly with all the specified configurations. Test the integration between the components and the advanced features.",
        "priority": "medium",
        "dependencies": [
          1,
          3,
          4,
          15
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define the basic OAM application structure with realtime-platform component",
            "description": "Create the initial YAML file structure for the OAM application with the realtime-platform component definition and basic configuration parameters.",
            "dependencies": [],
            "details": "Create the REALTIME-OAM-EXAMPLE.yaml file with the proper OAM application structure. Define the application metadata (name, description) and include the realtime-platform component with basic configuration such as name, database settings, and visualization settings. Set up the proper resource references and ensure the component follows the OAM specification format. Include comments explaining each section of the configuration.",
            "status": "done",
            "testStrategy": "Validate the YAML syntax using a linter. Ensure the file follows the OAM Application schema and that the realtime-platform component is properly defined."
          },
          {
            "id": 2,
            "title": "Implement multiple webservice components with realtime integration",
            "description": "Add multiple webservice components to the OAM application that reference the realtime-platform component using the realtime parameter.",
            "dependencies": [
              "17.1"
            ],
            "details": "Add at least three different webservice components that demonstrate different integration patterns with the realtime-platform. Each webservice should have a unique name, image, and port configuration. Configure the realtime parameter in each webservice to reference the realtime-platform component. Include examples of different access patterns (read-only, read-write, admin). Demonstrate how to configure topic subscriptions and publications for each service.",
            "status": "done",
            "testStrategy": "Verify that each webservice component correctly references the realtime-platform component. Check that the realtime parameter is properly configured with appropriate access levels."
          },
          {
            "id": 3,
            "title": "Configure advanced MQTT user settings and authentication",
            "description": "Implement custom MQTT user configurations with different permission levels and authentication mechanisms.",
            "dependencies": [
              "17.1"
            ],
            "details": "Add a comprehensive mqttUsers section to the realtime-platform component configuration. Define at least three different user types (admin, publisher, subscriber) with appropriate permission patterns. Configure username/password authentication for some users and certificate-based authentication for others. Include ACL patterns that demonstrate topic-level access control. Add comments explaining the security implications of each configuration option.",
            "status": "done",
            "testStrategy": "Validate that the MQTT user configurations follow the correct schema. Ensure that the authentication methods and ACL patterns are properly defined."
          },
          {
            "id": 4,
            "title": "Implement data retention and scaling configurations",
            "description": "Configure advanced data retention policies and scaling parameters for the realtime-platform component.",
            "dependencies": [
              "17.1"
            ],
            "details": "Add detailed configuration for data retention settings including retention periods for different data types, storage allocation, and cleanup policies. Configure scaling parameters for the Kafka, MQTT, and database components including replica counts, resource requests/limits, and auto-scaling policies. Demonstrate how to configure high-availability settings for production-grade deployments. Include performance tuning parameters for different workload profiles.",
            "status": "done",
            "testStrategy": "Check that all retention and scaling parameters are valid and follow the component schema. Verify that the configuration values are reasonable for a production environment."
          },
          {
            "id": 5,
            "title": "Configure Snowflake integration and advanced analytics features",
            "description": "Implement Snowflake integration settings and configure advanced analytics features of the realtime-platform.",
            "dependencies": [
              "17.1",
              "17.4"
            ],
            "details": "Add Snowflake integration configuration including connection parameters, authentication settings, and data sync policies. Configure data transformation pipelines that demonstrate how to process and enrich data before sending it to Snowflake. Set up advanced analytics features such as real-time dashboards, alerts, and anomaly detection. Include examples of custom metrics and KPIs that can be tracked. Demonstrate how to configure data export schedules and formats.",
            "status": "done",
            "testStrategy": "Validate the Snowflake connection parameters and ensure they follow security best practices. Test that the analytics configuration is complete and properly structured according to the component schema."
          }
        ]
      },
      {
        "id": 18,
        "title": "Implement Health Checks and Monitoring",
        "description": "Implement health checks and monitoring for all components in the real-time platform.",
        "details": "In the realtime-compositions.yaml file, implement health checks and monitoring for all components. Use the health check configurations provided in the PRD. Configure liveness and readiness probes for each component. Set up monitoring integration with the existing Prometheus/Grafana stack. Configure alerting for component failures. Implement log aggregation and monitoring.",
        "testStrategy": "Deploy the Health Checks and Monitoring configuration and verify that all health checks are working correctly. Test the monitoring integration by checking that metrics are being collected in Prometheus and displayed in Grafana. Test alerting by simulating component failures.",
        "priority": "medium",
        "dependencies": [
          7,
          8,
          9,
          10,
          13
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Health Check Endpoints for All Components",
            "description": "Implement health check endpoints for each component in the real-time platform, ensuring they expose appropriate liveness and readiness probes according to the PRD specifications.",
            "dependencies": [],
            "details": "For each component in the realtime-compositions.yaml file (MQTT Broker, Kafka Cluster, PostgreSQL, Lenses Agent, etc.), add health check endpoint configurations. For the MQTT Broker, configure TCP socket checks on port 1883. For Kafka, implement HTTP checks on the JMX exporter endpoint. For PostgreSQL, use a simple database connection check. For Lenses components, use their built-in health endpoints. Configure both liveness probes (to detect if a component is running) and readiness probes (to detect if a component is ready to accept traffic) with appropriate initialDelaySeconds, periodSeconds, and failureThreshold values.",
            "status": "pending",
            "testStrategy": "Test each health check endpoint individually by accessing them directly. Verify that they return appropriate status codes (200 for healthy, non-200 for unhealthy). Simulate failures to ensure the health checks detect them correctly."
          },
          {
            "id": 2,
            "title": "Set Up Prometheus Metrics Exporters",
            "description": "Configure metrics exporters for all components to expose monitoring data to Prometheus in a standardized format.",
            "dependencies": [
              "18.1"
            ],
            "details": "Add Prometheus annotations to each component's service definition to enable scraping. For Kafka, configure and expose the JMX exporter on port 9404. For PostgreSQL, deploy the postgres_exporter sidecar container. For MQTT Broker, add the mosquitto_exporter sidecar. For Lenses components, enable their built-in Prometheus metrics endpoints. Configure appropriate scrape intervals and metrics paths in the annotations. Ensure all exporters expose key performance metrics such as resource usage, request rates, error rates, and latency.",
            "status": "pending",
            "testStrategy": "Verify that Prometheus is discovering and scraping metrics from all components. Use the Prometheus UI to query metrics from each component and confirm they are being collected correctly."
          },
          {
            "id": 3,
            "title": "Implement Grafana Dashboards for Monitoring",
            "description": "Create comprehensive Grafana dashboards to visualize the health and performance metrics of all platform components.",
            "dependencies": [
              "18.2"
            ],
            "details": "Develop a set of Grafana dashboards that visualize the metrics collected by Prometheus. Create a main overview dashboard showing the health status of all components at a glance. Create component-specific dashboards for MQTT Broker, Kafka Cluster, PostgreSQL, and Lenses components with detailed metrics. Include panels for CPU/memory usage, connection counts, message throughput, error rates, and latency. Configure appropriate thresholds and color coding to highlight potential issues. Store dashboard definitions as code in the project repository.",
            "status": "pending",
            "testStrategy": "Load the dashboards in Grafana and verify that all panels display data correctly. Check that dashboard variables and filters work as expected. Simulate high load and component failures to ensure the dashboards accurately reflect system state."
          },
          {
            "id": 4,
            "title": "Configure Alerting Rules and Notifications",
            "description": "Set up alerting rules in Prometheus AlertManager to detect and notify about component failures and performance issues.",
            "dependencies": [
              "18.2",
              "18.3"
            ],
            "details": "Define Prometheus alerting rules for critical conditions such as component unavailability, high error rates, resource exhaustion, and performance degradation. Configure the AlertManager to route alerts to appropriate channels (email, Slack, PagerDuty) based on severity and component. Set up alert grouping and silencing policies to prevent alert storms. Define escalation paths for unresolved alerts. Create alert templates with actionable information including component details, error messages, and troubleshooting links. Store alerting configurations in the project repository alongside the component definitions.",
            "status": "pending",
            "testStrategy": "Test each alert by simulating the trigger condition. Verify that notifications are sent to the correct channels with the expected content. Test alert grouping, silencing, and escalation paths."
          },
          {
            "id": 5,
            "title": "Implement Log Aggregation and Monitoring",
            "description": "Set up centralized log collection, aggregation, and monitoring for all platform components.",
            "dependencies": [
              "18.1"
            ],
            "details": "Configure each component to output logs in a structured format (JSON). Deploy a log collector (Fluentd or Fluent Bit) as a DaemonSet to collect logs from all pods. Configure the log collector to parse and enrich logs with metadata such as component name, namespace, and pod ID. Set up log forwarding to a centralized logging system (Elasticsearch or Loki). Create log indices and retention policies based on component types. Configure log-based alerts for critical error patterns. Integrate log visualization with Grafana dashboards created in subtask 18.3. Ensure sensitive information is properly masked in logs.",
            "status": "pending",
            "testStrategy": "Verify that logs from all components are being collected and forwarded correctly. Check that log parsing and enrichment are working as expected. Test log-based alerts by generating error logs. Verify that logs are properly integrated with the monitoring dashboards."
          }
        ]
      },
      {
        "id": 19,
        "title": "Implement Security Hardening",
        "description": "Implement security hardening for all components in the real-time platform.",
        "details": "In the realtime-compositions.yaml file, implement security hardening for all components. Configure proper MQTT authentication and TLS. Set up Snowflake credential management via External Secrets Operator. Implement network policies to restrict communication between components. Configure service mesh security using Istio. Set up audit logging and access controls. Ensure all sensitive data is stored securely.",
        "testStrategy": "Deploy the Security Hardening configuration and verify that all security measures are working correctly. Test MQTT authentication and TLS. Verify that network policies are restricting communication as expected. Test service mesh security. Check that audit logging is capturing relevant events.",
        "priority": "medium",
        "dependencies": [
          7,
          8,
          9,
          10,
          13,
          15
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure MQTT Authentication and TLS",
            "description": "Implement secure authentication and TLS encryption for the MQTT broker to protect device communications",
            "dependencies": [],
            "details": "Update the realtime-compositions.yaml file to configure the MQTT broker security settings. Add TLS certificate configuration by creating a Secret resource containing the server certificate and private key. Modify the Mosquitto configuration to enable password-based authentication and TLS on ports 8883 (MQTT with TLS) and 9001 (WebSockets). Create a ConfigMap for mosquitto.conf with appropriate security settings including 'allow_anonymous false', 'password_file /mosquitto/config/passwd', and TLS certificate paths. Update volume mounts to include the TLS certificates and password file.",
            "status": "pending",
            "testStrategy": "Deploy the secured MQTT broker and verify TLS connections using mosquitto_pub/sub tools with certificates. Test authentication by attempting connections with valid and invalid credentials. Verify that anonymous connections are rejected and that encrypted communications work properly."
          },
          {
            "id": 2,
            "title": "Implement Snowflake Credential Management via External Secrets Operator",
            "description": "Set up secure credential management for Snowflake connections using External Secrets Operator",
            "dependencies": [
              "19.1"
            ],
            "details": "Install the External Secrets Operator in the cluster if not already present. Create a SecretStore resource that connects to your external secrets provider (AWS Secrets Manager, HashiCorp Vault, etc.). Define an ExternalSecret resource that references Snowflake credentials stored in the external provider. Configure the ExternalSecret to create a Kubernetes Secret with the format required by the Kafka Connect Snowflake connector. Update the Kafka component in realtime-compositions.yaml to mount this secret instead of using environment variables for credentials. Ensure the secret includes SNOWFLAKE_URL, SNOWFLAKE_USER, SNOWFLAKE_PRIVATE_KEY, and SNOWFLAKE_DATABASE fields.",
            "status": "pending",
            "testStrategy": "Verify that the External Secrets Operator successfully retrieves credentials from the external provider. Check that the Kubernetes Secret is created with the correct format. Test the Snowflake connector's ability to connect using these credentials. Attempt to rotate credentials in the external provider and verify that the changes propagate correctly."
          },
          {
            "id": 3,
            "title": "Implement Network Policies for Component Isolation",
            "description": "Create Kubernetes Network Policies to restrict communication between components to only necessary paths",
            "dependencies": [
              "19.1",
              "19.2"
            ],
            "details": "Define NetworkPolicy resources in realtime-compositions.yaml for each component. For the MQTT broker, allow only inbound connections on ports 1883, 8883, and 9001 from authorized sources. For Kafka, restrict access to only components that need to produce or consume messages (Lenses Agent, MQTT connector). For PostgreSQL, allow connections only from Lenses HQ, Lenses Agent, and Metabase. For Lenses components, restrict API access to authorized frontend services. Use podSelector, namespaceSelector, and ipBlock selectors to precisely control traffic. Implement default deny policies for each namespace to block unspecified traffic. Label all pods appropriately to enable policy targeting.",
            "status": "pending",
            "testStrategy": "Test network isolation by attempting to access services from unauthorized pods. Verify that legitimate communication paths work correctly. Use network policy testing tools like netassert or network-policy-validator to validate policy effectiveness. Test each component's ability to reach required dependencies while confirming that unauthorized access paths are blocked."
          },
          {
            "id": 4,
            "title": "Configure Service Mesh Security with Istio",
            "description": "Implement Istio service mesh for enhanced security controls including mTLS, authorization policies, and traffic encryption",
            "dependencies": [
              "19.3"
            ],
            "details": "Install Istio in the cluster if not already present. Add the necessary annotations to enable Istio sidecar injection for all components in realtime-compositions.yaml. Create DestinationRule resources to enforce mutual TLS (mTLS) between all services. Implement AuthorizationPolicy resources to define fine-grained access controls based on service identity, path, method, and headers. Configure Istio Gateway and VirtualService resources for secure ingress traffic with proper TLS termination. Set up Istio RequestAuthentication and PeerAuthentication for JWT validation and service-to-service authentication. Update the realtime-compositions.yaml to include these Istio resources alongside the component definitions.",
            "status": "pending",
            "testStrategy": "Verify mTLS encryption between services using istioctl x describe pod commands. Test authorization policies by attempting authorized and unauthorized requests. Validate JWT authentication for API endpoints. Use Istio's visualization tools (Kiali, Grafana) to verify the security posture of the service mesh. Test certificate rotation and ensure services remain available during rotation events."
          },
          {
            "id": 5,
            "title": "Implement Audit Logging and Secure Storage",
            "description": "Set up comprehensive audit logging for all components and ensure secure storage of sensitive data",
            "dependencies": [
              "19.4"
            ],
            "details": "Configure audit logging for all components in realtime-compositions.yaml. For Kafka, enable audit logging by setting the appropriate properties in server.properties (kafka.authorizer.logger=INFO, log4j.logger.kafka.request.logger=INFO). For PostgreSQL, enable pgaudit extension and configure it to log all DDL and DML operations. For MQTT, configure detailed logging of connection attempts and authentication events. Implement a centralized logging solution using Fluentd or Fluent Bit to collect logs from all components. Configure log forwarding to a secure SIEM system. For secure storage, implement encryption-at-rest for all persistent volumes using Kubernetes encryption providers. Configure Kubernetes Secrets encryption using KMS. Ensure all sensitive configuration values are stored as Secrets rather than ConfigMaps or environment variables.",
            "status": "pending",
            "testStrategy": "Verify that audit logs are generated for key security events (authentication attempts, authorization failures, data access). Test the centralized logging system by triggering events and confirming they appear in the log aggregation system. Validate that sensitive data is properly encrypted at rest by examining storage. Attempt to access encrypted volumes directly and verify they cannot be read without proper authentication."
          }
        ]
      },
      {
        "id": 20,
        "title": "Create Documentation and User Guide",
        "description": "Create comprehensive documentation and user guide for the real-time platform components.",
        "details": "Create documentation that explains how to use the realtime-platform component and the enhanced webservice component with realtime integration. Include examples of minimal and comprehensive configurations. Document all available parameters and their default values. Explain how to access the various services (Lenses HQ, MQTT, Metabase) and how to use the connection secrets. Create troubleshooting guides and operational runbooks. Document the architecture and data flow of the real-time platform.",
        "testStrategy": "Review the documentation for accuracy and completeness. Test the examples provided in the documentation to ensure they work as described. Have other team members review the documentation and provide feedback.",
        "priority": "medium",
        "dependencies": [
          1,
          2,
          3,
          4,
          15,
          16,
          17
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Document Architecture and Component Overview",
            "description": "Create comprehensive documentation of the real-time platform architecture, data flow diagrams, and component relationships.",
            "dependencies": [],
            "details": "Create a markdown document that includes: 1) High-level architecture diagram showing all components (Kafka, MQTT, Lenses HQ, Metabase, PostgreSQL), 2) Data flow diagrams showing how information moves through the system, 3) Component relationship diagrams showing dependencies, 4) Explanation of each component's purpose and role in the system, 5) System requirements and prerequisites. Include diagrams created with a tool like draw.io or mermaid and export them as SVG/PNG for inclusion in the documentation.",
            "status": "pending",
            "testStrategy": "Review the architecture documentation with technical leads to verify accuracy. Ensure all components are correctly represented and the data flow matches the actual implementation."
          },
          {
            "id": 2,
            "title": "Create Configuration Reference Documentation",
            "description": "Document all available configuration parameters for the realtime-platform and webservice components with realtime integration.",
            "dependencies": [
              "20.1"
            ],
            "details": "Create a comprehensive reference document that: 1) Lists all configuration parameters for the realtime-platform component, 2) Documents all parameters for webservice components with realtime integration, 3) Specifies default values, data types, and validation rules for each parameter, 4) Organizes parameters by functional area (e.g., Kafka settings, MQTT settings, scaling, etc.), 5) Includes tables showing parameter interdependencies and constraints. Reference the existing component definitions in realtime-component-definitions.yaml and realtime-compositions.yaml to ensure all parameters are documented.",
            "status": "pending",
            "testStrategy": "Validate the documentation against the actual code to ensure all parameters are correctly documented. Test setting each parameter to verify the documented behavior matches the implementation."
          },
          {
            "id": 3,
            "title": "Develop Example-Based User Guide",
            "description": "Create a user guide with step-by-step instructions and examples for common usage scenarios.",
            "dependencies": [
              "20.1",
              "20.2"
            ],
            "details": "Develop a user guide that includes: 1) Step-by-step instructions for deploying the realtime-platform using the minimal example (MINIMAL-REALTIME-OAM.yaml), 2) Instructions for deploying the comprehensive example (REALTIME-OAM-EXAMPLE.yaml), 3) Walkthrough of customizing configurations for specific use cases, 4) Examples of integrating webservices with the realtime platform, 5) Screenshots and command outputs showing expected results at each step. Include code snippets that users can copy and adapt for their own implementations.",
            "status": "pending",
            "testStrategy": "Follow the guide step-by-step in a test environment to verify the instructions work as written. Have team members unfamiliar with the system attempt to follow the guide and provide feedback."
          },
          {
            "id": 4,
            "title": "Create Service Access and Secrets Management Guide",
            "description": "Document how to access the various services (Lenses HQ, MQTT, Metabase) and manage connection secrets.",
            "dependencies": [
              "20.2"
            ],
            "details": "Create documentation that: 1) Explains how to retrieve and use connection secrets for each service, 2) Provides instructions for accessing the web interfaces of Lenses HQ and Metabase, 3) Documents MQTT connection parameters and authentication methods, 4) Includes examples of connecting to each service using different client libraries and tools, 5) Explains how to rotate credentials and manage access control. Include specific code examples in multiple languages (e.g., Python, JavaScript) showing how to connect to each service.",
            "status": "pending",
            "testStrategy": "Test all connection examples with actual services to verify they work correctly. Verify secret retrieval instructions by following them in a test environment."
          },
          {
            "id": 5,
            "title": "Develop Troubleshooting Guide and Operational Runbooks",
            "description": "Create comprehensive troubleshooting guides and operational runbooks for maintaining the real-time platform.",
            "dependencies": [
              "20.1",
              "20.2",
              "20.3",
              "20.4"
            ],
            "details": "Develop documentation that includes: 1) Common error scenarios and their solutions, 2) Diagnostic procedures for identifying issues, 3) Operational runbooks for routine maintenance tasks, 4) Monitoring and alerting recommendations, 5) Backup and recovery procedures, 6) Scaling guidelines and performance optimization tips, 7) Upgrade and migration procedures. Organize the troubleshooting guide by component and symptom, with clear steps to diagnose and resolve each issue. Include command examples and expected outputs.",
            "status": "pending",
            "testStrategy": "Review the troubleshooting guide with operations team members. Simulate common failure scenarios in a test environment and verify the documented recovery procedures work correctly."
          }
        ]
      },
      {
        "id": 21,
        "title": "Implement Enhanced WebService ComponentDefinition with Realtime Parameter Support",
        "description": "Enhance the existing webservice ComponentDefinition to support integration with realtime-platform components by adding a realtime parameter and implementing secret injection mechanisms.",
        "details": "1. Update the webservice ComponentDefinition in `/Users/socrateshlapolosa/Development/health-service-idp/crossplane/oam/application-component-definitions.yaml` to add a new optional `realtime` parameter that accepts a string value representing the name of a realtime-platform to connect to.\n\n2. Modify the CUE template in the webservice ComponentDefinition to include logic that:\n   - Checks if the `realtime` parameter is specified\n   - Validates if the referenced realtime-platform exists in the cluster\n   - Injects appropriate connection secrets when the realtime parameter is specified\n\n3. Implement the secret reference injection mechanism in the webservice XRD that follows the `{name}-{service}-secret` naming pattern for:\n   - MQTT broker (`{name}-mqtt-secret`)\n   - Kafka (`{name}-kafka-secret`)\n   - Database (`{name}-db-secret`)\n   - Metabase (`{name}-metabase-secret`)\n   - Lenses (`{name}-lenses-secret`)\n\n4. Update the webservice controller to:\n   - Query the cluster for the existence of the referenced realtime-platform\n   - Retrieve the connection secrets associated with the realtime-platform\n   - Mount the secrets as environment variables in the webservice container\n   - Handle error cases gracefully when the referenced platform doesn't exist\n\n5. Add documentation in the code comments explaining:\n   - The purpose of the realtime parameter\n   - The naming convention for secrets\n   - How to use the feature in application deployments\n   - Error handling behavior\n\n6. Ensure backward compatibility with existing webservice deployments that don't use the realtime parameter.",
        "testStrategy": "1. Create a test webservice component that references an existing realtime-platform by name:\n   ```yaml\n   apiVersion: core.oam.dev/v1beta1\n   kind: Component\n   metadata:\n     name: test-webservice\n   spec:\n     type: webservice\n     properties:\n       image: nginx:latest\n       realtime: \"my-realtime-platform\"\n   ```\n\n2. Deploy the test webservice and verify that:\n   - The webservice pod starts successfully\n   - The connection secrets are correctly injected into the pod's environment\n   - The secret names follow the `{name}-{service}-secret` pattern\n   - All five required secrets (mqtt, kafka, db, metabase, lenses) are properly mounted\n\n3. Test error handling by creating a webservice that references a non-existent realtime-platform and verify:\n   - The deployment proceeds without the realtime secrets\n   - Appropriate warning events are generated\n   - The application logs contain clear error messages about the missing platform\n\n4. Test backward compatibility by deploying a webservice without the realtime parameter and verify it works as before.\n\n5. Create a test case with multiple webservices connecting to the same realtime-platform to verify proper secret sharing.\n\n6. Verify that the webservice can successfully connect to each of the realtime services using the injected secrets.",
        "status": "done",
        "dependencies": [
          1,
          2,
          3,
          15
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Update WebService ComponentDefinition Schema",
            "description": "Modify the webservice ComponentDefinition to add a new optional 'realtime' parameter that accepts a string value representing the name of a realtime-platform to connect to.",
            "dependencies": [],
            "details": "1. Locate the webservice ComponentDefinition in `/Users/socrateshlapolosa/Development/health-service-idp/crossplane/oam/application-component-definitions.yaml`\n2. Add the new 'realtime' parameter to the schema section\n3. Document the parameter with appropriate description and examples\n4. Ensure backward compatibility with existing components",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement CUE Template Logic for Realtime Integration",
            "description": "Modify the CUE template in the webservice ComponentDefinition to include logic that validates and processes the realtime parameter.",
            "dependencies": [
              "21.1"
            ],
            "details": "1. Add logic to check if the 'realtime' parameter is specified\n2. Implement validation to verify if the referenced realtime-platform exists\n3. Add logic to inject the necessary environment variables and secrets for connecting to the realtime platform\n4. Implement error handling for cases where the referenced platform doesn't exist",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create Tests for Realtime Parameter Support",
            "description": "Develop and execute tests to verify the enhanced webservice ComponentDefinition works correctly with the realtime parameter.",
            "dependencies": [
              "21.1",
              "21.2"
            ],
            "details": "1. Create a test webservice component that references an existing realtime-platform\n2. Create a test case with an invalid realtime-platform name to verify validation\n3. Deploy the test components and verify that the proper environment variables and secrets are injected\n4. Verify that components without the realtime parameter still work as before",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 22,
        "title": "Implement Smart Real-time Service Management Logic",
        "description": "Create an intelligent service management system that detects existing applications in the cluster, analyzes their structure for routes package compliance, and enhances or creates services based on template standards.",
        "status": "done",
        "dependencies": [
          1,
          3,
          8,
          10,
          15,
          21
        ],
        "priority": "high",
        "details": "Implement the Smart Real-time Service Management logic with the following components:\n\n1. Service Existence Detection:\n   - Create a Kubernetes client utility that queries the cluster for existing applications\n   - Implement a service discovery mechanism that identifies applications by name, labels, and annotations\n   - Build a caching layer to optimize repeated lookups and reduce API server load\n   - Add logging for service discovery events with appropriate detail levels\n\n2. Routes Package Analysis:\n   - Develop an analyzer that examines application structure to detect if the service exposes a routes package\n   - Check for the following specific requirements:\n     a. Existence of a routes/ directory with __init__.py file\n     b. Modular route files (e.g., websocket.py, health.py)\n     c. FastAPI router registration pattern\n     d. WebSocket endpoint at /ws path\n     e. Health check endpoints\n   - Implement file system traversal to inspect application code structure\n   - Create a validation mechanism that verifies routes package implementation against template standards\n   - Compare against the reference implementation at https://github.com/heath-health/realtime_data_pipeline/tree/main/realtime_service/routes\n\n3. Decision Logic Implementation:\n   - Implement conditional logic for three scenarios:\n     a. If service exists and has complete routes package: log and exit without changes\n     b. If service exists but lacks routes package or has incomplete implementation: enhance with WebSocket and aiokafka integration\n     c. If service doesn't exist: create new service from template\n   - Add configuration options to control behavior for each scenario\n   - Implement rollback mechanisms for failed enhancement operations\n\n4. Template-based Service Enhancement:\n   - Create a template engine that can inject WebSocket and aiokafka components into existing services\n   - Implement code generation for required integration points\n   - Add configuration for WebSocket endpoints and Kafka topic subscriptions\n   - Ensure backward compatibility with existing service functionality\n   - For incomplete route packages, implement:\n     a. aiokafka consumer integration\n     b. WebSocket streaming implementation\n     c. Modular route structure following the reference implementation\n\n5. Onion Architecture Compliance:\n   - Implement validation checks for Onion Architecture compliance\n   - Verify proper separation of concerns across layers (domain, application, infrastructure)\n   - Check for dependency direction compliance (dependencies point inward)\n   - Generate compliance reports for detected architecture violations\n\n6. Integration with Existing Platform:\n   - Connect to the realtime platform components using the established secret patterns\n   - Implement proper error handling for connectivity issues\n   - Add metrics collection for service management operations\n   - Ensure all operations are properly logged for audit purposes",
        "testStrategy": "1. Unit Testing:\n   - Create unit tests for each component of the service management logic\n   - Mock Kubernetes API responses for service existence testing\n   - Test routes package detection with various application structures\n   - Test for each specific routes package requirement (directory structure, files, endpoints)\n   - Verify decision logic with all possible combinations of conditions\n   - Test template enhancement with different service configurations\n\n2. Integration Testing:\n   - Deploy test applications with and without routes packages\n   - Deploy applications with incomplete routes packages (missing some requirements)\n   - Verify that the system correctly identifies existing applications\n   - Test the enhancement process on applications missing routes packages\n   - Verify that new applications are created correctly from templates\n   - Test rollback functionality by simulating failures during enhancement\n\n3. Compliance Testing:\n   - Create test cases with various architecture violations\n   - Verify that the compliance checker correctly identifies all issues\n   - Test with both compliant and non-compliant applications\n   - Verify that compliance reports contain accurate information\n   - Test with applications having partial routes package implementation\n\n4. Performance Testing:\n   - Measure service discovery performance with large numbers of applications\n   - Test caching effectiveness for repeated lookups\n   - Verify that enhancement operations complete within acceptable timeframes\n   - Test system behavior under high concurrency conditions\n\n5. End-to-End Testing:\n   - Create a test environment with multiple application scenarios\n   - Verify the entire workflow from detection to enhancement/creation\n   - Test integration with WebSocket and Kafka components\n   - Verify that enhanced applications function correctly with realtime data\n   - Validate that enhanced services follow the reference implementation pattern",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Service Existence Detection",
            "description": "Create a Kubernetes client utility that detects and monitors applications in the cluster with efficient caching and logging.",
            "dependencies": [],
            "details": "Develop a Kubernetes client utility that queries the cluster for existing applications. Implement service discovery that identifies applications by name, labels, and annotations. Build a caching layer to optimize repeated lookups and reduce API server load. Add comprehensive logging for service discovery events with appropriate detail levels. Ensure the detection mechanism works in real-time to identify new or modified services.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Routes Package Compliance Analysis",
            "description": "Create logic to analyze application structure for routes package compliance and generate compliance reports.",
            "dependencies": [
              "22.1"
            ],
            "details": "Develop analysis logic that examines application structure to determine routes package compliance. Create parsers for different application types (microservices, monoliths, etc.). Implement a scoring system to evaluate compliance levels. Generate detailed compliance reports highlighting areas of conformance and non-conformance. Design the system to handle various programming languages and frameworks.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Service Enhancement and Creation",
            "description": "Build logic to enhance existing services or create new ones based on template standards.",
            "dependencies": [
              "22.1",
              "22.2"
            ],
            "details": "Develop service enhancement logic that can modify existing services to improve compliance with template standards. Create service generation capabilities for applications lacking proper service definitions. Implement template-based service creation with customizable parameters. Build validation mechanisms to ensure created/enhanced services meet all requirements. Include rollback capabilities for failed enhancement operations.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 23,
        "title": "Create Real-time Service Template for Kubernetes",
        "description": "Extend the existing microservices/shared-libs/agent-common library to add real-time capabilities, following Onion Architecture guidelines and 12-factor app principles. This will enable all microservices to leverage standardized real-time functionality.",
        "status": "done",
        "dependencies": [
          8,
          15,
          22
        ],
        "priority": "high",
        "details": "1. Analyze and extend the existing agent-common library:\n   - Review the current agent-common library structure and functionality\n   - Identify integration points for real-time capabilities\n   - Design extensions that maintain Onion Architecture principles\n   - Ensure backward compatibility with existing 18 microservices\n\n2. Implement WebSocket extensions for FastAPI:\n   - Create base WebSocket handler classes that integrate with existing FastAPI base classes\n   - Implement connection management utilities\n   - Add authentication and authorization support for WebSocket connections\n   - Create standardized error handling for WebSocket connections\n\n3. Develop streaming data models:\n   - Create base classes for real-time data streams\n   - Implement serialization/deserialization utilities\n   - Add validation mechanisms for streaming data\n   - Design composable stream transformation utilities\n\n4. Implement aiokafka integration helpers:\n   - Create producer and consumer base classes\n   - Implement connection pooling and management\n   - Add retry and error handling mechanisms\n   - Create utilities for topic management and message processing\n\n5. Develop real-time client utilities:\n   - Create client-side connection management utilities\n   - Implement reconnection and backoff strategies\n   - Add client-side data processing helpers\n   - Create standardized client error handling\n\n6. Ensure 12-factor app compliance:\n   - Maintain configuration management via environment variables\n   - Ensure proper logging integration\n   - Preserve stateless design principles\n   - Support proper backing service attachment\n\n7. Create Kubernetes integration utilities:\n   - Add Kubernetes-specific health check extensions\n   - Implement graceful startup/shutdown hooks\n   - Create resource management utilities\n   - Add Kubernetes-specific configuration helpers\n\n8. Document the extended library:\n   - Update the existing documentation with new capabilities\n   - Create examples of real-time service implementation\n   - Document migration paths for existing services\n   - Include performance considerations and best practices",
        "testStrategy": "1. Unit Testing:\n   - Create comprehensive unit tests for all new components\n   - Test integration with existing agent-common functionality\n   - Verify WebSocket implementation with various scenarios\n   - Test aiokafka integration with mocks\n\n2. Integration Testing:\n   - Create a test microservice using the extended library\n   - Verify WebSocket and Kafka functionality in a test environment\n   - Test compatibility with existing microservices\n   - Validate proper topic consumption and message processing\n\n3. Load Testing:\n   - Simulate high message throughput to test performance\n   - Verify memory usage under load\n   - Test concurrent WebSocket connections\n   - Measure and document performance characteristics\n\n4. Security Testing:\n   - Verify proper authentication and authorization\n   - Test secure WebSocket implementation\n   - Validate proper handling of sensitive data\n   - Check for potential data leakage\n\n5. Compliance Testing:\n   - Verify adherence to 12-factor app principles\n   - Validate Onion Architecture implementation\n   - Check for proper separation of concerns\n   - Ensure configuration is properly externalized\n\n6. Kubernetes Deployment Testing:\n   - Test deployment with the extended library in different Kubernetes environments\n   - Verify proper resource utilization\n   - Test scaling behavior\n   - Validate health checks and readiness probes\n\n7. Documentation Testing:\n   - Review documentation for completeness\n   - Verify examples work as described\n   - Test migration paths for existing services\n   - Validate troubleshooting guides",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze existing agent-common library and define extension architecture",
            "description": "Analyze the existing microservices/shared-libs/agent-common library and define the architecture for real-time capability extensions.",
            "status": "done",
            "dependencies": [],
            "details": "Review the existing agent-common library structure, components, and functionality. Document the current FastAPI base classes, dependency injection patterns, and health check implementations. Define clear integration points for real-time capabilities while maintaining Onion Architecture principles. Create architecture diagrams showing how the new components will integrate with existing ones. Identify potential backward compatibility issues and mitigation strategies.",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement WebSocket base classes",
            "description": "Develop WebSocket base classes that integrate with existing FastAPI base classes in agent-common.",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "Create WebSocket handler base classes that leverage existing dependency injection patterns. Implement connection management utilities for tracking active connections. Add authentication and authorization support compatible with existing patterns. Develop standardized error handling for WebSocket connections. Create middleware for WebSocket request processing. Ensure all implementations follow the existing architectural patterns in agent-common.",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Develop streaming data models",
            "description": "Create base classes and utilities for real-time streaming data models.",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "Design and implement base classes for real-time data streams that integrate with existing data models. Create serialization/deserialization utilities for different data formats (JSON, Avro, etc.). Implement validation mechanisms for streaming data that leverage existing validation patterns. Develop composable stream transformation utilities for data processing. Ensure all models follow domain-driven design principles and maintain proper separation of concerns.",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement aiokafka integration helpers",
            "description": "Develop aiokafka integration utilities that follow the existing patterns in agent-common.",
            "status": "done",
            "dependencies": [
              1,
              3
            ],
            "details": "Create producer and consumer base classes that integrate with the dependency injection system. Implement connection pooling and management utilities. Add retry and error handling mechanisms following existing patterns. Develop utilities for topic management and message processing. Create adapters that abstract Kafka-specific details behind clean interfaces. Ensure proper configuration management using existing patterns.",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Develop real-time client utilities",
            "description": "Create client-side utilities for connecting to and consuming real-time data streams.",
            "status": "done",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Implement client-side connection management utilities for WebSocket connections. Create reconnection and backoff strategies for handling connection issues. Develop client-side data processing helpers for handling streaming data. Implement standardized client error handling and logging. Create utilities for managing multiple concurrent connections. Ensure all client utilities are compatible with the server-side implementations.",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Enhance Kubernetes integration",
            "description": "Extend the existing Kubernetes integration in agent-common with real-time specific features.",
            "status": "done",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Add Kubernetes-specific health check extensions for real-time components. Implement graceful startup/shutdown hooks for WebSocket and Kafka connections. Create resource management utilities for real-time services. Develop Kubernetes-specific configuration helpers for real-time components. Ensure all extensions maintain compatibility with existing Kubernetes integration features.",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Create comprehensive testing suite",
            "description": "Develop unit, integration, and end-to-end tests for the extended library.",
            "status": "done",
            "dependencies": [
              2,
              3,
              4,
              6
            ],
            "details": "Implement unit tests for all new components following existing testing patterns. Create integration tests for WebSocket and Kafka functionality. Develop end-to-end tests using a test microservice. Set up CI pipeline integration for automated testing. Implement test fixtures and mocks for external dependencies. Create performance and load tests for real-time components. Document test coverage and results.",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Update documentation and create examples",
            "description": "Update the existing agent-common documentation and create examples of real-time service implementation.",
            "status": "done",
            "dependencies": [
              2,
              3,
              4,
              6,
              7
            ],
            "details": "Update the existing documentation with new real-time capabilities. Create comprehensive examples of real-time service implementation using the extended library. Document migration paths for existing services to adopt real-time features. Include performance considerations and best practices. Create troubleshooting guides for common issues. Document all configuration options and their default values. Provide architecture diagrams and data flow explanations.",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 24,
        "title": "Implement Application Claim Controller with Realtime-Platform Integration",
        "description": "Enhance the Application Claim Controller to process application claims with realtime parameter, check realtime-platform existence in the cluster, and orchestrate service deployment with smart service management logic.",
        "details": "1. Update the Application Claim Controller in `/Users/socrateshlapolosa/Development/health-service-idp/controllers/application_controller.go` to handle the new realtime parameter:\n\n```go\n// Add to the Application struct or related types\ntype ApplicationSpec struct {\n    // existing fields\n    Realtime *string `json:\"realtime,omitempty\"`\n}\n\n// In the reconciliation logic\nfunc (r *ApplicationReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {\n    // existing code\n    \n    // Check if realtime parameter is specified\n    if app.Spec.Realtime != nil && *app.Spec.Realtime != \"\" {\n        // Verify realtime-platform existence in the cluster\n        exists, err := r.verifyRealtimePlatformExists(ctx, *app.Spec.Realtime, app.Namespace)\n        if err != nil {\n            return ctrl.Result{}, err\n        }\n        \n        if !exists {\n            // Log error and update status\n            log.Error(err, \"Specified realtime-platform does not exist\", \"name\", *app.Spec.Realtime)\n            meta.SetStatusCondition(&app.Status.Conditions, metav1.Condition{\n                Type:    \"RealtimePlatformReady\",\n                Status:  metav1.ConditionFalse,\n                Reason:  \"RealtimePlatformNotFound\",\n                Message: fmt.Sprintf(\"Realtime platform %s not found in namespace %s\", *app.Spec.Realtime, app.Namespace),\n            })\n            return ctrl.Result{}, err\n        }\n        \n        // Call service enhancement orchestration\n        if err := r.orchestrateServiceEnhancement(ctx, app); err != nil {\n            return ctrl.Result{}, err\n        }\n    }\n    \n    // Continue with existing reconciliation\n}\n```\n\n2. Implement the realtime-platform existence verification function:\n\n```go\nfunc (r *ApplicationReconciler) verifyRealtimePlatformExists(ctx context.Context, name, namespace string) (bool, error) {\n    // Create a RealtimePlatformClaim object to look up\n    realtimePlatform := &platformv1alpha1.RealtimePlatformClaim{}\n    \n    // Check if the realtime platform exists\n    err := r.Client.Get(ctx, types.NamespacedName{\n        Name:      name,\n        Namespace: namespace,\n    }, realtimePlatform)\n    \n    if err != nil {\n        if errors.IsNotFound(err) {\n            return false, nil\n        }\n        return false, err\n    }\n    \n    // Check if the realtime platform is ready\n    ready := meta.FindStatusCondition(realtimePlatform.Status.Conditions, \"Ready\")\n    if ready == nil || ready.Status != metav1.ConditionTrue {\n        return false, fmt.Errorf(\"realtime platform %s exists but is not ready\", name)\n    }\n    \n    return true, nil\n}\n```\n\n3. Implement the service enhancement orchestration function that calls the smart service management logic:\n\n```go\nfunc (r *ApplicationReconciler) orchestrateServiceEnhancement(ctx context.Context, app *appv1alpha1.Application) error {\n    // Get the realtime platform details\n    realtimePlatform := &platformv1alpha1.RealtimePlatformClaim{}\n    err := r.Client.Get(ctx, types.NamespacedName{\n        Name:      *app.Spec.Realtime,\n        Namespace: app.Namespace,\n    }, realtimePlatform)\n    if err != nil {\n        return err\n    }\n    \n    // Extract connection secrets from the realtime platform status\n    connectionSecrets := extractConnectionSecrets(realtimePlatform)\n    \n    // Call the smart service management logic\n    serviceManager := smartservice.NewManager(r.Client, r.Scheme)\n    enhancementOptions := smartservice.EnhancementOptions{\n        ApplicationName:    app.Name,\n        Namespace:          app.Namespace,\n        RealtimePlatform:   *app.Spec.Realtime,\n        ConnectionSecrets:  connectionSecrets,\n        TemplateParameters: extractTemplateParameters(app),\n    }\n    \n    return serviceManager.EnhanceOrCreateService(ctx, enhancementOptions)\n}\n\nfunc extractConnectionSecrets(platform *platformv1alpha1.RealtimePlatformClaim) map[string]string {\n    secrets := make(map[string]string)\n    \n    // Extract connection details from platform status\n    if platform.Status.ConnectionDetails != nil {\n        for k, v := range platform.Status.ConnectionDetails {\n            secrets[k] = v\n        }\n    }\n    \n    return secrets\n}\n\nfunc extractTemplateParameters(app *appv1alpha1.Application) map[string]interface{} {\n    params := make(map[string]interface{})\n    \n    // Extract relevant parameters from the application spec\n    // that will be used for template deployment\n    \n    return params\n}\n```\n\n4. Create the smart service management package in `/Users/socrateshlapolosa/Development/health-service-idp/pkg/smartservice/manager.go`:\n\n```go\npackage smartservice\n\nimport (\n    \"context\"\n    \n    \"k8s.io/apimachinery/pkg/runtime\"\n    \"sigs.k8s.io/controller-runtime/pkg/client\"\n)\n\n// EnhancementOptions contains parameters for service enhancement\ntype EnhancementOptions struct {\n    ApplicationName    string\n    Namespace          string\n    RealtimePlatform   string\n    ConnectionSecrets  map[string]string\n    TemplateParameters map[string]interface{}\n}\n\n// Manager handles smart service management operations\ntype Manager struct {\n    client client.Client\n    scheme *runtime.Scheme\n}\n\n// NewManager creates a new smart service manager\nfunc NewManager(client client.Client, scheme *runtime.Scheme) *Manager {\n    return &Manager{\n        client: client,\n        scheme: scheme,\n    }\n}\n\n// EnhanceOrCreateService enhances an existing service or creates a new one\nfunc (m *Manager) EnhanceOrCreateService(ctx context.Context, options EnhancementOptions) error {\n    // Check if service exists\n    exists, err := m.serviceExists(ctx, options.ApplicationName, options.Namespace)\n    if err != nil {\n        return err\n    }\n    \n    if exists {\n        return m.enhanceExistingService(ctx, options)\n    }\n    \n    return m.createNewService(ctx, options)\n}\n\n// serviceExists checks if a service already exists\nfunc (m *Manager) serviceExists(ctx context.Context, name, namespace string) (bool, error) {\n    // Implementation to check if service exists\n    return false, nil\n}\n\n// enhanceExistingService adds realtime capabilities to an existing service\nfunc (m *Manager) enhanceExistingService(ctx context.Context, options EnhancementOptions) error {\n    // Implementation to enhance existing service\n    return nil\n}\n\n// createNewService creates a new service from template\nfunc (m *Manager) createNewService(ctx context.Context, options EnhancementOptions) error {\n    // Implementation to create new service from template\n    return nil\n}\n```\n\n5. Implement template deployment orchestration in `/Users/socrateshlapolosa/Development/health-service-idp/pkg/smartservice/template.go`:\n\n```go\npackage smartservice\n\nimport (\n    \"context\"\n    \"text/template\"\n    \"bytes\"\n    \n    \"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured\"\n    \"k8s.io/apimachinery/pkg/runtime\"\n    \"sigs.k8s.io/yaml\"\n)\n\n// TemplateDeployer handles deployment of service templates\ntype TemplateDeployer struct {\n    client client.Client\n    scheme *runtime.Scheme\n}\n\n// NewTemplateDeployer creates a new template deployer\nfunc NewTemplateDeployer(client client.Client, scheme *runtime.Scheme) *TemplateDeployer {\n    return &TemplateDeployer{\n        client: client,\n        scheme: scheme,\n    }\n}\n\n// DeployTemplate processes and deploys a service template\nfunc (t *TemplateDeployer) DeployTemplate(ctx context.Context, templateName string, params map[string]interface{}) error {\n    // Get template content\n    templateContent, err := t.getTemplateContent(templateName)\n    if err != nil {\n        return err\n    }\n    \n    // Process template with parameters\n    processedTemplate, err := t.processTemplate(templateContent, params)\n    if err != nil {\n        return err\n    }\n    \n    // Convert processed template to Kubernetes resources\n    resources, err := t.convertToResources(processedTemplate)\n    if err != nil {\n        return err\n    }\n    \n    // Apply resources to the cluster\n    return t.applyResources(ctx, resources)\n}\n\n// getTemplateContent retrieves the template content\nfunc (t *TemplateDeployer) getTemplateContent(templateName string) (string, error) {\n    // Implementation to get template content\n    return \"\", nil\n}\n\n// processTemplate applies parameters to the template\nfunc (t *TemplateDeployer) processTemplate(templateContent string, params map[string]interface{}) (string, error) {\n    tmpl, err := template.New(\"service\").Parse(templateContent)\n    if err != nil {\n        return \"\", err\n    }\n    \n    var buf bytes.Buffer\n    if err := tmpl.Execute(&buf, params); err != nil {\n        return \"\", err\n    }\n    \n    return buf.String(), nil\n}\n\n// convertToResources converts YAML to Kubernetes resources\nfunc (t *TemplateDeployer) convertToResources(yamlContent string) ([]unstructured.Unstructured, error) {\n    // Implementation to convert YAML to resources\n    return nil, nil\n}\n\n// applyResources applies resources to the cluster\nfunc (t *TemplateDeployer) applyResources(ctx context.Context, resources []unstructured.Unstructured) error {\n    // Implementation to apply resources\n    return nil\n}\n```\n\n6. Update the controller's RBAC permissions in `/Users/socrateshlapolosa/Development/health-service-idp/controllers/application_controller.go`:\n\n```go\n//+kubebuilder:rbac:groups=platform.example.org,resources=realtimeplatformclaims,verbs=get;list;watch\n//+kubebuilder:rbac:groups=apps,resources=deployments,verbs=get;list;watch;create;update;patch;delete\n//+kubebuilder:rbac:groups=core,resources=services;configmaps;secrets,verbs=get;list;watch;create;update;patch;delete\n```\n\n7. Register the new controller functions in the main.go file to ensure they're properly initialized:\n\n```go\nfunc init() {\n    // Register the new types\n    utilruntime.Must(platformv1alpha1.AddToScheme(scheme))\n}\n\nfunc main() {\n    // Existing code\n    \n    if err = (&controllers.ApplicationReconciler{\n        Client: mgr.GetClient(),\n        Scheme: mgr.GetScheme(),\n        // Add any new dependencies\n    }).SetupWithManager(mgr); err != nil {\n        setupLog.Error(err, \"unable to create controller\", \"controller\", \"Application\")\n        os.Exit(1)\n    }\n}\n```",
        "testStrategy": "1. Unit Testing:\n   - Create unit tests for the `verifyRealtimePlatformExists` function:\n     ```go\n     func TestVerifyRealtimePlatformExists(t *testing.T) {\n         // Test cases:\n         // 1. Platform exists and is ready\n         // 2. Platform exists but is not ready\n         // 3. Platform does not exist\n         // 4. Error retrieving platform\n     }\n     ```\n   - Create unit tests for the `orchestrateServiceEnhancement` function:\n     ```go\n     func TestOrchestrateServiceEnhancement(t *testing.T) {\n         // Test cases:\n         // 1. Successful enhancement\n         // 2. Failed to get realtime platform\n         // 3. Failed to extract connection secrets\n         // 4. Failed to enhance service\n     }\n     ```\n   - Create unit tests for the smart service management logic:\n     ```go\n     func TestEnhanceOrCreateService(t *testing.T) {\n         // Test cases:\n         // 1. Service exists and is enhanced\n         // 2. Service does not exist and is created\n         // 3. Error checking service existence\n         // 4. Error enhancing service\n         // 5. Error creating service\n     }\n     ```\n   - Create unit tests for the template deployment:\n     ```go\n     func TestDeployTemplate(t *testing.T) {\n         // Test cases:\n         // 1. Successful template deployment\n         // 2. Failed to get template content\n         // 3. Failed to process template\n         // 4. Failed to convert to resources\n         // 5. Failed to apply resources\n     }\n     ```\n\n2. Integration Testing:\n   - Create a test application with realtime parameter:\n     ```yaml\n     apiVersion: apps.example.org/v1alpha1\n     kind: Application\n     metadata:\n       name: test-app\n       namespace: default\n     spec:\n       realtime: \"test-realtime-platform\"\n     ```\n   - Create a test realtime platform:\n     ```yaml\n     apiVersion: platform.example.org/v1alpha1\n     kind: RealtimePlatformClaim\n     metadata:\n       name: test-realtime-platform\n       namespace: default\n     spec:\n       # Required fields\n     ```\n   - Deploy both resources and verify:\n     - The controller correctly identifies the realtime parameter\n     - The controller verifies the realtime platform exists\n     - The service enhancement orchestration is called\n     - The template deployment is executed\n\n3. End-to-End Testing:\n   - Deploy a complete application with realtime parameter\n   - Verify that all components are created correctly\n   - Test the application's functionality with the realtime platform\n   - Verify that connection secrets are correctly injected\n   - Test error scenarios:\n     - Non-existent realtime platform\n     - Realtime platform not ready\n     - Failed template deployment\n\n4. Manual Testing:\n   - Use kubectl to inspect the created resources\n   - Verify that the application status correctly reflects the realtime platform integration\n   - Check logs for expected messages during reconciliation\n   - Verify that the application can connect to the realtime platform services",
        "status": "done",
        "dependencies": [
          1,
          2,
          3,
          15,
          21,
          22,
          23
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Application Controller for Realtime Parameter",
            "description": "Modify the Application Controller to handle the new realtime parameter in the ApplicationSpec struct and implement the reconciliation logic to process this parameter.",
            "dependencies": [],
            "details": "1. Add the realtime field to the ApplicationSpec struct\n2. Update the reconciliation logic in Reconcile() function to check for the realtime parameter\n3. Implement validation for the realtime parameter value\n4. Add appropriate logging for realtime parameter processing\n5. Update the controller's RBAC permissions if needed",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Realtime Platform Verification",
            "description": "Create a function to verify if the realtime-platform exists in the cluster and is ready for use before proceeding with application deployment.",
            "dependencies": [
              "24.1"
            ],
            "details": "1. Implement the verifyRealtimePlatformExists() function\n2. Add logic to check if the platform is in a ready state\n3. Implement error handling for cases when the platform doesn't exist or isn't ready\n4. Add appropriate timeout and retry mechanisms\n5. Create unit tests for the verification function with different scenarios",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Smart Service Management Logic",
            "description": "Enhance the controller to orchestrate service deployment with smart service management logic based on the realtime parameter and platform availability.",
            "dependencies": [
              "24.1",
              "24.2"
            ],
            "details": "1. Implement logic to determine which services to deploy based on the realtime parameter\n2. Add conditional deployment of realtime-dependent resources\n3. Implement status updates to reflect realtime platform integration status\n4. Add reconciliation logic to handle changes in realtime parameter value\n5. Implement cleanup logic for when realtime is disabled or platform becomes unavailable",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 25,
        "title": "Create Generic Domain-Agnostic Real-time Platform Requirements",
        "description": "Update all existing components and documentation to ensure the real-time platform is domain-agnostic rather than healthcare-specific, defining generic streaming use cases and terminology.",
        "details": "1. Review all existing tasks and components to identify healthcare-specific terminology:\n   - Scan through realtime-component-definitions.yaml, realtime-xrds.yaml, realtime-compositions.yaml\n   - Identify terms like \"blood pressure\", \"heart rate\", \"patient\", \"clinical\", etc.\n   - Create a mapping document of healthcare terms to generic equivalents\n\n2. Update schema definitions and parameter names:\n   - Replace healthcare metrics with generic terms:\n     * \"blood_pressure\" → \"sensor_data\"\n     * \"heart_rate\" → \"device_telemetry\" \n     * \"patient_vitals\" → \"metrics\"\n   - Update all topic naming conventions to use generic patterns\n   - Modify example data structures to use domain-agnostic field names\n\n3. Redefine the generic real-time streaming use case:\n   - Document the workflow: Developer creates OAM definition → Platform provisions MQTT broker (exposed via Knative), Kafka cluster, stream processing, and WebSocket endpoints → Developer can send test data to MQTT endpoint and receive processed results via WebSocket\n   - Create sequence diagrams showing the generic data flow\n   - Update architectural documentation to use industry-standard IoT/streaming terminology\n\n4. Modify example OAM definitions:\n   - Update MINIMAL-REALTIME-OAM.yaml and REALTIME-OAM-EXAMPLE.yaml with generic examples\n   - Create sample data generators that produce generic telemetry data\n   - Provide examples of generic stream processing transformations\n\n5. Update documentation:\n   - Revise user guides to use domain-agnostic terminology\n   - Create new examples showing how the platform can be used in different industries (manufacturing, transportation, smart cities)\n   - Update troubleshooting guides with generic scenarios",
        "testStrategy": "1. Terminology Audit:\n   - Create a comprehensive checklist of all files and components\n   - Verify each file has been reviewed and all healthcare-specific terms have been replaced\n   - Have a team member not familiar with healthcare terminology review all documentation to identify any remaining domain-specific language\n\n2. Schema Validation:\n   - Validate all updated YAML files for syntax correctness\n   - Test that the modified schemas accept generic data examples\n   - Verify that the schemas reject malformed data appropriately\n\n3. End-to-End Testing:\n   - Deploy the updated platform with generic configurations\n   - Send generic sensor data through the MQTT broker\n   - Verify data flows through Kafka topics with new naming conventions\n   - Confirm processed data is available via WebSocket endpoints\n   - Test that visualization components correctly display generic metrics\n\n4. Documentation Testing:\n   - Have developers from different domains (not healthcare) review the documentation\n   - Ask them to follow the guides to implement a simple use case\n   - Collect feedback on any remaining healthcare-specific terminology or assumptions\n   - Verify all examples work as described with generic data types",
        "status": "pending",
        "dependencies": [
          1,
          2,
          3,
          4,
          16,
          17,
          20
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Audit and map healthcare-specific terminology",
            "description": "Conduct a comprehensive audit of all components and documentation to identify healthcare-specific terminology and create a mapping document for generic replacements.",
            "dependencies": [],
            "details": "Review all files including realtime-component-definitions.yaml, realtime-xrds.yaml, realtime-compositions.yaml, and example files. Create a spreadsheet with columns for: original healthcare term, generic replacement term, files where found, and context of usage. Focus on terms like 'patient', 'vital signs', 'blood pressure', 'heart rate', 'clinical', 'medical', etc. Ensure replacements maintain technical accuracy while being applicable across multiple domains.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Update schema definitions and parameter names",
            "description": "Modify all schema definitions, parameter names, and data models to use domain-agnostic terminology based on the mapping document.",
            "dependencies": [
              "25.1"
            ],
            "details": "Replace healthcare-specific schema elements with generic equivalents (e.g., 'blood_pressure' → 'sensor_data', 'patient_id' → 'entity_id'). Update all parameter names in component definitions, XRDs, and compositions. Ensure data types and relationships are preserved during renaming. Document any breaking changes that might affect existing implementations.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Redefine generic real-time streaming use cases",
            "description": "Develop new domain-agnostic use cases that demonstrate the platform's versatility across different industries.",
            "dependencies": [
              "25.1",
              "25.2"
            ],
            "details": "Create at least three generic use cases that showcase the platform's capabilities: 1) Industrial IoT sensor monitoring, 2) Financial transaction processing, and 3) Supply chain event tracking. For each use case, define the data sources, processing requirements, and visualization needs. Ensure these examples can be implemented using the updated components.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Update example YAML files with domain-agnostic configurations",
            "description": "Revise all example YAML files to use generic terminology and demonstrate cross-industry applications.",
            "dependencies": [
              "25.1",
              "25.2",
              "25.3"
            ],
            "details": "Update MINIMAL-REALTIME-OAM.yaml and REALTIME-OAM-EXAMPLE.yaml with domain-agnostic configurations. Create additional examples that demonstrate the platform's use in different industries based on the new use cases. Ensure examples include comments explaining how to adapt them for specific domains.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Revise documentation and user guides",
            "description": "Update all documentation to reflect domain-agnostic terminology and provide guidance for industry-specific adaptations.",
            "dependencies": [
              "25.1",
              "25.2",
              "25.3",
              "25.4"
            ],
            "details": "Revise user guides, architecture documentation, and operational runbooks to use generic terminology. Add sections on how to adapt the platform for specific industries. Update troubleshooting guides to cover generic scenarios. Create a new 'Industry Adaptation Guide' that provides templates for customizing the platform for specific domains.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement and test domain-agnostic validation",
            "description": "Update validation logic to ensure it works with generic data models and perform comprehensive testing across multiple domain scenarios.",
            "dependencies": [
              "25.2",
              "25.3",
              "25.4"
            ],
            "details": "Modify any validation rules that were healthcare-specific to work with generic data models. Create test cases for each of the new use cases to verify platform functionality. Test data ingestion, processing, and visualization with sample data from different domains. Document validation results and any edge cases discovered during testing.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 26,
        "title": "Implement End-to-End Test Data Flow for Real-time Platform",
        "description": "Create a comprehensive test flow that validates the complete data pipeline from MQTT ingestion through Kafka topics and stream processing to WebSocket output, with test data generators and automated verification scripts.",
        "details": "Implement the following components for end-to-end testing of the real-time platform:\n\n1. Test Data Generators:\n   - Create a Python-based test data generator for simulated sensor data (temperature, pressure, velocity, etc.)\n   - Implement configurable data generation patterns (steady, random, sine wave, spike)\n   - Support batch and continuous data generation modes\n   - Include device ID and timestamp in generated data\n   - Format data according to the platform's Avro schemas\n\n2. MQTT Connectivity:\n   - Expose MQTT broker endpoints using Knative serving\n   - Configure the following endpoints:\n     - `/mqtt/publish` - HTTP endpoint that converts POST requests to MQTT messages\n     - `/mqtt/subscribe` - WebSocket endpoint for subscribing to MQTT topics\n   - Implement proper authentication and TLS for external connectivity\n   - Configure Istio ingress gateway rules for these endpoints\n\n3. WebSocket Consumer Service:\n   - Develop a Kafka consumer service that subscribes to processed data topics\n   - Implement WebSocket server using Spring Boot or Node.js\n   - Create endpoints for each specialized data type (blood_pressure, heart_rate, etc.)\n   - Support filtering and query parameters for data selection\n   - Implement proper error handling and reconnection logic\n   - Expose service via Istio ingress gateway\n\n4. Automated Test Scripts:\n   - Create a test suite using Python or JavaScript that:\n     - Generates and sends sample data via MQTT\n     - Verifies data appears in Kafka topics using Kafka consumer API\n     - Confirms data is processed correctly by stream processing\n     - Validates processed data is available via WebSocket endpoints\n   - Implement test scenarios for:\n     - Normal operation with various data types\n     - Error conditions (malformed data, connection drops)\n     - High volume data testing\n     - Long-running stability tests\n\n5. Monitoring Dashboard:\n   - Create a simple dashboard in Metabase to visualize test results\n   - Include metrics for end-to-end latency, message counts, and error rates\n   - Set up alerts for test failures\n\nAll components should be containerized and deployed as part of the real-time platform composition. Update the realtime-compositions.yaml file to include these test components with appropriate dependencies on the core platform services.",
        "testStrategy": "1. Unit Testing:\n   - Test each component in isolation with mock dependencies\n   - Verify data generator produces correctly formatted data\n   - Test MQTT endpoints with direct connections\n   - Validate WebSocket service with simulated Kafka messages\n\n2. Integration Testing:\n   - Deploy the complete test infrastructure alongside the real-time platform\n   - Verify connectivity between all components:\n     - Data generator → MQTT broker\n     - MQTT broker → Kafka (via connector)\n     - Kafka → Stream processing\n     - Stream processing → Processed Kafka topics\n     - Processed topics → WebSocket service\n   - Confirm data integrity is maintained throughout the pipeline\n\n3. End-to-End Testing:\n   - Run the automated test scripts against the deployed platform\n   - Verify data flows from generator to WebSocket output\n   - Measure and record end-to-end latency\n   - Test with different data volumes (10, 100, 1000 messages per second)\n   - Verify all data types are correctly processed\n\n4. Security Testing:\n   - Attempt to connect to MQTT and WebSocket endpoints without authentication\n   - Verify TLS is properly configured on all external endpoints\n   - Test Istio ingress rules for proper access control\n\n5. Performance Testing:\n   - Run sustained load tests (1 hour minimum)\n   - Monitor system resource usage during tests\n   - Identify bottlenecks in the processing pipeline\n\n6. Validation:\n   - Verify test results in the monitoring dashboard\n   - Confirm all test scenarios pass consistently\n   - Document any issues or limitations discovered",
        "status": "pending",
        "dependencies": [
          9,
          10,
          11,
          12,
          13,
          18,
          19
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Test Data Generator",
            "description": "Create a Python-based test data generator that simulates sensor data with configurable patterns and formats according to the platform's Avro schemas.",
            "dependencies": [],
            "details": "Develop a Python script that generates test data with the following capabilities:\n- Generate simulated sensor data (temperature, pressure, velocity, etc.)\n- Support configurable data patterns (steady, random, sine wave, spike)\n- Implement both batch and continuous generation modes\n- Include device ID and timestamp in all generated data\n- Format output according to the platform's Avro schemas\n- Add command-line parameters to control generation rate, pattern type, and output destination\n- Include documentation on usage and examples",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Develop End-to-End Test Pipeline",
            "description": "Create a test pipeline that validates data flow from MQTT ingestion through Kafka topics and stream processing to WebSocket output.",
            "dependencies": [
              "26.1"
            ],
            "details": "Implement a comprehensive test pipeline with these components:\n- MQTT connection module to publish test data to the broker\n- Kafka consumer to verify data reaches the correct topics\n- Stream processing validation to ensure transformations are applied correctly\n- WebSocket client to verify final output data\n- Logging system to capture metrics at each stage of the pipeline\n- Configuration file to set connection parameters for all components\n- Error handling and reporting mechanisms",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create Automated Verification Scripts",
            "description": "Develop automated verification scripts that validate data integrity and system performance throughout the end-to-end flow.",
            "dependencies": [
              "26.1",
              "26.2"
            ],
            "details": "Create verification scripts that:\n- Compare input test data with output data to verify data integrity\n- Measure and report latency at each stage of the pipeline\n- Validate security measures including authentication and encryption\n- Test system behavior under various load conditions\n- Generate detailed test reports with pass/fail status\n- Implement CI/CD integration for automated testing\n- Create dashboards to visualize test results and system performance metrics",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 28,
        "title": "Enhance Secret Management for WebService Integration",
        "description": "Expand the secret management system to support enhanced webservice integration with realtime-platform components, implementing cross-component discovery mechanisms and standardized secret naming conventions.",
        "details": "1. Update the secret management logic in `/Users/socrateshlapolosa/Development/health-service-idp/controllers/secret_controller.go` to implement:\n\n   a. Standardized secret naming using the `{name}-{service}-secret` pattern for all microservice integrations\n   ```go\n   func generateSecretName(componentName, serviceName string) string {\n       return fmt.Sprintf(\"%s-%s-secret\", componentName, serviceName)\n   }\n   ```\n\n   b. Cross-component secret discovery mechanism that allows webservices to discover secrets from realtime-platform components:\n   ```go\n   func (r *SecretController) discoverRealtimePlatformSecrets(ctx context.Context, realtimeName string) ([]corev1.Secret, error) {\n       // Query for secrets belonging to the specified realtime platform\n       secretList := &corev1.SecretList{}\n       if err := r.List(ctx, secretList, client.MatchingLabels{\n           \"app.kubernetes.io/part-of\": \"realtime-platform\",\n           \"realtime.platform.example.org/name\": realtimeName,\n       }); err != nil {\n           return nil, fmt.Errorf(\"failed to list realtime platform secrets: %w\", err)\n       }\n       \n       return secretList.Items, nil\n   }\n   ```\n\n   c. Secret injection capability for webservice components:\n   ```go\n   func (r *SecretController) injectSecretsToWebservice(ctx context.Context, webservice *appsv1.Deployment, secrets []corev1.Secret) error {\n       // For each container in the deployment\n       for i := range webservice.Spec.Template.Spec.Containers {\n           container := &webservice.Spec.Template.Spec.Containers[i]\n           \n           // Add environment variables from each secret\n           for _, secret := range secrets {\n               container.EnvFrom = append(container.EnvFrom, corev1.EnvFromSource{\n                   SecretRef: &corev1.SecretEnvSource{\n                       LocalObjectReference: corev1.LocalObjectReference{\n                           Name: secret.Name,\n                       },\n                   },\n               })\n           }\n       }\n       \n       return r.Update(ctx, webservice)\n   }\n   ```\n\n2. Modify the webservice ComponentDefinition in `/Users/socrateshlapolosa/Development/health-service-idp/crossplane/oam/application-component-definitions.yaml` to add logic for automatic secret reference injection when the realtime parameter is specified:\n   ```cue\n   parameter: {\n       // existing parameters\n       realtime?: string\n   }\n   \n   // In the template section\n   if parameter.realtime != _|_ {\n       spec: template: spec: {\n           // Add annotations for secret discovery\n           metadata: annotations: {\n               \"realtime.platform.example.org/integration\": parameter.realtime\n           }\n           // Add init container for secret validation\n           initContainers: [{\n               name: \"secret-validator\"\n               image: \"bitnami/kubectl:latest\"\n               command: [\"sh\", \"-c\"]\n               args: [\"kubectl get secret $(REALTIME_NAME)-connection-secret || exit 1\"]\n               env: [{\n                   name: \"REALTIME_NAME\"\n                   value: parameter.realtime\n               }]\n           }]\n       }\n   }\n   ```\n\n3. Update the Application Claim Controller in `/Users/socrateshlapolosa/Development/health-service-idp/controllers/application_controller.go` to ensure secrets are discoverable:\n   ```go\n   // Add to the reconciliation logic\n   func (r *ApplicationReconciler) ensureSecretDiscoverability(ctx context.Context, app *platformv1alpha1.Application) error {\n       if app.Spec.Realtime == nil {\n           return nil // No realtime integration, nothing to do\n       }\n       \n       // Find all secrets related to the realtime platform\n       realtimeSecrets, err := r.secretController.discoverRealtimePlatformSecrets(ctx, *app.Spec.Realtime)\n       if err != nil {\n           return err\n       }\n       \n       // Add labels to make secrets discoverable by the application\n       for i := range realtimeSecrets {\n           secret := &realtimeSecrets[i]\n           if secret.Labels == nil {\n               secret.Labels = make(map[string]string)\n           }\n           secret.Labels[\"app.kubernetes.io/managed-by\"] = app.Name\n           secret.Labels[\"app.kubernetes.io/discoverable\"] = \"true\"\n           \n           if err := r.Update(ctx, secret); err != nil {\n               return fmt.Errorf(\"failed to update secret %s: %w\", secret.Name, err)\n           }\n       }\n       \n       return nil\n   }\n   ```\n\n4. Create a new SecretInjector controller that watches for deployments with the realtime integration annotation and injects the appropriate secrets:\n   ```go\n   // In a new file: controllers/secret_injector_controller.go\n   type SecretInjectorController struct {\n       client.Client\n       Log logr.Logger\n       Scheme *runtime.Scheme\n   }\n   \n   // +kubebuilder:rbac:groups=apps,resources=deployments,verbs=get;list;watch;update;patch\n   // +kubebuilder:rbac:groups=core,resources=secrets,verbs=get;list;watch\n   \n   func (r *SecretInjectorController) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {\n       log := r.Log.WithValues(\"deployment\", req.NamespacedName)\n       \n       // Get the deployment\n       deployment := &appsv1.Deployment{}\n       if err := r.Get(ctx, req.NamespacedName, deployment); err != nil {\n           return ctrl.Result{}, client.IgnoreNotFound(err)\n       }\n       \n       // Check if this deployment has realtime integration\n       realtimeName, ok := deployment.Annotations[\"realtime.platform.example.org/integration\"]\n       if !ok {\n           return ctrl.Result{}, nil // No realtime integration\n       }\n       \n       // Discover realtime platform secrets\n       secrets, err := r.discoverRealtimePlatformSecrets(ctx, realtimeName)\n       if err != nil {\n           log.Error(err, \"Failed to discover realtime platform secrets\")\n           return ctrl.Result{}, err\n       }\n       \n       // Inject secrets into the deployment\n       if err := r.injectSecretsToWebservice(ctx, deployment, secrets); err != nil {\n           log.Error(err, \"Failed to inject secrets into webservice\")\n           return ctrl.Result{}, err\n       }\n       \n       log.Info(\"Successfully injected realtime platform secrets\", \"count\", len(secrets))\n       return ctrl.Result{}, nil\n   }\n   ```\n\n5. Register the new controller in `main.go`:\n   ```go\n   if err = (&controllers.SecretInjectorController{\n       Client: mgr.GetClient(),\n       Log:    ctrl.Log.WithName(\"controllers\").WithName(\"SecretInjector\"),\n       Scheme: mgr.GetScheme(),\n   }).SetupWithManager(mgr); err != nil {\n       setupLog.Error(err, \"unable to create controller\", \"controller\", \"SecretInjector\")\n       os.Exit(1)\n   }\n   ```",
        "testStrategy": "1. Unit Testing:\n   - Create unit tests for the secret naming standardization function:\n     ```go\n     func TestGenerateSecretName(t *testing.T) {\n         testCases := []struct {\n             componentName string\n             serviceName   string\n             expected      string\n         }{\n             {\"myapp\", \"postgres\", \"myapp-postgres-secret\"},\n             {\"health-service\", \"kafka\", \"health-service-kafka-secret\"},\n             {\"\", \"redis\", \"-redis-secret\"}, // Edge case\n         }\n         \n         for _, tc := range testCases {\n             result := generateSecretName(tc.componentName, tc.serviceName)\n             if result != tc.expected {\n                 t.Errorf(\"generateSecretName(%s, %s) = %s; want %s\", \n                     tc.componentName, tc.serviceName, result, tc.expected)\n             }\n         }\n     }\n     ```\n   \n   - Test the secret discovery mechanism with mock client:\n     ```go\n     func TestDiscoverRealtimePlatformSecrets(t *testing.T) {\n         // Setup mock client with predefined secrets\n         mockClient := fake.NewClientBuilder().\n             WithObjects(\n                 &corev1.Secret{\n                     ObjectMeta: metav1.ObjectMeta{\n                         Name: \"test-kafka-secret\",\n                         Labels: map[string]string{\n                             \"app.kubernetes.io/part-of\": \"realtime-platform\",\n                             \"realtime.platform.example.org/name\": \"test-realtime\",\n                         },\n                     },\n                 },\n                 &corev1.Secret{\n                     ObjectMeta: metav1.ObjectMeta{\n                         Name: \"test-postgres-secret\",\n                         Labels: map[string]string{\n                             \"app.kubernetes.io/part-of\": \"realtime-platform\",\n                             \"realtime.platform.example.org/name\": \"test-realtime\",\n                         },\n                     },\n                 },\n             ).Build()\n         \n         controller := &SecretController{\n             Client: mockClient,\n         }\n         \n         secrets, err := controller.discoverRealtimePlatformSecrets(context.Background(), \"test-realtime\")\n         if err != nil {\n             t.Fatalf(\"discoverRealtimePlatformSecrets failed: %v\", err)\n         }\n         \n         if len(secrets) != 2 {\n             t.Errorf(\"Expected 2 secrets, got %d\", len(secrets))\n         }\n     }\n     ```\n\n2. Integration Testing:\n   - Create a test webservice component that references a realtime platform:\n     ```yaml\n     apiVersion: core.oam.dev/v1beta1\n     kind: Component\n     metadata:\n       name: test-webservice\n     spec:\n       type: webservice\n       properties:\n         image: nginx:latest\n         realtime: \"test-realtime\"\n     ```\n   \n   - Create a test realtime platform with secrets:\n     ```yaml\n     apiVersion: platform.example.org/v1alpha1\n     kind: RealtimePlatformClaim\n     metadata:\n       name: test-realtime\n     spec:\n       name: test-realtime\n     ```\n   \n   - Verify that the webservice deployment has the correct annotations and init containers\n   - Verify that the secrets from the realtime platform are correctly labeled for discovery\n   - Verify that the SecretInjector controller correctly injects the secrets into the webservice deployment\n\n3. End-to-End Testing:\n   - Deploy a complete application with webservice and realtime platform components\n   - Verify that the webservice can access the secrets from the realtime platform\n   - Test the application functionality that depends on the realtime platform integration\n   - Verify that the secrets follow the standardized naming convention\n   - Test the behavior when the realtime platform is deleted or unavailable\n\n4. Security Testing:\n   - Verify that secrets are only accessible to the components that should have access to them\n   - Test RBAC permissions to ensure proper access control\n   - Verify that secret values are not exposed in logs or error messages\n   - Test the behavior when a component tries to access a secret it doesn't have permission to access",
        "status": "pending",
        "dependencies": [
          15,
          21,
          24,
          3
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement standardized secret naming convention",
            "description": "Update the secret management logic to implement a standardized naming pattern for all microservice integrations",
            "dependencies": [],
            "details": "1. Modify the secret_controller.go file to add the generateSecretName function\n2. Update existing secret creation functions to use this standardized pattern\n3. Add unit tests for the naming function\n4. Document the naming convention in code comments and developer documentation",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Develop cross-component secret discovery mechanism",
            "description": "Create a mechanism that allows webservices to discover and access secrets from other components",
            "dependencies": [
              "28.1"
            ],
            "details": "1. Implement a discovery service in secret_controller.go that can query for secrets based on component relationships\n2. Add authentication and authorization checks for secret access\n3. Create an API endpoint for secret discovery requests\n4. Implement caching for frequently accessed secrets to improve performance",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Integrate with existing realtime-platform components",
            "description": "Connect the enhanced secret management system with the realtime-platform components",
            "dependencies": [
              "28.1",
              "28.2"
            ],
            "details": "1. Update the Application Claim Controller to handle realtime parameter secrets\n2. Modify the secret injection logic to properly format secrets for realtime-platform components\n3. Implement validation to ensure referenced realtime-platform components exist\n4. Add integration tests that verify end-to-end secret management with realtime components",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 29,
        "title": "Consolidate PostgreSQL Implementation for All Use Cases",
        "description": "Update the existing PostgreSQL implementation to serve as a unified backend for both general platform and real-time platform needs, supporting multiple database instances with a flexible, reusable component design.",
        "details": "1. Review the existing PostgreSQL implementation in Task 5 and identify all current use cases:\n   - Real-time platform databases (hq, agent1, metabaseappdb)\n   - General platform database requirements\n\n2. Modify the PostgreSQL component in realtime-compositions.yaml to:\n   - Create a more flexible database configuration that can handle both standalone and multi-database setups\n   - Support dynamic database creation based on input parameters\n   - Allow for custom user permissions per database\n   - Implement connection pooling for improved performance\n\n3. Update the component to support the following database configurations:\n   - General platform database (postgres)\n   - Lenses HQ database (hq)\n   - Lenses Agent database (agent1)\n   - Metabase database (metabaseappdb)\n\n4. Implement a parameter-driven approach that allows:\n   - Specification of multiple databases in a single PostgreSQL instance\n   - Custom resource allocation per database type\n   - Flexible authentication mechanisms\n   - Optional high availability configuration\n\n5. Ensure backward compatibility with existing claims by:\n   - Maintaining the same secret output format\n   - Supporting existing parameter names\n   - Adding new parameters with sensible defaults\n\n6. Update the Crossplane composition to:\n   - Use a single PostgreSQL component definition for all use cases\n   - Support conditional configuration based on claim type\n   - Generate appropriate connection secrets for each database\n\n7. Implement proper resource management:\n   - Configure appropriate resource requests/limits\n   - Set up backup and recovery mechanisms\n   - Implement monitoring and health checks\n\n8. Document the new unified PostgreSQL component:\n   - Create usage examples for different scenarios\n   - Document all available parameters and their defaults\n   - Provide migration guidance for existing implementations",
        "testStrategy": "1. Unit Testing:\n   - Validate the updated PostgreSQL component definition syntax\n   - Verify that all parameters are correctly defined and have appropriate defaults\n   - Test parameter validation logic\n\n2. Integration Testing:\n   - Deploy a standalone PostgreSQL instance and verify it works correctly\n   - Deploy a multi-database PostgreSQL instance with all required databases (hq, agent1, metabaseappdb)\n   - Test that each database is properly initialized with correct schemas and permissions\n\n3. Compatibility Testing:\n   - Test with existing claims to ensure backward compatibility\n   - Verify that existing applications can connect using the same connection secrets\n   - Test migration from old to new implementation\n\n4. Performance Testing:\n   - Benchmark the consolidated PostgreSQL implementation under various loads\n   - Test connection pooling effectiveness\n   - Verify resource utilization is appropriate for the configured workload\n\n5. Functional Testing:\n   - Verify that Lenses HQ can connect to its database\n   - Verify that Lenses Agent can connect to its database\n   - Verify that Metabase can connect to its database\n   - Test general platform applications with the unified PostgreSQL component\n\n6. Security Testing:\n   - Verify proper isolation between databases\n   - Test that user permissions are correctly applied\n   - Validate that connection secrets are properly secured\n\n7. Documentation Testing:\n   - Review documentation for completeness and accuracy\n   - Test examples to ensure they work as documented\n   - Verify migration guidance is clear and effective",
        "status": "done",
        "dependencies": [
          7,
          10
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze current PostgreSQL implementations and use cases",
            "description": "Review all existing PostgreSQL implementations across the platform to identify common patterns, requirements, and use cases that need to be supported in the consolidated implementation.",
            "dependencies": [],
            "details": "1. Review Task 5 implementation details for the real-time platform databases (hq, agent1, metabaseappdb)\n2. Analyze database requirements from Task 13 (Metabase) and other components\n3. Document all connection patterns, authentication methods, and permission requirements\n4. Identify volume mount and persistence requirements across implementations\n5. Create a comprehensive list of all database instances needed across the platform\n6. Document initialization requirements for each database instance",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Design flexible PostgreSQL component architecture",
            "description": "Create a flexible component architecture that can support both standalone and multi-database PostgreSQL deployments with configurable parameters for different use cases.",
            "dependencies": [
              "29.1"
            ],
            "details": "1. Design a parameterized PostgreSQL component that can be configured for different deployment scenarios\n2. Create a schema for dynamic database creation based on input parameters\n3. Design a permission model that allows for custom user permissions per database\n4. Define configuration options for resource allocation (CPU, memory, storage)\n5. Create a connection secret format that works for all consumers\n6. Design initialization scripts structure for different database types\n7. Document the component architecture with examples for different use cases",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement and test consolidated PostgreSQL component",
            "description": "Update the realtime-compositions.yaml file with the new consolidated PostgreSQL implementation and thoroughly test it with all identified use cases.",
            "dependencies": [
              "29.2"
            ],
            "details": "1. Implement the consolidated PostgreSQL component in realtime-compositions.yaml\n2. Create reusable templates for database initialization\n3. Implement dynamic database and user creation logic\n4. Configure appropriate security settings (network policies, authentication)\n5. Set up comprehensive testing for all identified use cases\n6. Test with Metabase, Lenses HQ, and Lenses Agent configurations\n7. Verify that all components can connect properly to their respective databases\n8. Document the implementation with usage examples for future reference",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 30,
        "title": "Implement Simple Ping-Pong Default Service",
        "description": "Create a default simple ping-pong service that demonstrates the full real-time platform functionality with a message flow from MQTT to Kafka to stream processing to WebSocket.",
        "details": "Implement a simple ping-pong service that showcases the complete real-time platform functionality:\n\n1. Avro Schema Definition:\n   - Create two Avro schemas in the `/schemas` directory:\n     - `ping-message.avsc`: Define a simple schema with fields for message content and timestamp\n     - `pong-message.avsc`: Define a schema that extends the ping message with a response field\n\n2. Kafka Topic Configuration:\n   - Configure two Kafka topics in the Lenses platform:\n     - `ping_topic`: For incoming ping messages\n     - `pong_topic`: For outgoing pong responses\n\n3. MQTT to Kafka Bridge:\n   - Configure the MQTT Source Connector to:\n     - Subscribe to the MQTT topic `ping/messages`\n     - Convert and forward messages to the Kafka `ping_topic`\n     - Use the ping Avro schema for serialization\n\n4. Stream Processing Query:\n   - Implement a Lenses SQL query that:\n     - Reads from `ping_topic`\n     - Transforms each message by prepending \"pong: \" to the original message\n     - Writes the result to `pong_topic`\n     - SQL example: `INSERT INTO pong_topic SELECT STRUCT(message := CONCAT('pong: ', message), timestamp := timestamp) FROM ping_topic`\n\n5. WebSocket Endpoint:\n   - Create a WebSocket service that:\n     - Subscribes to the `pong_topic` Kafka topic\n     - Streams messages in real-time to connected clients\n     - Exposes an endpoint at `/ws/pong`\n     - Handles client connections and disconnections properly\n\n6. Test Data Generator:\n   - Implement a simple Python script that:\n     - Connects to the MQTT broker\n     - Periodically publishes ping messages to the `ping/messages` topic\n     - Allows configuration of message rate and content\n\n7. Configuration in realtime-compositions.yaml:\n   - Add the necessary configurations to deploy all components\n   - Ensure proper connectivity between MQTT, Kafka, and WebSocket services\n   - Configure health checks for the new components\n\nThis implementation will demonstrate the complete flow: MQTT → Kafka → Stream Processing → WebSocket, providing a simple but functional example of the platform's capabilities.",
        "testStrategy": "1. Unit Testing:\n   - Validate Avro schemas using the Avro schema validator\n   - Test the stream processing query with sample data in Lenses\n   - Verify WebSocket endpoint functionality with a direct connection\n\n2. Integration Testing:\n   - Deploy the complete ping-pong service\n   - Run the test data generator to publish ping messages to MQTT\n   - Verify messages flow through Kafka topics using Lenses UI\n   - Confirm transformed messages appear in the pong_topic\n   - Connect to the WebSocket endpoint using a browser-based client\n   - Verify that \"pong: \" messages are received in real-time\n\n3. End-to-End Testing:\n   - Create a simple web page that:\n     - Allows users to input ping messages\n     - Publishes them to the MQTT broker\n     - Connects to the WebSocket endpoint\n     - Displays received pong messages\n   - Test the complete flow with various message types and rates\n   - Verify latency is within acceptable limits (< 500ms)\n\n4. Performance Testing:\n   - Test with increasing message rates (1/sec, 10/sec, 100/sec)\n   - Monitor system performance and resource usage\n   - Identify any bottlenecks in the processing pipeline\n\n5. Documentation Verification:\n   - Ensure all components are properly documented\n   - Verify that the service can be used as a reference implementation",
        "status": "done",
        "dependencies": [
          8,
          9,
          10,
          11,
          12
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Avro Schemas for Ping-Pong Service",
            "description": "Define the Avro schemas for ping and pong messages that will be used in the service",
            "dependencies": [],
            "details": "Create two Avro schema files in the `/schemas` directory:\n1. `ping-message.avsc`: Include fields for message content, timestamp, and a unique identifier\n2. `pong-message.avsc`: Extend the ping message schema with a response field and processing timestamp\nEnsure schemas are properly formatted and validated",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Configure Kafka Topics and Stream Processing",
            "description": "Set up the Kafka topics and implement the stream processing logic for the ping-pong service",
            "dependencies": [
              "30.1"
            ],
            "details": "1. Configure two Kafka topics in the Lenses platform:\n   - `ping_topic`: For incoming ping messages\n   - `pong_topic`: For outgoing pong responses\n2. Implement a stream processing query in Lenses SQL that:\n   - Consumes messages from the ping_topic\n   - Transforms them by adding a response field\n   - Produces the result to the pong_topic\n3. Ensure proper error handling and monitoring",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement MQTT to WebSocket Message Flow",
            "description": "Create the complete message flow from MQTT input to WebSocket output",
            "dependencies": [
              "30.1",
              "30.2"
            ],
            "details": "1. Configure MQTT broker to accept ping messages on the topic `device/ping`\n2. Implement a connector to transfer messages from MQTT to Kafka ping_topic\n3. Set up a WebSocket server endpoint that subscribes to the pong_topic\n4. Create a simple web interface that demonstrates the ping-pong functionality\n5. Document the complete flow with a sequence diagram",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 31,
        "title": "Implement Real-time Service Following GitHub Template with Onion Architecture",
        "description": "Create a real-time service implementation based on the GitHub template structure with Onion Architecture, FastAPI, WebSocket endpoints, and Kafka integration while following 12-factor app principles.",
        "details": "Implement a real-time service following the structure from https://github.com/heath-health/realtime_data_pipeline/tree/main/realtime_service while incorporating Onion Architecture principles from CLAUDE.md:\n\n1. Set up the project structure with Onion Architecture layers:\n   - `domain/`: Core domain models, entities, and business logic\n   - `application/`: Use cases, application services, and orchestration\n   - `infrastructure/`: External systems integration (Kafka, databases)\n   - `interface/`: API endpoints, controllers, and presentation logic\n\n2. Implement the FastAPI application with dependency injection:\n   - Create `src/main.py` as the entry point\n   - Use `fastapi.Depends` for dependency injection\n   - Set up proper middleware for logging, error handling, and CORS\n\n3. Create a modular routes package:\n   - Implement `routes/__init__.py` with route registration functions\n   - Create separate route modules for different functional areas\n   - Ensure routes are properly registered with the FastAPI app\n\n4. Implement WebSocket endpoint:\n   - Create a `/ws` endpoint for real-time data streaming\n   - Implement connection management for multiple clients\n   - Add proper error handling and reconnection logic\n\n5. Set up aiokafka consumer:\n   - Configure async Kafka consumer for topic consumption\n   - Implement proper message deserialization (JSON/Avro)\n   - Create message routing to appropriate WebSocket clients\n\n6. Implement configuration management:\n   - Use environment variables for all configuration\n   - Create a secret-based configuration system\n   - Follow 12-factor app principles for configuration\n\n7. Add health check endpoints:\n   - Implement `/health` and `/readiness` endpoints\n   - Add Kafka connection status checks\n   - Include system diagnostics information\n\n8. Create a multi-stage Dockerfile:\n   - Use Python slim base image\n   - Implement proper caching for dependencies\n   - Optimize for security and size\n\n9. Set up Poetry for dependency management:\n   - Create `pyproject.toml` with all required dependencies\n   - Include development dependencies for testing\n   - Configure Poetry settings for reproducible builds\n\n10. Implement proper logging:\n    - Set up structured logging with JSON format\n    - Include request ID for traceability\n    - Configure log levels based on environment\n\n11. Add unit and integration tests:\n    - Create test fixtures for FastAPI and WebSocket testing\n    - Implement Kafka consumer mocks\n    - Set up CI/CD pipeline integration",
        "testStrategy": "1. Unit Testing:\n   - Test each layer of the Onion Architecture independently\n   - Verify domain models and business logic with unit tests\n   - Test application services with mocked dependencies\n   - Verify infrastructure adapters with integration tests\n   - Test interface controllers with FastAPI TestClient\n\n2. WebSocket Testing:\n   - Create WebSocket client test fixtures\n   - Test connection establishment and message handling\n   - Verify reconnection logic works correctly\n   - Test multiple concurrent client connections\n   - Validate proper message routing from Kafka to WebSockets\n\n3. Kafka Integration Testing:\n   - Set up test Kafka topics with test data\n   - Verify consumer group configuration\n   - Test message deserialization with various formats\n   - Validate error handling for malformed messages\n   - Test reconnection to Kafka after connection loss\n\n4. Configuration Testing:\n   - Verify environment variable loading\n   - Test secret-based configuration\n   - Validate configuration validation logic\n   - Test configuration defaults and overrides\n\n5. End-to-End Testing:\n   - Deploy the service in a test environment\n   - Connect test clients to WebSocket endpoints\n   - Publish test messages to Kafka topics\n   - Verify messages flow correctly from Kafka to WebSockets\n   - Test service under load with multiple clients\n\n6. Docker Image Testing:\n   - Build the Docker image and verify size optimization\n   - Test the image in different environments\n   - Validate multi-stage build efficiency\n   - Verify security scanning passes\n\n7. CI/CD Integration:\n   - Set up automated tests in CI pipeline\n   - Configure code coverage reporting\n   - Implement linting and static analysis checks",
        "status": "pending",
        "dependencies": [
          8,
          10,
          23
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up project structure with Onion Architecture",
            "description": "Create the foundational project structure following Onion Architecture principles with proper separation of concerns",
            "dependencies": [],
            "details": "1. Create the base project structure with the following layers:\n   - `domain/`: Core domain models, entities, value objects, and business logic\n   - `application/`: Use cases, application services, and orchestration\n   - `infrastructure/`: External systems integration (Kafka, databases)\n   - `interface/`: API controllers, WebSocket handlers\n2. Set up dependency injection to maintain proper layer isolation\n3. Configure project settings following 12-factor app principles (environment variables, config management)\n4. Create base classes and interfaces for each layer",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement WebSocket endpoints with FastAPI",
            "description": "Create real-time communication endpoints using FastAPI's WebSocket support",
            "dependencies": [
              "31.1"
            ],
            "details": "1. Set up FastAPI application with WebSocket route handlers\n2. Implement connection management for WebSocket clients\n3. Create message serialization/deserialization for WebSocket payloads\n4. Implement authentication and authorization for WebSocket connections\n5. Add error handling and reconnection logic\n6. Create health check endpoints for the service",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Integrate Kafka for event processing",
            "description": "Connect the real-time service with Kafka for event streaming and processing",
            "dependencies": [
              "31.1",
              "31.2"
            ],
            "details": "1. Implement Kafka consumer and producer adapters in the infrastructure layer\n2. Create event handlers for processing incoming Kafka messages\n3. Set up schema validation using Schema Registry\n4. Implement retry logic and error handling for Kafka operations\n5. Create message transformation between domain events and Kafka messages\n6. Configure Kafka connection settings with proper security",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 32,
        "title": "Create Service Enhancement Templates Following GitHub Structure",
        "description": "Create parameterized templates to enhance existing services that lack proper real-time capabilities, following the structure from health-health/realtime_data_pipeline with Onion Architecture and 12-factor app principles.",
        "details": "Develop a set of parameterized templates that can be applied to existing services to enhance them with real-time capabilities:\n\n1. Create a routes/ package template with:\n   - __init__.py file with proper imports and router registration\n   - WebSocket endpoint at /ws path with aiokafka consumer setup\n   - Health check endpoints (/health, /ready, /live)\n   - Proper error handling and logging\n\n2. Implement Onion Architecture structure templates:\n   - domain/ layer: Entity models, value objects, domain events, and business logic\n   - application/ layer: Use cases, application services, and orchestration\n   - infrastructure/ layer: External systems integration (Kafka, databases)\n   - interface/ layer: API controllers, WebSocket handlers, and serialization\n\n3. Set up dependency injection framework:\n   - Create a container.py file with FastAPI dependency injection using Depends\n   - Implement service locator pattern for resolving dependencies\n   - Ensure proper lifecycle management for resources\n\n4. Implement configuration management:\n   - Secret-based configuration loading from environment variables\n   - Configuration validation and type checking\n   - Default fallback values for development environments\n\n5. Configure Kafka integration:\n   - Default topic consumption (ping_topic, pong_topic)\n   - Proper consumer group management\n   - Serialization/deserialization of messages\n   - Error handling and retry mechanisms\n\n6. Create documentation:\n   - README.md with usage instructions\n   - Example implementation\n   - Configuration options\n   - Troubleshooting guide\n\nThe templates must be designed to be applied to existing services that lack the routes package or proper structure, with minimal disruption to existing functionality. All templates must follow CLAUDE.md guidelines for Onion Architecture and adhere to 12-factor app principles.",
        "testStrategy": "1. Unit Testing:\n   - Create unit tests for each template component\n   - Verify that the WebSocket endpoint correctly consumes from Kafka topics\n   - Test dependency injection with various scenarios\n   - Validate configuration loading from secrets\n\n2. Integration Testing:\n   - Apply templates to a sample existing service\n   - Verify that the enhanced service maintains original functionality\n   - Test WebSocket connections and message flow\n   - Validate Kafka consumer group behavior\n\n3. Template Validation:\n   - Verify that templates can be applied with different parameters\n   - Test with services of varying complexity and structure\n   - Ensure templates handle edge cases (missing directories, conflicting files)\n   - Validate that the resulting service follows Onion Architecture principles\n\n4. Documentation Testing:\n   - Have another developer follow the documentation to apply templates\n   - Verify that all configuration options are clearly documented\n   - Test troubleshooting steps for common issues\n\n5. Performance Testing:\n   - Measure overhead added by the templates\n   - Test WebSocket connection handling under load\n   - Verify Kafka consumer performance with high message volumes",
        "status": "pending",
        "dependencies": [
          23,
          31
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Design Domain Layer Templates",
            "description": "Create parameterized templates for the domain layer of the Onion Architecture, including entity models, value objects, and domain services.",
            "dependencies": [],
            "details": "Develop templates for domain entities that represent the core business objects, value objects for immutable data structures, and domain services for business logic. Include base classes and interfaces that can be extended. Ensure proper separation of concerns and that domain models are persistence-ignorant. Add documentation on how to extend these templates for specific business domains.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Application Layer Templates",
            "description": "Create templates for the application layer including use cases, commands, queries, and event handlers.",
            "dependencies": [
              "32.1"
            ],
            "details": "Design templates for application services that orchestrate domain objects to perform specific use cases. Include CQRS patterns with command and query handlers. Create templates for event handlers that process domain events. Implement interfaces for external dependencies that will be injected. Ensure all application services are stateless and follow single responsibility principle.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Develop Infrastructure Layer Templates",
            "description": "Create templates for the infrastructure layer including repositories, external service clients, and data access implementations.",
            "dependencies": [
              "32.1",
              "32.2"
            ],
            "details": "Implement repository templates that provide data persistence capabilities. Create adapters for external services and APIs. Develop data access implementations that connect to various data sources. Include templates for caching mechanisms, message brokers (Kafka/MQTT), and logging services. Ensure all implementations adhere to interfaces defined in the domain and application layers.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Create WebSocket and API Routes Templates",
            "description": "Develop templates for WebSocket endpoints and REST API routes that integrate with the application layer.",
            "dependencies": [
              "32.2"
            ],
            "details": "Create templates for WebSocket endpoints that consume from Kafka topics and broadcast to clients. Implement REST API route templates with proper error handling and validation. Include health check endpoints (/health, /ready, /live) with appropriate status checks. Add middleware templates for authentication, logging, and error handling. Ensure all routes follow RESTful principles and include proper documentation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement Dependency Injection Framework Setup",
            "description": "Create templates for setting up a dependency injection framework that wires all layers together.",
            "dependencies": [
              "32.1",
              "32.2",
              "32.3"
            ],
            "details": "Develop templates for registering services, repositories, and other dependencies in a DI container. Create factory methods for complex object creation. Implement lifetime management (singleton, scoped, transient) for different types of services. Include templates for conditional registration based on environment. Ensure the DI setup follows the dependency inversion principle of SOLID.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Design Configuration Management Templates",
            "description": "Create templates for configuration management that adhere to 12-factor app principles.",
            "dependencies": [],
            "details": "Implement templates for loading configuration from environment variables, files, and secrets. Create strongly-typed configuration classes with validation. Develop templates for different environments (development, testing, production). Include templates for sensitive information handling using secure storage. Ensure configurations follow the 12-factor app principles of storing config in the environment and separating config from code.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Create Integration and Testing Templates",
            "description": "Develop templates for integration testing, unit testing, and monitoring of the enhanced services.",
            "dependencies": [
              "32.1",
              "32.2",
              "32.3",
              "32.4"
            ],
            "details": "Create unit test templates for each layer of the Onion Architecture. Implement integration test templates that verify the interaction between layers. Develop templates for monitoring and observability including metrics, logging, and tracing. Include templates for CI/CD pipeline integration. Ensure all testing follows best practices and provides good coverage of the codebase.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 33,
        "title": "Create realtime-platform ComponentDefinition",
        "description": "Create a new ComponentDefinition for realtime-platform that provides an ultra-minimal developer interface abstracting Kafka, MQTT, Lenses, and Metabase infrastructure behind simple parameters.",
        "details": "Create a new file at `/Users/socrateshlapolosa/Development/health-service-idp/crossplane/oam/realtime-platform-components.yaml` that defines a new ComponentDefinition called \"realtime-platform\". This component will:\n\n1. Abstract the complete Kafka+MQTT+Lenses+Metabase infrastructure stack behind a simple interface\n2. Expose minimal parameters to developers:\n   - name: The name of the realtime platform instance\n   - database (default: postgres): Database backend for storing metadata and metrics\n   - visualization (default: metabase): Visualization tool for real-time data\n   - iot (default: true): Whether to enable MQTT for IoT device connectivity\n\nThe ComponentDefinition should:\n- Follow OAM schema standards and KubeVela conventions\n- Include a CUE template that defines the component's workload and traits\n- Reference the appropriate XRDs and compositions from Task #2 and #4\n- Ensure the component can be referenced by the webservice component's 'realtime' parameter\n- Include proper schema validation for all parameters\n- Document all parameters with clear descriptions\n- Implement sensible defaults for optional parameters\n- Generate appropriate connection secrets that can be consumed by other components\n\nExample structure:\n```yaml\napiVersion: core.oam.dev/v1beta1\nkind: ComponentDefinition\nmetadata:\n  name: realtime-platform\n  namespace: vela-system\n  annotations:\n    definition.oam.dev/description: \"Ultra-minimal developer interface for real-time streaming platform\"\nspec:\n  workload:\n    definition:\n      apiVersion: claim.crossplane.io/v1alpha1\n      kind: RealtimePlatformClaim\n  schematicTrait: true\n  extension:\n    template: |\n      parameter: {\n        // Required parameters\n        name: string\n        \n        // Optional parameters with defaults\n        database: *\"postgres\" | string\n        visualization: *\"metabase\" | string\n        iot: *true | bool\n      }\n      \n      // Output\n      output: {\n        apiVersion: \"claim.crossplane.io/v1alpha1\"\n        kind: \"RealtimePlatformClaim\"\n        metadata: {\n          name: parameter.name\n        }\n        spec: {\n          compositionRef: {\n            name: \"realtime-platform\"\n          }\n          parameters: {\n            name: parameter.name\n            database: parameter.database\n            visualization: parameter.visualization\n            iot: parameter.iot\n          }\n        }\n      }\n```\n\nEnsure this component is separate from existing infrastructure components and provides a high-level abstraction that hides the complexity of the underlying streaming platform infrastructure.",
        "testStrategy": "1. Validate the YAML syntax using a linter:\n   ```bash\n   yamllint crossplane/oam/realtime-platform-components.yaml\n   ```\n\n2. Verify the ComponentDefinition schema conforms to OAM standards:\n   ```bash\n   kubectl vela component validate-yaml crossplane/oam/realtime-platform-components.yaml\n   ```\n\n3. Apply the ComponentDefinition to a KubeVela environment:\n   ```bash\n   kubectl apply -f crossplane/oam/realtime-platform-components.yaml\n   ```\n\n4. Verify the component is registered correctly:\n   ```bash\n   kubectl get componentdefinition realtime-platform -n vela-system -o yaml\n   ```\n\n5. Create a test application that uses the realtime-platform component with minimal configuration:\n   ```yaml\n   apiVersion: core.oam.dev/v1beta1\n   kind: Application\n   metadata:\n     name: test-realtime\n   spec:\n     components:\n       - name: test-platform\n         type: realtime-platform\n         properties:\n           name: test-platform\n   ```\n\n6. Apply the test application and verify that all underlying resources are created correctly:\n   ```bash\n   kubectl apply -f test-realtime-app.yaml\n   kubectl get realtimeplatformclaim test-platform\n   ```\n\n7. Verify that the component generates the expected connection secrets that can be consumed by other components.\n\n8. Test integration with the webservice component by creating a test application that references the realtime-platform component:\n   ```yaml\n   apiVersion: core.oam.dev/v1beta1\n   kind: Application\n   metadata:\n     name: test-integration\n   spec:\n     components:\n       - name: test-platform\n         type: realtime-platform\n         properties:\n           name: test-platform\n       - name: test-service\n         type: webservice\n         properties:\n           image: nginx\n           realtime: test-platform\n   ```\n\n9. Verify that the webservice component correctly receives the connection secrets from the realtime-platform component.",
        "status": "done",
        "dependencies": [
          1,
          2,
          4
        ],
        "priority": "medium",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-07-21T18:19:37.112Z",
      "updated": "2025-07-22T08:27:00.256Z",
      "description": "Tasks for master context"
    }
  }
}