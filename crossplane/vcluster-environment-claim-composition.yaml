apiVersion: apiextensions.crossplane.io/v1
kind: Composition
metadata:
  name: vcluster-environment-claim-composition
  labels:
    crossplane.io/xrd: xvclusterenvironmentclaims.platform.example.org
    provider: helm-kubernetes
spec:
  writeConnectionSecretsToNamespace: crossplane-system
  compositeTypeRef:
    apiVersion: platform.example.org/v1alpha1
    kind: XVClusterEnvironmentClaim
  resources:
  # Create namespace for the vCluster
  - name: vcluster-namespace
    base:
      apiVersion: kubernetes.crossplane.io/v1alpha1
      kind: Object
      spec:
        forProvider:
          manifest:
            apiVersion: v1
            kind: Namespace
            metadata:
              name: placeholder
              labels:
                vcluster.loft.sh/namespace: "true"
        providerConfigRef:
          name: kubernetes-provider
    patches:
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.metadata.name
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-namespace"
  
  # Create vCluster using vcluster CLI with Priority 2 configuration
  - name: vcluster-create-job
    base:
      apiVersion: kubernetes.crossplane.io/v1alpha1
      kind: Object
      spec:
        forProvider:
          manifest:
            apiVersion: batch/v1
            kind: Job
            metadata:
              name: placeholder
              namespace: placeholder
            spec:
              template:
                spec:
                  restartPolicy: OnFailure
                  serviceAccountName: crossplane-admin
                  containers:
                  - name: vcluster-create
                    image: alpine/k8s:1.28.4
                    command:
                    - /bin/sh
                    - -c
                    - |
                      set -x
                      echo "Creating vCluster using CLI..."
                      
                      # Install vcluster CLI
                      wget -q -O /tmp/vcluster https://github.com/loft-sh/vcluster/releases/latest/download/vcluster-linux-amd64
                      chmod +x /tmp/vcluster
                      
                      # Create values file with experimental sync config
                      cat <<'EOF' > /tmp/vcluster-values.yaml
                      experimental:
                        genericSync:
                          role:
                            extraRules:
                              - apiGroups: ["serving.knative.dev"]
                                resources: ["services", "configurations", "revisions", "routes"]
                                verbs: ["create", "delete", "patch", "update", "get", "list", "watch"]
                              - apiGroups: ["core.oam.dev"]
                                resources: ["applications"]
                                verbs: ["create", "delete", "patch", "update", "get", "list", "watch"]
                              - apiGroups: ["helm.crossplane.io"]
                                resources: ["releases"]
                                verbs: ["create", "delete", "patch", "update", "get", "list", "watch"]
                          clusterRole:
                            extraRules:
                              - apiGroups: ["apiextensions.k8s.io"]
                                resources: ["customresourcedefinitions"]
                                verbs: ["get", "list", "watch"]
                              - apiGroups: ["serving.knative.dev"]
                                resources: ["services", "configurations", "revisions", "routes"]
                                verbs: ["create", "delete", "patch", "update", "get", "list", "watch"]
                              - apiGroups: ["core.oam.dev"]
                                resources: ["applications"]
                                verbs: ["create", "delete", "patch", "update", "get", "list", "watch"]
                              - apiGroups: ["helm.crossplane.io"]
                                resources: ["releases"]
                                verbs: ["create", "delete", "patch", "update", "get", "list", "watch"]
                          export:
                            - apiVersion: serving.knative.dev/v1
                              kind: Service
                            - apiVersion: serving.knative.dev/v1
                              kind: Configuration
                            - apiVersion: serving.knative.dev/v1
                              kind: Revision
                            - apiVersion: serving.knative.dev/v1
                              kind: Route
                            - apiVersion: core.oam.dev/v1beta1
                              kind: Application
                            - apiVersion: helm.crossplane.io/v1beta1
                              kind: Release
                      EOF
                      
                      # Check if vCluster already exists
                      if /tmp/vcluster list | grep -q "${VCLUSTER_NAME}"; then
                        echo "vCluster ${VCLUSTER_NAME} already exists, skipping creation"
                        exit 0
                      fi
                      
                      # Create vCluster with sync config
                      echo "Creating vCluster ${VCLUSTER_NAME} in namespace ${NAMESPACE}..."
                      /tmp/vcluster create ${VCLUSTER_NAME} \
                        --namespace ${NAMESPACE} \
                        --values /tmp/vcluster-values.yaml \
                        --connect=false \
                        --expose || {
                          echo "Failed to create vCluster"
                          exit 1
                        }
                      
                      echo "vCluster ${VCLUSTER_NAME} created successfully!"
                      
                      # Wait for vCluster to be ready
                      echo "Waiting for vCluster to be ready..."
                      for i in {1..60}; do
                        if kubectl get secret vc-${VCLUSTER_NAME} -n ${NAMESPACE} &>/dev/null; then
                          echo "vCluster is ready!"
                          break
                        fi
                        echo "Waiting... ($i/60)"
                        sleep 5
                      done
                    env:
                    - name: VCLUSTER_NAME
                      value: placeholder
                    - name: NAMESPACE
                      value: placeholder
        providerConfigRef:
          name: kubernetes-provider
    patches:
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-vcluster-create"
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.metadata.namespace
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-vcluster-create"
    # Patch environment variables
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].env[0].value
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].env[1].value

  # Setup CRDs and ComponentDefinitions in vCluster
  - name: vcluster-crd-setup
    base:
      apiVersion: kubernetes.crossplane.io/v1alpha1
      kind: Object
      spec:
        forProvider:
          manifest:
            apiVersion: batch/v1
            kind: Job
            metadata:
              name: placeholder
              namespace: placeholder
            spec:
              template:
                spec:
                  restartPolicy: OnFailure
                  serviceAccountName: crossplane-admin
                  containers:
                  - name: crd-setup
                    image: alpine/k8s:1.28.4
                    command:
                    - /bin/sh
                    - -c
                    - |
                      set -x  # Enable debug output
                      echo "========================================="
                      echo "Starting CRD setup job for vCluster"
                      echo "VCLUSTER_NAME: ${VCLUSTER_NAME}"
                      echo "NAMESPACE: ${NAMESPACE}"
                      echo "========================================="
                      
                      # First, label CRDs in host cluster for sync
                      echo "Step 1: Labeling CRDs in host cluster for sync..."
                      for crd in services.serving.knative.dev configurations.serving.knative.dev revisions.serving.knative.dev routes.serving.knative.dev; do
                        echo "  Labeling Knative CRD: $crd"
                        kubectl label crd $crd sync=true --overwrite || echo "    Warning: Could not label $crd"
                      done
                      
                      for crd in applications.core.oam.dev componentdefinitions.core.oam.dev traitdefinitions.core.oam.dev policydefinitions.core.oam.dev workloaddefinitions.core.oam.dev applicationrevisions.core.oam.dev; do
                        echo "  Labeling OAM CRD: $crd"
                        kubectl label crd $crd sync=true --overwrite || echo "    Warning: Could not label $crd"
                      done
                      
                      echo "Step 2: Waiting for vCluster to be created..."
                      # First wait for the vCluster creation job to complete
                      echo "Waiting for vCluster creation job to complete..."
                      kubectl wait --for=condition=complete job/${VCLUSTER_NAME}-vcluster-create -n ${NAMESPACE} --timeout=300s || {
                        echo "vCluster creation job did not complete in time"
                        kubectl get job/${VCLUSTER_NAME}-vcluster-create -n ${NAMESPACE}
                      }
                      
                      # Save original kubeconfig
                      export HOST_KUBECONFIG=$KUBECONFIG
                      # Setup kubeconfig for vCluster
                      export VCLUSTER_KUBECONFIG=/tmp/kubeconfig
                      
                      # Expected secret name (vcluster CLI creates it as vc-<name>)
                      SECRET_NAME="vc-${VCLUSTER_NAME}"
                      echo "Looking for secret: $SECRET_NAME in namespace: $NAMESPACE"
                      
                      # Wait for secret to be created
                      for i in {1..30}; do
                        if kubectl get secret $SECRET_NAME -n ${NAMESPACE} &>/dev/null; then
                          echo "Secret found! Extracting kubeconfig..."
                          kubectl get secret $SECRET_NAME -n ${NAMESPACE} -o jsonpath='{.data.config}' | base64 -d > $VCLUSTER_KUBECONFIG
                          echo "Kubeconfig saved to $VCLUSTER_KUBECONFIG"
                          break
                        fi
                        echo "Waiting for secret $SECRET_NAME... ($i/30)"
                        kubectl get secrets -n ${NAMESPACE} | grep -E "vcluster|kubeconfig" || echo "No vCluster secrets found yet"
                        sleep 2
                      done
                      
                      # Check if kubeconfig was created
                      if [ ! -f $VCLUSTER_KUBECONFIG ]; then
                        echo "ERROR: Kubeconfig not found at $VCLUSTER_KUBECONFIG"
                        kubectl get secrets -n ${NAMESPACE}
                        exit 1
                      fi
                      
                      echo "Step 3: Updating kubeconfig server URL..."
                      echo "Original kubeconfig server:"
                      grep "server:" $VCLUSTER_KUBECONFIG
                      
                      # Update to use service name with full DNS
                      sed -i "s|https://localhost:8443|https://${VCLUSTER_NAME}.${NAMESPACE}.svc.cluster.local:443|g" $VCLUSTER_KUBECONFIG
                      
                      echo "Updated kubeconfig server:"
                      grep "server:" $VCLUSTER_KUBECONFIG
                      
                      # Wait for vCluster to be ready
                      echo "Step 4: Waiting for vCluster API server to be ready..."
                      for i in {1..60}; do
                        if kubectl --kubeconfig=$VCLUSTER_KUBECONFIG get nodes 2>&1; then
                          echo "vCluster API server is ready!"
                          kubectl --kubeconfig=$VCLUSTER_KUBECONFIG get nodes
                          break
                        fi
                        echo "Waiting for vCluster API... ($i/60)"
                        sleep 5
                      done
                      
                      # Define CRDs to copy
                      echo "Step 5: Checking what CRDs exist in host cluster..."
                      echo "Total CRDs in host cluster:"
                      kubectl get crd | wc -l
                      echo "Knative CRDs in host:"
                      kubectl get crd | grep knative || echo "No Knative CRDs found"
                      echo "OAM CRDs in host:"
                      kubectl get crd | grep oam || echo "No OAM CRDs found"
                      
                      KNATIVE_CRDS="services.serving.knative.dev configurations.serving.knative.dev revisions.serving.knative.dev routes.serving.knative.dev"
                      OAM_CRDS="applications.core.oam.dev componentdefinitions.core.oam.dev traitdefinitions.core.oam.dev policydefinitions.core.oam.dev workloaddefinitions.core.oam.dev applicationrevisions.core.oam.dev"
                      ALL_CRDS="$KNATIVE_CRDS $OAM_CRDS"
                      
                      echo "CRDs to copy: $ALL_CRDS"
                      
                      # Copy CRDs from host to vCluster
                      echo "Step 6: Copying CRDs from host to vCluster..."
                      if kubectl get crd $ALL_CRDS -o yaml > /tmp/crds-to-copy.yaml; then
                        echo "CRDs exported successfully to /tmp/crds-to-copy.yaml"
                        echo "File size: $(wc -l /tmp/crds-to-copy.yaml)"
                        
                        echo "Applying CRDs to vCluster..."
                        kubectl --kubeconfig=$VCLUSTER_KUBECONFIG apply -f /tmp/crds-to-copy.yaml || {
                          echo "Some CRDs failed to apply, checking specific errors..."
                          kubectl --kubeconfig=$VCLUSTER_KUBECONFIG apply -f /tmp/crds-to-copy.yaml 2>&1 | head -20
                        }
                      else
                        echo "ERROR: Failed to export CRDs. Checking what CRDs exist..."
                        kubectl get crd | grep -E "knative|oam"
                      fi
                      
                      # Verify CRDs were copied
                      echo "Step 7: Verifying CRDs in vCluster..."
                      echo "Total CRDs in vCluster:"
                      kubectl --kubeconfig=$VCLUSTER_KUBECONFIG get crd | wc -l
                      echo "Knative and OAM CRDs:"
                      kubectl --kubeconfig=$VCLUSTER_KUBECONFIG get crd | grep -E "knative|oam" || echo "Warning: Expected CRDs not found"
                      
                      # Create vela-system namespace
                      echo "Step 8: Creating vela-system namespace..."
                      kubectl --kubeconfig=$VCLUSTER_KUBECONFIG create namespace vela-system --dry-run=client -o yaml | \
                        kubectl --kubeconfig=$VCLUSTER_KUBECONFIG apply -f -
                      
                      echo "Verifying namespace creation:"
                      kubectl --kubeconfig=$VCLUSTER_KUBECONFIG get ns
                      
                      # Copy ComponentDefinitions from host to vCluster (optional)
                      echo "Step 9: Copying ComponentDefinitions to vCluster..."
                      echo "Note: In Priority 2 architecture, OAM runs in host cluster"
                      echo "ComponentDefinitions are optional in vCluster"
                      
                      # Only copy if they exist in host
                      if kubectl get componentdefinition -n vela-system &>/dev/null; then
                        echo "Found ComponentDefinitions in host, copying to vCluster for reference..."
                        kubectl get componentdefinition -n vela-system -o yaml | \
                          kubectl --kubeconfig=$VCLUSTER_KUBECONFIG apply -f - 2>/dev/null || \
                          echo "Note: ComponentDefinitions copy is optional"
                      else
                        echo "No ComponentDefinitions to copy (KubeVela runs in host)"
                      fi
                      
                      echo "Step 10: Final verification..."
                      echo "Namespaces in vCluster:"
                      kubectl --kubeconfig=$VCLUSTER_KUBECONFIG get ns
                      echo ""
                      echo "CRDs in vCluster (total):"
                      kubectl --kubeconfig=$VCLUSTER_KUBECONFIG get crd | wc -l
                      echo ""
                      echo "ComponentDefinitions in vCluster:"
                      kubectl --kubeconfig=$VCLUSTER_KUBECONFIG get componentdefinitions -A
                      echo ""
                      echo "========================================="
                      echo "CRDs and ComponentDefinitions setup complete!"
                      echo "========================================="
                    env:
                    - name: VCLUSTER_NAME
                      value: placeholder
                    - name: NAMESPACE
                      value: placeholder
        providerConfigRef:
          name: kubernetes-provider
    patches:
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-crd-setup"
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.metadata.namespace
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-crd-setup"
    # Patch environment variables
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].env[0].value
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].env[1].value

  # Create ServiceAccount for installer jobs
  - name: vcluster-admin-serviceaccount
    base:
      apiVersion: kubernetes.crossplane.io/v1alpha1
      kind: Object
      spec:
        forProvider:
          manifest:
            apiVersion: v1
            kind: ServiceAccount
            metadata:
              name: vcluster-admin
              namespace: placeholder
        providerConfigRef:
          name: kubernetes-provider
    patches:
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.metadata.namespace
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-admin-sa"

  # Create ClusterRoleBinding for vcluster-admin ServiceAccount
  - name: vcluster-admin-clusterrolebinding
    base:
      apiVersion: kubernetes.crossplane.io/v1alpha1
      kind: Object
      spec:
        forProvider:
          manifest:
            apiVersion: rbac.authorization.k8s.io/v1
            kind: ClusterRoleBinding
            metadata:
              name: placeholder
            roleRef:
              apiGroup: rbac.authorization.k8s.io
              kind: ClusterRole
              name: cluster-admin
            subjects:
            - kind: ServiceAccount
              name: vcluster-admin
              namespace: placeholder
        providerConfigRef:
          name: kubernetes-provider
    patches:
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-admin-crb"
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-admin-crb"
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.subjects[0].namespace

  # Install Knative if capability is enabled
  - name: knative-installer
    base:
      apiVersion: kubernetes.crossplane.io/v1alpha1
      kind: Object
      spec:
        forProvider:
          manifest:
            apiVersion: batch/v1
            kind: Job
            metadata:
              name: knative-installer
              namespace: placeholder
            spec:
              template:
                spec:
                  serviceAccountName: vcluster-admin
                  restartPolicy: OnFailure
                  containers:
                  - name: installer
                    image: bitnami/kubectl:1.28
                    command: ["/bin/sh"]
                    args:
                    - -c
                    - |
                      set -e
                      echo "üöÄ Installing Knative in vCluster..."
                      
                      # Wait for vCluster to be ready
                      echo "‚è≥ Waiting for vCluster to be ready..."
                      sleep 30
                      
                      # Connect to vCluster
                      VCLUSTER_NAME="${VCLUSTER_NAME}"
                      NAMESPACE="${NAMESPACE}"
                      
                      echo "üì° Connecting to vCluster: $VCLUSTER_NAME in namespace $NAMESPACE"
                      
                      # Get vCluster kubeconfig
                      vcluster connect $VCLUSTER_NAME --namespace $NAMESPACE --update-current=false --kube-config=/tmp/kubeconfig.yaml
                      export KUBECONFIG=/tmp/kubeconfig.yaml
                      
                      # Check if Knative is already installed
                      if kubectl get namespace knative-serving >/dev/null 2>&1; then
                        echo "‚úÖ Knative namespace already exists"
                        if kubectl get deployment -n knative-serving controller >/dev/null 2>&1; then
                          echo "‚úÖ Knative is already installed"
                          exit 0
                        fi
                      fi
                      
                      echo "üì¶ Installing Knative Serving..."
                      
                      # Create namespace
                      kubectl create namespace knative-serving --dry-run=client -o yaml | kubectl apply -f -
                      
                      # Install Knative Serving CRDs and Core
                      kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.12.0/serving-crds.yaml
                      kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.12.0/serving-core.yaml
                      
                      # Install Kourier as the networking layer (lightweight alternative to Istio)
                      kubectl apply -f https://github.com/knative/net-kourier/releases/download/knative-v1.12.0/kourier.yaml
                      
                      # Configure Knative to use Kourier
                      kubectl patch configmap/config-network \
                        --namespace knative-serving \
                        --type merge \
                        --patch '{"data":{"ingress-class":"kourier.ingress.networking.knative.dev"}}'
                      
                      # Configure domain
                      kubectl patch configmap/config-domain \
                        --namespace knative-serving \
                        --type merge \
                        --patch '{"data":{"svc.cluster.local":""}}'
                      
                      # Wait for Knative to be ready
                      echo "‚è≥ Waiting for Knative components to be ready..."
                      kubectl wait --for=condition=Ready pods --all -n knative-serving --timeout=300s || true
                      
                      echo "‚úÖ Knative installation completed"
                      
                      # Verify installation
                      echo "üîç Verifying Knative installation:"
                      kubectl get pods -n knative-serving
                      kubectl get pods -n kourier-system
                      
                      echo "‚úÖ Knative is ready in vCluster $VCLUSTER_NAME"
                    env:
                    - name: VCLUSTER_NAME
                      value: placeholder
                    - name: NAMESPACE
                      value: placeholder
        providerConfigRef:
          name: kubernetes-provider
    patches:
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-knative-installer"
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.metadata.namespace
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-knative-installer"
    # Patch environment variables
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].env[0].value
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].env[1].value
    # Only create if knative capability is enabled
    - type: FromCompositeFieldPath
      fromFieldPath: spec.capabilities.knative
      toFieldPath: spec.forProvider.manifest.metadata.annotations.skip-resource
      transforms:
      - type: map
        map:
          "false": "true"
          "true": "false"

  # Create ServiceAccount for Crossplane operations
  - name: vcluster-service-account
    base:
      apiVersion: kubernetes.crossplane.io/v1alpha1
      kind: Object
      spec:
        forProvider:
          manifest:
            apiVersion: v1
            kind: ServiceAccount
            metadata:
              name: crossplane-admin
              namespace: default
        providerConfigRef:
          name: kubernetes-provider
    patches:
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.metadata.namespace
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-service-account"

  # Create ClusterRoleBinding for Crossplane ServiceAccount
  - name: vcluster-service-account-binding
    base:
      apiVersion: kubernetes.crossplane.io/v1alpha1
      kind: Object
      spec:
        forProvider:
          manifest:
            apiVersion: rbac.authorization.k8s.io/v1
            kind: ClusterRoleBinding
            metadata:
              name: crossplane-admin-binding
            roleRef:
              apiGroup: rbac.authorization.k8s.io
              kind: ClusterRole
              name: cluster-admin
            subjects:
            - kind: ServiceAccount
              name: crossplane-admin
              namespace: default
        providerConfigRef:
          name: kubernetes-provider
    patches:
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-crossplane-admin-binding"
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.subjects[0].namespace
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-service-account-binding"

  # Create EBS StorageClass
  - name: vcluster-ebs-storageclass
    base:
      apiVersion: kubernetes.crossplane.io/v1alpha1
      kind: Object
      spec:
        forProvider:
          manifest:
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: ebs-fast-storage
              annotations:
                storageclass.kubernetes.io/is-default-class: "false"
            provisioner: kubernetes.io/aws-ebs
            volumeBindingMode: WaitForFirstConsumer
            parameters:
              type: gp3
              encrypted: "true"
            reclaimPolicy: Delete
            allowVolumeExpansion: true
        providerConfigRef:
          name: kubernetes-provider
    patches:
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-ebs-storageclass"

  # Note: KubeVela is NOT installed in vCluster for Priority 2 architecture
  # KubeVela runs in the host cluster and manages vCluster workloads remotely

  # Create vCluster Provider Config using kubeconfig secret
  - name: vcluster-provider-config
    base:
      apiVersion: kubernetes.crossplane.io/v1alpha1
      kind: ProviderConfig
      metadata:
        name: placeholder
      spec:
        credentials:
          source: Secret
          secretRef:
            name: placeholder
            namespace: placeholder
            key: config
    patches:
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: metadata.name
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.credentials.secretRef.name
      transforms:
      - type: string
        string:
          fmt: "vc-%s"  # vcluster CLI creates secret as vc-<name>
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.credentials.secretRef.namespace

  # Optional: Register vCluster with KubeVela for multi-cluster management
  # Note: In Priority 2, this is optional since workloads sync to host
  # Uncomment if you want KubeVela to directly manage vCluster resources  # Create ServiceAccount for KubeVela access in vCluster
  - name: kubevela-access-serviceaccount
    base:
      apiVersion: kubernetes.crossplane.io/v1alpha1
      kind: Object
      spec:
        forProvider:
          manifest:
            apiVersion: v1
            kind: ServiceAccount
            metadata:
              name: kubevela-access
              namespace: kube-system
        providerConfigRef:
          name: placeholder-vcluster
    patches:
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.providerConfigRef.name
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-kubevela-sa"

  # Create ClusterRoleBinding for KubeVela access in vCluster
  - name: kubevela-access-clusterrolebinding
    base:
      apiVersion: kubernetes.crossplane.io/v1alpha1
      kind: Object
      spec:
        forProvider:
          manifest:
            apiVersion: rbac.authorization.k8s.io/v1
            kind: ClusterRoleBinding
            metadata:
              name: kubevela-access
            roleRef:
              apiGroup: rbac.authorization.k8s.io
              kind: ClusterRole
              name: cluster-admin
            subjects:
            - kind: ServiceAccount
              name: kubevela-access
              namespace: kube-system
        providerConfigRef:
          name: placeholder-vcluster
    patches:
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.providerConfigRef.name
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-kubevela-crb"

  # Create token secret for KubeVela access
  - name: kubevela-token-secret
    base:
      apiVersion: kubernetes.crossplane.io/v1alpha1
      kind: Object
      spec:
        forProvider:
          manifest:
            apiVersion: v1
            kind: Secret
            metadata:
              name: kubevela-access-token
              namespace: kube-system
              annotations:
                kubernetes.io/service-account.name: kubevela-access
            type: kubernetes.io/service-account-token
        providerConfigRef:
          name: placeholder-vcluster
    patches:
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.providerConfigRef.name
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-kubevela-token"

  # Job to extract token and create ClusterGateway
  - name: vcluster-register-clustergateway
    base:
      apiVersion: kubernetes.crossplane.io/v1alpha1
      kind: Object
      spec:
        forProvider:
          manifest:
            apiVersion: batch/v1
            kind: Job
            metadata:
              name: placeholder
              namespace: placeholder
            spec:
              template:
                spec:
                  serviceAccountName: crossplane-admin
                  restartPolicy: OnFailure
                  containers:
                  - name: register-clustergateway
                    image: alpine/k8s:1.28.4
                    command:
                    - /bin/sh
                    - -c
                    - |
                      set -e
                      echo "Registering vCluster with ClusterGateway..."
                      
                      VCLUSTER_NAME="${VCLUSTER_NAME}"
                      NAMESPACE="${NAMESPACE}"
                      
                      # Install vela CLI
                      echo "Installing vela CLI..."
                      wget -q -O /tmp/vela https://github.com/kubevela/kubevela/releases/download/v1.9.11/vela-v1.9.11-linux-amd64.tar.gz
                      tar -xzf /tmp/vela -C /tmp/
                      chmod +x /tmp/linux-amd64/vela
                      export PATH=/tmp/linux-amd64:$PATH
                      
                      # Wait for vCluster creation job to complete first
                      echo "Waiting for vCluster creation job to complete..."
                      kubectl wait --for=condition=complete job/${VCLUSTER_NAME}-vcluster-create -n ${NAMESPACE} --timeout=600s || {
                        echo "vCluster creation job did not complete, checking status..."
                        kubectl get job/${VCLUSTER_NAME}-vcluster-create -n ${NAMESPACE}
                      }
                      
                      # Wait for vCluster pod to be ready
                      echo "Waiting for vCluster pod to be ready..."
                      kubectl wait --for=condition=ready pod -l app=vcluster -n $NAMESPACE --timeout=300s || true
                      
                      # Wait for vCluster secret to be created
                      echo "Waiting for vCluster secret..."
                      for i in $(seq 1 60); do
                        if kubectl get secret vc-${VCLUSTER_NAME} -n ${NAMESPACE} >/dev/null 2>&1; then
                          echo "vCluster secret found!"
                          break
                        fi
                        echo "Waiting for vCluster secret... ($i/60)"
                        sleep 5
                      done
                      
                      # Get vCluster kubeconfig using vcluster CLI
                      echo "Getting vCluster kubeconfig..."
                      # Install vcluster CLI
                      wget -q -O /tmp/vcluster https://github.com/loft-sh/vcluster/releases/latest/download/vcluster-linux-amd64
                      chmod +x /tmp/vcluster
                      
                      # Get the kubeconfig with proper external endpoint
                      /tmp/vcluster connect ${VCLUSTER_NAME} -n ${NAMESPACE} --print > /tmp/vcluster.kubeconfig
                      
                      # Extract the endpoint URL from kubeconfig
                      ENDPOINT=$(grep "server:" /tmp/vcluster.kubeconfig | awk '{print $2}' | sed 's|https://||' | cut -d: -f1)
                      echo "vCluster endpoint: $ENDPOINT"
                      
                      # Wait for DNS to be resolvable
                      echo "Waiting for DNS propagation of $ENDPOINT..."
                      for i in $(seq 1 30); do
                        if nslookup $ENDPOINT >/dev/null 2>&1; then
                          echo "DNS resolved successfully!"
                          break
                        fi
                        echo "Waiting for DNS... ($i/30)"
                        sleep 10
                      done
                      
                      # Register vCluster with KubeVela using vela cluster join
                      echo "Registering vCluster with KubeVela..."
                      
                      # First, try to detach the existing cluster if it exists
                      if vela cluster list | grep -q ${VCLUSTER_NAME}; then
                        echo "Existing cluster registration found, detaching first..."
                        vela cluster detach ${VCLUSTER_NAME} 2>/dev/null || true
                        sleep 5
                      fi
                      
                      # Now register the vCluster with the new endpoint
                      for attempt in $(seq 1 3); do
                        if vela cluster join /tmp/vcluster.kubeconfig --name ${VCLUSTER_NAME}; then
                          echo "‚úÖ Successfully registered vCluster!"
                          break
                        else
                          echo "Registration attempt $attempt failed, retrying in 30s..."
                          sleep 30
                        fi
                      done
                      
                      # Verify registration
                      echo "Verifying cluster registration..."
                      if vela cluster list | grep -q ${VCLUSTER_NAME}; then
                        echo "‚úÖ vCluster ${VCLUSTER_NAME} successfully registered with KubeVela"
                        
                        # Create namespace in vCluster for OAM deployment
                        echo "Creating namespace ${VCLUSTER_NAME} in vCluster..."
                        export KUBECONFIG=/tmp/vcluster.kubeconfig
                        kubectl create namespace ${VCLUSTER_NAME} --dry-run=client -o yaml | kubectl apply -f -
                        echo "‚úÖ Namespace ${VCLUSTER_NAME} created in vCluster"
                        
                        # Also create the default namespace if not exists (for KubeVela)
                        kubectl create namespace default --dry-run=client -o yaml | kubectl apply -f -
                        
                        # Create knative-docker-sa service account in default namespace
                        echo "Creating knative-docker-sa service account in default namespace..."
                        kubectl create serviceaccount knative-docker-sa -n default --dry-run=client -o yaml | kubectl apply -f -
                        
                        # Copy docker registry secret to vCluster default namespace
                        echo "Copying docker registry secret to vCluster..."
                        unset KUBECONFIG
                        kubectl get secret docker-registry-secret -n default -o yaml 2>/dev/null | \
                          sed 's/namespace: default/namespace: default/' | \
                          KUBECONFIG=/tmp/vcluster.kubeconfig kubectl apply -f - || \
                          echo "Note: docker-registry-secret not found in host cluster, skipping"
                        export KUBECONFIG=/tmp/vcluster.kubeconfig
                        
                        # Apply webservice WorkloadDefinition to vCluster for Knative support
                        echo "Applying webservice WorkloadDefinition to vCluster..."
                        cat <<'WD_EOF' | kubectl apply -f -
                        apiVersion: core.oam.dev/v1beta1
                        kind: WorkloadDefinition
                        metadata:
                          name: webservice
                          namespace: default
                          annotations:
                            definition.oam.dev/description: "Knative Service workload for serverless deployment"
                        spec:
                          definitionRef:
                            name: services.serving.knative.dev
                          schematic:
                            cue:
                              template: |
                                output: {
                                  apiVersion: "serving.knative.dev/v1"
                                  kind: "Service"
                                  metadata: {
                                    name: context.name
                                    namespace: context.namespace
                                  }
                                  spec: {
                                    template: {
                                      metadata: {
                                        annotations: {
                                          "autoscaling.knative.dev/minScale": "0"
                                          "autoscaling.knative.dev/maxScale": "10"
                                        }
                                      }
                                      spec: {
                                        containers: [{
                                          image: parameter.image
                                          ports: [{
                                            containerPort: parameter.port
                                          }]
                                          resources: {
                                            limits: {
                                              cpu: "500m"
                                              memory: "512Mi"
                                            }
                                          }
                                        }]
                                      }
                                    }
                                  }
                                }
                                parameter: {
                                  image: string
                                  port: *8080 | int
                                }
                        WD_EOF
                        echo "‚úÖ WorkloadDefinition applied to vCluster"
                      else
                        echo "‚ö†Ô∏è vCluster registration may have failed, please check manually"
                      fi
                    env:
                    - name: VCLUSTER_NAME
                      value: placeholder
                    - name: NAMESPACE
                      value: placeholder
        providerConfigRef:
          name: kubernetes-provider
    patches:
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-register-clustergateway"
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.metadata.name
      transforms:
      - type: string
        string:
          fmt: "%s-register-clustergateway"
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.metadata.namespace
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].env[0].value
    - type: FromCompositeFieldPath
      fromFieldPath: spec.name
      toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].env[1].value